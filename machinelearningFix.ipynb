{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e1989b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    classification_report, confusion_matrix,\n",
    "    cohen_kappa_score, matthews_corrcoef\n",
    ")\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "# Basisstijl voor plots\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
    "\n",
    "plt.rcParams[\"axes.grid\"] = True\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "\n",
    "# STAP 1: DATA INLADEN EN EERSTE VERKENNING\n",
    "\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"STAP 1: DATA INLADEN EN EERSTE VERKENNING\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "\n",
    "\n",
    "# Laad de dataset\n",
    "\n",
    "df = pd.read_csv(\"ML-sleep_health_lifestyle_dataset.csv\")\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\nDataset geladen met {df.shape[0]} rijen en {df.shape[1]} kolommen\")\n",
    "\n",
    "print(\"\\nEerste 5 rijen:\")\n",
    "\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9769f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STAP 2: DATA INSPECTIE EN KWALITEITSCONTROLE\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STAP 2: DATA INSPECTIE EN KWALITEITSCONTROLE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nInformatie over datatypes en geheugengebruik:\")\n",
    "df.info()\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"CONTROLE OP ONTBREKENDE WAARDEN\")\n",
    "print(\"-\"*80)\n",
    "missing_values = df.isnull().sum()\n",
    "missing_pct = 100 * df.isnull().sum() / len(df)\n",
    "missing_table = pd.DataFrame({\n",
    "    'Aantal Missing': missing_values,\n",
    "    'Percentage': missing_pct\n",
    "})\n",
    "print(missing_table[missing_table['Aantal Missing'] > 0])\n",
    "\n",
    "if missing_table['Aantal Missing'].sum() == 0:\n",
    "    print(\"\\n✓ Geen ontbrekende waarden gevonden in de dataset\")\n",
    "    print(\"Dit is gunstig voor modelontwikkeling omdat we geen imputatiestrategieën nodig hebben.\")\n",
    "else:\n",
    "    print(\"\\n⚠ Er zijn ontbrekende waarden die behandeld moeten worden\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"BESCHRIJVENDE STATISTIEKEN - NUMERIEKE VARIABELEN\")\n",
    "print(\"-\"*80)\n",
    "display(df.describe())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3504e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STAP 3: TARGET VARIABELE ANALYSE\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STAP 3: TARGET VARIABELE ANALYSE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nDe target variabele 'Sleep Disorder' vormt de basis voor ons classificatieprobleem.\")\n",
    "print(\"We onderzoeken de verdeling van klassen om te bepalen of we te maken hebben met\")\n",
    "print(\"class imbalance en of dit speciale aandacht vereist tijdens modeltraining.\\n\")\n",
    "\n",
    "# Vervang NaN in Sleep Disorder met 'None'\n",
    "df['Sleep Disorder'] = df['Sleep Disorder'].fillna('None')\n",
    "\n",
    "print(\"Verdeling van Sleep Disorder:\")\n",
    "class_distribution = df['Sleep Disorder'].value_counts()\n",
    "print(class_distribution)\n",
    "print(f\"\\nPercentages:\")\n",
    "print(df['Sleep Disorder'].value_counts(normalize=True) * 100)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "df['Sleep Disorder'].value_counts().plot(kind='bar', color=['#2ecc71', '#e74c3c', '#3498db'])\n",
    "plt.title(\"Verdeling van Sleep Disorder Klassen\", fontsize=14, fontweight='bold')\n",
    "plt.xlabel(\"Klasse\")\n",
    "plt.ylabel(\"Aantal observaties\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"INTERPRETATIE VAN KLASSENBALANS\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Bereken imbalance ratio\n",
    "min_class = class_distribution.min()\n",
    "max_class = class_distribution.max()\n",
    "imbalance_ratio = max_class / min_class\n",
    "\n",
    "print(f\"\\nImbalance ratio (grootste/kleinste klasse): {imbalance_ratio:.2f}\")\n",
    "\n",
    "if imbalance_ratio < 1.5:\n",
    "    print(\" De klassen zijn redelijk gebalanceerd. Standaard modeltraining is geschikt.\")\n",
    "elif imbalance_ratio < 3:\n",
    "    print(\"Er is enige klassenonevenwichtigheid. Overweeg stratified sampling en\")\n",
    "    print(\"  class_weight='balanced' parameter bij sommige modellen.\")\n",
    "else:\n",
    "    print(\"Significante klassenonevenwichtigheid gedetecteerd!\")\n",
    "    print(\"  Overweeg: SMOTE, class weighting, of stratified k-fold cross-validation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff8c5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# STAP 4: MULTICLASS VS BINARY CLASSIFICATIE - ONDERBOUWING\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STAP 4: MODELTYPE KEUZE - MULTICLASS VS BINARY CLASSIFICATIE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "RATIONALE VOOR MULTICLASS CLASSIFICATIE:\n",
    "-----------------------------------------\n",
    "\n",
    "Onze target variabele heeft drie categorieën:\n",
    "1. None (geen slaapstoornis)\n",
    "2. Insomnia (slapeloosheid)\n",
    "3. Sleep Apnea (slaapapneu)\n",
    "\n",
    "WAAROM MULTICLASS IN PLAATS VAN BINARY?\n",
    "========================================\n",
    "\n",
    "1. KLINISCHE RELEVANTIE:\n",
    "   • Insomnia en Sleep Apnea hebben verschillende oorzaken, symptomen en behandelingen\n",
    "   • Een binair model (wel/geen stoornis) zou deze cruciale distinctie verliezen\n",
    "   • Voor medisch personeel is het essentieel om het TYPE stoornis te identificeren\n",
    "\n",
    "2. BEHANDELINGSIMPLICATIES:\n",
    "   • Insomnia → vaak cognitieve gedragstherapie, slaaphygiëne, medicatie\n",
    "   • Sleep Apnea → CPAP-apparaat, gewichtsreductie, operatieve ingrepen\n",
    "   • De aanpak verschilt fundamenteel\n",
    "\n",
    "3. DIAGNOSTISCHE WAARDE:\n",
    "   • Verschillende risicoprofielen: Sleep Apnea correleert met BMI en hartslag,\n",
    "     Insomnia vaak met stress en levensstijlfactoren\n",
    "   • Een multiclass model kan deze subtiele patronen onderscheiden\n",
    "\n",
    "4. MODELCOMPLEXITEIT VS INFORMATIEBEHOUD:\n",
    "   • Trade-off: multiclass is complexer, maar behoudt essentiële informatie\n",
    "   • In medische context weegt informatieverlies zwaarder dan modelcomplexiteit\n",
    "\n",
    "\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbce46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# STAP 5: FEATURE TYPE IDENTIFICATIE EN CATEGORISATIE\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STAP 5: FEATURE TYPE IDENTIFICATIE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Numerieke kolommen\n",
    "num_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "# Verwijder Person ID uit numerieke features (is geen predictive feature)\n",
    "num_cols = [col for col in num_cols if col != 'Person ID']\n",
    "\n",
    "# Categorische kolommen\n",
    "cat_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "# Verwijder target uit categorische features\n",
    "cat_feature_cols = [col for col in cat_cols if col != 'Sleep Disorder']\n",
    "\n",
    "print(f\"\\nNumerieke features ({len(num_cols)}):\")\n",
    "for col in num_cols:\n",
    "    print(f\"  • {col}\")\n",
    "\n",
    "print(f\"\\nCategorische features ({len(cat_feature_cols)}):\")\n",
    "for col in cat_feature_cols:\n",
    "    unique_values = df[col].nunique()\n",
    "    print(f\"  • {col} ({unique_values} unieke waarden)\")\n",
    "    print(f\"    Waarden: {df[col].unique()[:5].tolist()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615b9d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STAP 6: TRAIN-TEST SPLIT - DATA PARTITIONERING\n",
    "# ============================================================================\n",
    "\n",
    "# Split on raw data (before encoding/scaling to prevent data leakage)\n",
    "X = df.drop(['Sleep Disorder', 'Person ID'], axis=1)\n",
    "y = df['Sleep Disorder']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training: {X_train.shape[0]} samples ({100*len(X_train)/len(X):.0f}%)\")\n",
    "print(f\"Test: {X_test.shape[0]} samples ({100*len(X_test)/len(X):.0f}%)\")\n",
    "\n",
    "# Verify stratification\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "for ax, data, title in zip(axes, \n",
    "                            [y, y_train, y_test], \n",
    "                            ['Original', 'Train (80%)', 'Test (20%)']):\n",
    "    data.value_counts(normalize=True).sort_index().plot(\n",
    "        kind='bar', ax=ax, color=['skyblue', 'lightgreen', 'lightcoral'][axes.tolist().index(ax)]\n",
    "    )\n",
    "    ax.set_title(title)\n",
    "    ax.set_ylabel('Proportion')\n",
    "    ax.set_ylim(0, 0.6)\n",
    "plt.suptitle('Stratified Split Verification', fontsize=12, y=0.98)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0715b357",
   "metadata": {},
   "source": [
    "7. Exploratory Data Analysis (EDA)\n",
    "\n",
    "Let op: analyses alleen op de trainingset om data leakage te voorkomen.\n",
    "\n",
    "Doelen:\n",
    "\n",
    "Discriminerende features voor slaapstoornis-classificatie vinden\n",
    "\n",
    "Multicollineariteit detecteren\n",
    "\n",
    "Inzicht in class separability\n",
    "\n",
    "Outlierbehandeling en featureselectie onderbouwen\n",
    "\n",
    "Verwachte patronen:\n",
    "\n",
    "BMI: hoger bij Sleep Apnea (obesitas = belangrijke risicofactor)\n",
    "\n",
    "Hartslag: hoger bij Sleep Apnea (zuurstoftekort → sympathische activatie)\n",
    "\n",
    "Slaapkwaliteit: lager bij beide stoornissen (primaire indicator)\n",
    "\n",
    "Stressniveau: hoger bij Insomnia (hyperarousal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8a445e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Stap 7\n",
    "# ============================================================================\n",
    "\n",
    "# Combine X_train and y_train for analysis\n",
    "df_train = X_train.copy()\n",
    "df_train['Sleep Disorder'] = y_train.values\n",
    "\n",
    "print(f\"Training set: {df_train.shape[0]} samples × {df_train.shape[1]} features\\n\")\n",
    "\n",
    "# --- Per-Class Feature Comparison ---\n",
    "print(\"=\"*80)\n",
    "print(\"Mean Feature Values by Sleep Disorder Class\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "for col in num_cols:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(\"-\"*40)\n",
    "    \n",
    "    class_stats = df_train.groupby('Sleep Disorder')[col].agg(['mean', 'std']).round(2)\n",
    "    display(class_stats)\n",
    "    \n",
    "    # Calculate discriminative power\n",
    "    means = df_train.groupby('Sleep Disorder')[col].mean()\n",
    "    rel_diff = (means.max() - means.min()) / means.mean()\n",
    "    \n",
    "    if rel_diff > 0.2:\n",
    "        print(f\"  ✓ Strong discriminator ({rel_diff*100:.0f}% relative difference)\")\n",
    "    elif rel_diff > 0.1:\n",
    "        print(f\"  → Moderate discriminator ({rel_diff*100:.0f}% relative difference)\")\n",
    "\n",
    "# --- Distributions ---\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Feature Distributions\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "axes = df_train[num_cols].hist(figsize=(15, 12), bins=20, edgecolor='black')\n",
    "plt.suptitle(\"Training Set Feature Distributions\", fontsize=14, y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Correlation Analysis ---\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Correlation Analysis\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "correlation_matrix = df_train[num_cols].corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt='.2f', \n",
    "            square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title(\"Correlation Matrix (Training Set)\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Detect multicollinearity\n",
    "print(\"\\nMulticollinearity check (|r| > 0.7):\")\n",
    "strong_corr = []\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.7:\n",
    "            print(f\"  ⚠ {correlation_matrix.columns[i]} ↔ {correlation_matrix.columns[j]}: \"\n",
    "                  f\"r = {correlation_matrix.iloc[i, j]:.3f}\")\n",
    "            strong_corr.append((correlation_matrix.columns[i], correlation_matrix.columns[j]))\n",
    "\n",
    "if not strong_corr:\n",
    "    print(\"  ✓ No strong correlations detected\")\n",
    "\n",
    "# --- Pairplot (subset of key features) ---\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Pairwise Relationships (Key Features)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "key_features = [\n",
    "    'Sleep Duration', 'Quality of Sleep', 'Stress Level',\n",
    "    'Physical Activity Level', 'Heart Rate', 'BMI'\n",
    "]\n",
    "key_features = [f for f in key_features if f in df_train.columns]\n",
    "\n",
    "sns.pairplot(df_train[key_features + ['Sleep Disorder']], \n",
    "             hue='Sleep Disorder', diag_kind='kde', palette='Set2')\n",
    "plt.suptitle(\"Feature Relationships by Sleep Disorder\", y=1.01, fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ EDA complete - insights will guide outlier treatment and feature selection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ca2286",
   "metadata": {},
   "source": [
    "## Stap 8 – Outlier-detectie en behandeling\n",
    "\n",
    "We gebruiken de **IQR-methode** om outliers te herkennen:\n",
    "\n",
    "- **IQR = Q3 − Q1**\n",
    "- **Lower bound = Q1 − 1.5 × IQR**\n",
    "- **Upper bound = Q3 + 1.5 × IQR**\n",
    "\n",
    "### Waarom niet gewoon outliers verwijderen?\n",
    "In dit project verwijderen we outliers **niet automatisch**. In plaats daarvan gebruiken we een **conservatieve aanpak: capping/winsorization**.  \n",
    "Dat betekent: extreem lage of hoge waarden worden **afgekapt** naar een minimum- of maximumgrens.\n",
    "\n",
    "**Voordeel:** we behouden alle observaties (belangrijk bij medische data), maar verminderen de invloed van onrepresentatieve uitschieters op het model.\n",
    "\n",
    "### Data leakage voorkomen (heel belangrijk)\n",
    "We berekenen outlier-grenzen **alleen op de trainingsdata** en passen die daarna toe op zowel train als test.\n",
    "\n",
    "**Werkwijze:**\n",
    "1. Bereken Q1, Q3 en IQR op `X_train`\n",
    "2. Bepaal lower/upper bounds op basis van die train-statistieken\n",
    "3. Pas capping toe op `X_train`\n",
    "4. Pas **dezelfde** bounds toe op `X_test` (dus **niet** opnieuw berekenen)\n",
    "\n",
    "Zo voorkomen we dat testinformatie per ongeluk het model beïnvloedt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc56110f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STAP 8: OUTLIER DETECTIE EN BEHANDELING \n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"OUTLIER IDENTIFICATIE PER VARIABELE (TRAINING SET)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Maak kopieën om originele data te behouden\n",
    "X_train_clean = X_train.copy()\n",
    "X_test_clean = X_test.copy()\n",
    "\n",
    "# Identificeer numerieke kolommen in X_train\n",
    "num_cols_train = X_train_clean.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "print(f\"\\nNumerieke features in training set: {len(num_cols_train)}\")\n",
    "print(f\"Features: {num_cols_train}\\n\")\n",
    "\n",
    "# Bereken outlier statistics op TRAIN data\n",
    "outlier_info = {}\n",
    "outlier_bounds = {}  # Opslaan voor toepassen op test set\n",
    "\n",
    "for col in num_cols_train:\n",
    "    # Bereken op TRAIN\n",
    "    q1 = X_train_clean[col].quantile(0.25)\n",
    "    q3 = X_train_clean[col].quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    lower = q1 - 1.5 * iqr\n",
    "    upper = q3 + 1.5 * iqr\n",
    "    \n",
    "    # Identificeer outliers in train\n",
    "    mask = (X_train_clean[col] < lower) | (X_train_clean[col] > upper)\n",
    "    outlier_count = int(mask.sum())\n",
    "    outlier_pct = 100 * outlier_count / len(X_train_clean)\n",
    "    \n",
    "    # Opslaan voor rapportage\n",
    "    outlier_info[col] = {\n",
    "        'count': outlier_count,\n",
    "        'pct': float(outlier_pct),\n",
    "        'lower': float(lower),\n",
    "        'upper': float(upper),\n",
    "        'Q1': float(q1),\n",
    "        'Q3': float(q3),\n",
    "        'IQR': float(iqr)\n",
    "    }\n",
    "    \n",
    "    # Opslaan bounds voor test set\n",
    "    outlier_bounds[col] = {'lower': lower, 'upper': upper}\n",
    "\n",
    "outlier_summary = pd.DataFrame.from_dict(outlier_info, orient='index')\n",
    "outlier_summary = outlier_summary.sort_values('pct', ascending=False)\n",
    "\n",
    "print(\"\\nOutlier Samenvatting Training Set (gesorteerd op percentage):\")\n",
    "display(outlier_summary)\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"VISUALISATIE: BOXPLOTS VOOR OUTLIER DETECTIE (TRAINING SET)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "n_cols_plot = len(num_cols_train)\n",
    "n_rows = (n_cols_plot + 2) // 3  # Bereken aantal rijen nodig\n",
    "n_cols = min(3, n_cols_plot)\n",
    "\n",
    "plt.figure(figsize=(16, 4 * n_rows))\n",
    "for i, col in enumerate(num_cols_train, 1):\n",
    "    plt.subplot(n_rows, n_cols, i)\n",
    "    sns.boxplot(x=X_train_clean[col], color='skyblue')\n",
    "    title = f\"{col}\\n({outlier_summary.loc[col, 'count']:.0f} outliers, {outlier_summary.loc[col, 'pct']:.1f}%)\"\n",
    "    plt.title(title, fontsize=10)\n",
    "    plt.xlabel('')\n",
    "plt.suptitle(\"Boxplots - Outlier Detectie (Training Set)\", fontsize=14, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# STAP 8B: OUTLIER BEHANDELING - CAPPING/WINSORIZATION\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"IMPLEMENTATIE VAN BEHANDELING (CAPPING / WINSORIZATION)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Strategie 1: Winsorization voor Daily Steps (1% - 99%)\n",
    "if 'Daily Steps' in X_train_clean.columns:\n",
    "    print(\"\\n1. DAILY STEPS - Winsorization / Capping (1% - 99%)\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # TRAIN: bereken percentiles\n",
    "    low_p_train = X_train_clean['Daily Steps'].quantile(0.01)\n",
    "    high_p_train = X_train_clean['Daily Steps'].quantile(0.99)\n",
    "    \n",
    "    # Toon originele ranges\n",
    "    original_min_train = X_train_clean['Daily Steps'].min()\n",
    "    original_max_train = X_train_clean['Daily Steps'].max()\n",
    "    original_min_test = X_test_clean['Daily Steps'].min()\n",
    "    original_max_test = X_test_clean['Daily Steps'].max()\n",
    "    \n",
    "    print(f\"   TRAIN originele range: {original_min_train:.0f} - {original_max_train:.0f} stappen\")\n",
    "    print(f\"   TEST originele range: {original_min_test:.0f} - {original_max_test:.0f} stappen\")\n",
    "    print(f\"   Capping grenzen (van train): {low_p_train:.0f} - {high_p_train:.0f} stappen\")\n",
    "    \n",
    "    # Pas capping toe op TRAIN\n",
    "    X_train_clean['Daily Steps'] = np.clip(X_train_clean['Daily Steps'], low_p_train, high_p_train)\n",
    "    \n",
    "    # Pas DEZELFDE grenzen toe op TEST\n",
    "    X_test_clean['Daily Steps'] = np.clip(X_test_clean['Daily Steps'], low_p_train, high_p_train)\n",
    "    \n",
    "    print(f\"   ✓ Capping toegepast op train ({len(X_train_clean)} samples)\")\n",
    "    print(f\"   ✓ DEZELFDE grenzen toegepast op test ({len(X_test_clean)} samples)\")\n",
    "    print(\"   Rationale: Extreme waarden kunnen tracking fouten zijn. Winsorization behoudt data maar beperkt invloed.\")\n",
    "\n",
    "# Strategie 2: Heart Rate - capping naar plausibele range (40 - 120 bpm)\n",
    "if 'Heart Rate' in X_train_clean.columns:\n",
    "    print(\"\\n2. HEART RATE - Capping naar plausibele range (40 - 120 bpm)\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Medische plausibiliteitsgrenzen (domeinkennis)\n",
    "    hr_lower, hr_upper = 40, 120\n",
    "    \n",
    "    # Toon originele ranges\n",
    "    original_hr_min_train = X_train_clean['Heart Rate'].min()\n",
    "    original_hr_max_train = X_train_clean['Heart Rate'].max()\n",
    "    original_hr_min_test = X_test_clean['Heart Rate'].min()\n",
    "    original_hr_max_test = X_test_clean['Heart Rate'].max()\n",
    "    \n",
    "    print(f\"   TRAIN originele hartslag: {original_hr_min_train:.0f} - {original_hr_max_train:.0f} bpm\")\n",
    "    print(f\"   TEST originele hartslag: {original_hr_min_test:.0f} - {original_hr_max_test:.0f} bpm\")\n",
    "    print(f\"   Capping grenzen (medisch plausibel): {hr_lower} - {hr_upper} bpm\")\n",
    "    \n",
    "    # Pas capping toe op TRAIN\n",
    "    X_train_clean['Heart Rate'] = X_train_clean['Heart Rate'].clip(lower=hr_lower, upper=hr_upper)\n",
    "    \n",
    "    # Pas DEZELFDE grenzen toe op TEST\n",
    "    X_test_clean['Heart Rate'] = X_test_clean['Heart Rate'].clip(lower=hr_lower, upper=hr_upper)\n",
    "    \n",
    "    print(f\"   ✓ Capping toegepast op train ({len(X_train_clean)} samples)\")\n",
    "    print(f\"   ✓ DEZELFDE grenzen toegepast op test ({len(X_test_clean)} samples)\")\n",
    "    print(\"   Rationale: Clipping behoudt observaties maar vermindert invloed van onrealistische uitschieters.\")\n",
    "\n",
    "# Strategie 3: Plausibiliteitscontroles (geen verwijdering, enkel rapportage)\n",
    "print(\"\\n3. ANDERE VARIABELEN - Plausibiliteitscheck (geen verwijdering)\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Sleep Duration (we behouden, want korte/lange slaap kan klinisch relevant zijn)\n",
    "if 'Sleep Duration' in X_train_clean.columns:\n",
    "    invalid_sleep_train = (X_train_clean['Sleep Duration'] < 0) | (X_train_clean['Sleep Duration'] > 24)\n",
    "    invalid_sleep_test = (X_test_clean['Sleep Duration'] < 0) | (X_test_clean['Sleep Duration'] > 24)\n",
    "    \n",
    "    if invalid_sleep_train.sum() > 0 or invalid_sleep_test.sum() > 0:\n",
    "        print(f\"   ⚠ Sleep Duration: {invalid_sleep_train.sum()} train + {invalid_sleep_test.sum()} test waarden buiten 0-24 uur\")\n",
    "    else:\n",
    "        print(\"   ✓ Sleep Duration: alle waarden binnen 0-24 uur (train + test)\")\n",
    "\n",
    "# BMI (optioneel)\n",
    "if 'BMI' in X_train_clean.columns:\n",
    "    invalid_bmi_train = (X_train_clean['BMI'] < 10) | (X_train_clean['BMI'] > 70)\n",
    "    invalid_bmi_test = (X_test_clean['BMI'] < 10) | (X_test_clean['BMI'] > 70)\n",
    "    \n",
    "    if invalid_bmi_train.sum() > 0 or invalid_bmi_test.sum() > 0:\n",
    "        print(f\"   ⚠ BMI: {invalid_bmi_train.sum()} train + {invalid_bmi_test.sum()} test waarden buiten 10-70\")\n",
    "    else:\n",
    "        print(\"   ✓ BMI waarden binnen plausibel bereik (train + test)\")\n",
    "\n",
    "# Age\n",
    "if 'Age' in X_train_clean.columns:\n",
    "    invalid_age_train = (X_train_clean['Age'] < 18) | (X_train_clean['Age'] > 100)\n",
    "    invalid_age_test = (X_test_clean['Age'] < 18) | (X_test_clean['Age'] > 100)\n",
    "    \n",
    "    if invalid_age_train.sum() > 0 or invalid_age_test.sum() > 0:\n",
    "        print(f\"   ⚠ Age: {invalid_age_train.sum()} train + {invalid_age_test.sum()} test waarden buiten 18-100 jaar\")\n",
    "    else:\n",
    "        print(\"   ✓ Age waarden binnen plausibel bereik (train + test)\")\n",
    "\n",
    "# ============================================================================\n",
    "# STAP 8C: RESULTAAT VAN OUTLIER BEHANDELING\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"RESULTAAT VAN OUTLIER BEHANDELING (CAPPING / WINSORIZATION)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# We hebben geen rijen verwijderd; we hebben waarden gecapped/winsorized\n",
    "rows_removed = 0\n",
    "pct_removed = 0.0\n",
    "\n",
    "print(f\"\\nTRAINING SET:\")\n",
    "print(f\"  • Originele grootte: {X_train.shape[0]} rijen × {X_train.shape[1]} kolommen\")\n",
    "print(f\"  • Na behandeling: {X_train_clean.shape[0]} rijen × {X_train_clean.shape[1]} kolommen\")\n",
    "print(f\"  • Verwijderd: {rows_removed} rijen ({pct_removed:.2f}%)\")\n",
    "\n",
    "print(f\"\\nTEST SET:\")\n",
    "print(f\"  • Originele grootte: {X_test.shape[0]} rijen × {X_test.shape[1]} kolommen\")\n",
    "print(f\"  • Na behandeling: {X_test_clean.shape[0]} rijen × {X_test_clean.shape[1]} kolommen\")\n",
    "print(f\"  • Verwijderd: {rows_removed} rijen ({pct_removed:.2f}%)\")\n",
    "\n",
    "print(\"\\n✓ Geen rijen verwijderd - we hebben capping toegepast op extreme waarden\")\n",
    "print(\"✓ Outlier grenzen bepaald op TRAIN en toegepast op BEIDE sets (geen data leakage)\")\n",
    "print(\"✓ Extreme waarden beperkt via winsorization/clipping\")\n",
    "\n",
    "# Update de variabelen voor gebruik in volgende stappen\n",
    "X_train = X_train_clean\n",
    "X_test = X_test_clean\n",
    "\n",
    "print(\"\\n X_train en X_test zijn nu updated met outlier behandeling\")\n",
    "print(\"   Deze worden gebruikt in volgende stappen (encoding, scaling)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40589c25",
   "metadata": {},
   "source": [
    "## Stap 9 – Feature encoding (heel kort)\n",
    "\n",
    "Modellen hebben **numerieke** input nodig, dus we zetten categorische data om.\n",
    "\n",
    "- **Blood Pressure**: niet one-hot (te veel unieke waarden), maar splitsen:\n",
    "  - `\"120/80\"` → `Systolic=120`, `Diastolic=80`\n",
    "\n",
    "- **Categorische features**: **one-hot encoding** met `drop_first=True`.\n",
    "  - Train en test moeten dezelfde kolommen hebben → `get_dummies()` + `align()`.\n",
    "\n",
    "- **Target (y)**: **LabelEncoder**\n",
    "  - `fit` op `y_train`, `transform` op `y_train` én `y_test`.\n",
    "\n",
    "**Belangrijk:** alles **fit op train**, daarna **toepassen op test** → geen data leakage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf95802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STAP 9: FEATURE ENCODING - CATEGORISCHE VARIABELEN TRANSFORMEREN\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def split_blood_pressure(df):\n",
    "    \"\"\"Split Blood Pressure kolom in Systolic en Diastolic\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Split \"120/80\" in twee delen\n",
    "    bp_split = df['Blood Pressure'].str.split('/', expand=True)\n",
    "    \n",
    "    # Converteer naar integers\n",
    "    df['Systolic'] = bp_split[0].astype(int)\n",
    "    df['Diastolic'] = bp_split[1].astype(int)\n",
    "    \n",
    "    # Verwijder originele Blood Pressure kolom\n",
    "    df = df.drop('Blood Pressure', axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(f\"\\nVOOR transformatie:\")\n",
    "print(f\"  • X_train shape: {X_train.shape}\")\n",
    "print(f\"  • X_test shape: {X_test.shape}\")\n",
    "print(f\"  • Blood Pressure unieke waarden in train: {X_train['Blood Pressure'].nunique()}\")\n",
    "print(f\"  • Blood Pressure unieke waarden in test: {X_test['Blood Pressure'].nunique()}\")\n",
    "\n",
    "print(\"\\nVoorbeeld Blood Pressure waarden:\")\n",
    "print(X_train['Blood Pressure'].head(10).tolist())\n",
    "\n",
    "# Voer transformatie uit\n",
    "X_train = split_blood_pressure(X_train)\n",
    "X_test = split_blood_pressure(X_test)\n",
    "\n",
    "print(f\"\\nNA transformatie:\")\n",
    "print(f\"  • X_train shape: {X_train.shape}\")\n",
    "print(f\"  • X_test shape: {X_test.shape}\")\n",
    "\n",
    "print(\"\\nNieuwe kolommen toegevoegd:\")\n",
    "print(f\"  • Systolic - Range train: [{X_train['Systolic'].min()}, {X_train['Systolic'].max()}]\")\n",
    "print(f\"  • Systolic - Range test: [{X_test['Systolic'].min()}, {X_test['Systolic'].max()}]\")\n",
    "print(f\"  • Diastolic - Range train: [{X_train['Diastolic'].min()}, {X_train['Diastolic'].max()}]\")\n",
    "print(f\"  • Diastolic - Range test: [{X_test['Diastolic'].min()}, {X_test['Diastolic'].max()}]\")\n",
    "\n",
    "print(\"\\nVoorbeeld eerste 5 rijen:\")\n",
    "print(X_train[['Systolic', 'Diastolic']].head())\n",
    "\n",
    "print(\"\\n✓ Blood Pressure succesvol gesplitst in 2 numerieke features\")\n",
    "\n",
    "# ============================================================================\n",
    "# STAP 9B: IDENTIFICEER RESTERENDE CATEGORISCHE FEATURES\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"IDENTIFICATIE VAN CATEGORISCHE FEATURES\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Identificeer categorische kolommen in X_train\n",
    "cat_cols_train = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(f\"\\nCategorische features in training set ({len(cat_cols_train)}):\")\n",
    "for col in cat_cols_train:\n",
    "    unique_vals_train = X_train[col].nunique()\n",
    "    unique_vals_test = X_test[col].nunique()\n",
    "    print(f\"  • {col}:\")\n",
    "    print(f\"    - Train: {unique_vals_train} categorieën → {X_train[col].unique().tolist()}\")\n",
    "    print(f\"    - Test: {unique_vals_test} categorieën → {X_test[col].unique().tolist()}\")\n",
    "\n",
    "# Check voor nieuwe categorieën in test set (kunnen problemen geven)\n",
    "print(\"\\n CONTROLE: Nieuwe categorieën in test set?\")\n",
    "for col in cat_cols_train:\n",
    "    train_cats = set(X_train[col].unique())\n",
    "    test_cats = set(X_test[col].unique())\n",
    "    new_cats = test_cats - train_cats\n",
    "    \n",
    "    if new_cats:\n",
    "        print(f\"   {col}: Test heeft nieuwe categorieën: {new_cats}\")\n",
    "        print(f\"     → Deze worden behandeld als 'unknown' tijdens encoding\")\n",
    "    else:\n",
    "        print(f\"  ✓ {col}: Geen nieuwe categorieën in test\")\n",
    "\n",
    "# ============================================================================\n",
    "# STAP 9C: ONE-HOT ENCODING VOOR FEATURES\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\nVOOR encoding:\")\n",
    "print(f\"  • X_train: {X_train.shape[0]} rijen × {X_train.shape[1]} kolommen\")\n",
    "print(f\"  • X_test: {X_test.shape[0]} rijen × {X_test.shape[1]} kolommen\")\n",
    "\n",
    "# One-hot encoding op TRAIN\n",
    "X_train_encoded = pd.get_dummies(X_train, columns=cat_cols_train, drop_first=True)\n",
    "\n",
    "# One-hot encoding op TEST (met dezelfde kolommen)\n",
    "X_test_encoded = pd.get_dummies(X_test, columns=cat_cols_train, drop_first=True)\n",
    "\n",
    "print(f\"\\nNA encoding (voor align):\")\n",
    "print(f\"  • X_train_encoded: {X_train_encoded.shape[0]} rijen × {X_train_encoded.shape[1]} kolommen\")\n",
    "print(f\"  • X_test_encoded: {X_test_encoded.shape[0]} rijen × {X_test_encoded.shape[1]} kolommen\")\n",
    "\n",
    "# BELANGRIJK: Align zodat beide sets dezelfde kolommen hebben\n",
    "# Kolommen die in train maar niet in test zitten → voeg toe aan test met 0\n",
    "# Kolommen die in test maar niet in train zitten → verwijder uit test\n",
    "X_train_encoded, X_test_encoded = X_train_encoded.align(X_test_encoded, join='left', axis=1, fill_value=0)\n",
    "\n",
    "print(f\"\\nNA align (train en test hebben nu IDENTIEKE kolommen):\")\n",
    "print(f\"  • X_train_encoded: {X_train_encoded.shape[0]} rijen × {X_train_encoded.shape[1]} kolommen\")\n",
    "print(f\"  • X_test_encoded: {X_test_encoded.shape[0]} rijen × {X_test_encoded.shape[1]} kolommen\")\n",
    "\n",
    "# Toon nieuwe kolommen\n",
    "new_cols = [col for col in X_train_encoded.columns if col not in X_train.columns]\n",
    "print(f\"\\nNieuwe dummy kolommen gecreëerd ({len(new_cols)}):\")\n",
    "for col in new_cols[:15]:  # Toon eerste 15\n",
    "    print(f\"  • {col}\")\n",
    "if len(new_cols) > 15:\n",
    "    print(f\"  ... en {len(new_cols) - 15} meer\")\n",
    "\n",
    "print(\"\\n✓ One-hot encoding voltooid\")\n",
    "print(\"✓ Train en test hebben identieke kolommen\")\n",
    "\n",
    "# ============================================================================\n",
    "# STAP 9D: LABEL ENCODING VOOR TARGET VARIABELE\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "print(f\"\\nVOOR encoding:\")\n",
    "print(f\"  • y_train: {y_train.shape[0]} samples, type: {y_train.dtype}\")\n",
    "print(f\"  • y_test: {y_test.shape[0]} samples, type: {y_test.dtype}\")\n",
    "print(f\"\\nUnieke klassen in y_train: {sorted(y_train.unique())}\")\n",
    "print(f\"Unieke klassen in y_test: {sorted(y_test.unique())}\")\n",
    "\n",
    "# Label encoding voor target\n",
    "le = LabelEncoder()\n",
    "\n",
    "# FIT op y_train (encoder leert de klassen van train data)\n",
    "le.fit(y_train)\n",
    "\n",
    "# TRANSFORM beide sets met gefitte encoder\n",
    "y_train_encoded = le.transform(y_train)\n",
    "y_test_encoded = le.transform(y_test)\n",
    "\n",
    "# Converteer naar pandas Series voor consistentie\n",
    "y_train_encoded = pd.Series(y_train_encoded, index=y_train.index, name='Sleep Disorder')\n",
    "y_test_encoded = pd.Series(y_test_encoded, index=y_test.index, name='Sleep Disorder')\n",
    "\n",
    "print(f\"\\nNA encoding:\")\n",
    "print(f\"  • y_train_encoded: {y_train_encoded.shape[0]} samples, type: {y_train_encoded.dtype}\")\n",
    "print(f\"  • y_test_encoded: {y_test_encoded.shape[0]} samples, type: {y_test_encoded.dtype}\")\n",
    "\n",
    "# Toon mapping\n",
    "label_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "print(\"\\nLabel Encoding Mapping:\")\n",
    "for original, encoded in sorted(label_mapping.items(), key=lambda x: x[1]):\n",
    "    count_train = (y_train_encoded == encoded).sum()\n",
    "    count_test = (y_test_encoded == encoded).sum()\n",
    "    print(f\"  '{original}' → {encoded}\")\n",
    "    print(f\"    - Train: {count_train} samples ({100*count_train/len(y_train_encoded):.1f}%)\")\n",
    "    print(f\"    - Test: {count_test} samples ({100*count_test/len(y_test_encoded):.1f}%)\")\n",
    "\n",
    "print(\"\\nRationale: Label encoding voor target is noodzakelijk voor sklearn classificatie.\")\n",
    "print(\"De numerieke waarden hebben geen ordinale betekenis in multiclass setting.\")\n",
    "\n",
    "# ============================================================================\n",
    "# STAP 9E: VERIFICATIE EN SAMENVATTING\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"VERIFICATIE EN SAMENVATTING\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(\"\\nEerste 5 rijen TRAINING set (features + target):\")\n",
    "display(pd.concat([X_train_encoded.head(), y_train_encoded.head()], axis=1))\n",
    "\n",
    "print(\"\\nEerste 5 rijen TEST set (features + target):\")\n",
    "display(pd.concat([X_test_encoded.head(), y_test_encoded.head()], axis=1))\n",
    "\n",
    "print(\"\\nKolomtypes in X_train_encoded:\")\n",
    "print(X_train_encoded.dtypes.value_counts())\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ FEATURE ENCODING VOLTOOID\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nSamenvatting:\")\n",
    "print(f\"  • Blood Pressure: Gesplitst in Systolic + Diastolic (2 numerieke features)\")\n",
    "print(f\"  • Categorische features: {len(cat_cols_train)} → One-hot encoded\")\n",
    "print(f\"  • Train features: {X_train.shape[1]} → {X_train_encoded.shape[1]} kolommen\")\n",
    "print(f\"  • Test features: {X_test.shape[1]} → {X_test_encoded.shape[1]} kolommen\")\n",
    "print(f\"  • Target variabele: Label encoded (3 klassen)\")\n",
    "print(f\"  • Train samples: {X_train_encoded.shape[0]}\")\n",
    "print(f\"  • Test samples: {X_test_encoded.shape[0]}\")\n",
    "\n",
    "print(\"\\n BELANGRIJK: Encoder fit op TRAIN, transform op TEST\")\n",
    "print(\"   Dit voorkomt data leakage!\")\n",
    "\n",
    "print(\"\\nReductie in features:\")\n",
    "print(f\"  • Blood Pressure zou ~{X_train['Blood Pressure'].nunique() if 'Blood Pressure' in X_train.columns else 100} dummies geven\")\n",
    "print(f\"  • Door splitsing: slechts 2 numerieke features (Systolic, Diastolic)\")\n",
    "print(f\"  • Besparing: ~{100-2} features!\")\n",
    "\n",
    "# Update variabelen voor volgende stappen\n",
    "X_train = X_train_encoded\n",
    "X_test = X_test_encoded\n",
    "y_train = y_train_encoded\n",
    "y_test = y_test_encoded\n",
    "\n",
    "print(\"\\n✓ X_train, X_test, y_train, y_test zijn nu ge-encoded\")\n",
    "print(\"   Deze worden gebruikt in volgende stappen (scaling, modeling)\")\n",
    "\n",
    "# Check correlatie tussen Systolic en Diastolic\n",
    "if 'Systolic' in X_train.columns and 'Diastolic' in X_train.columns:\n",
    "    corr = X_train[['Systolic', 'Diastolic']].corr().iloc[0, 1]\n",
    "    print(f\"\\n CORRELATIE CHECK:\")\n",
    "    print(f\"   Systolic ↔ Diastolic: r = {corr:.3f}\")\n",
    "    if abs(corr) > 0.9:\n",
    "        print(f\"    Hoge correlatie gedetecteerd (|r| > 0.9)\")\n",
    "        print(f\"   → Overweeg één van beide te verwijderen om multicollineariteit te reduceren\")\n",
    "    else:\n",
    "        print(f\"   ✓ Correlatie acceptabel\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"READY VOOR STAP 10: FEATURE SCALING\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b46b5e3",
   "metadata": {},
   "source": [
    "## Stap 10 – Feature scaling (heel kort)\n",
    "\n",
    "Features hebben verschillende schalen (bv. *Daily Steps* vs *Heart Rate*). Zonder scaling kunnen sommige modellen hierdoor “gedomineerd” worden.\n",
    "\n",
    "**Keuze:** `StandardScaler` (Z-score)  \n",
    "- Formule: `x_scaled = (x - μ) / σ`  \n",
    "- Resultaat op train: mean ≈ 0 en std ≈ 1\n",
    "\n",
    "**Data leakage voorkomen (cruciaal):**\n",
    "1. `scaler.fit(X_train)` → leert μ en σ **alleen** van train  \n",
    "2. `X_train_scaled = scaler.transform(X_train)`  \n",
    "3. `X_test_scaled = scaler.transform(X_test)` → gebruikt **train** μ en σ (dus geen `fit` op test)\n",
    "\n",
    "**Let op:** test hoeft na scaling niet exact mean=0/std=1 te hebben — dat is normaal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c37583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STAP 10: FEATURE SCALING - NORMALISATIE EN STANDAARDISATIE\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"HUIDIGE DATA STATUS\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(f\"\\nTRAINING SET:\")\n",
    "print(f\"  • X_train shape: {X_train.shape[0]} samples × {X_train.shape[1]} features\")\n",
    "print(f\"  • y_train shape: {y_train.shape[0]} samples\")\n",
    "\n",
    "print(f\"\\nTEST SET:\")\n",
    "print(f\"  • X_test shape: {X_test.shape[0]} samples × {X_test.shape[1]} features\")\n",
    "print(f\"  • y_test shape: {y_test.shape[0]} samples\")\n",
    "\n",
    "print(f\"\\nFeature kolommen ({len(X_train.columns)}):\")\n",
    "# Toon eerste 15 kolommen\n",
    "cols_to_show = X_train.columns.tolist()[:15]\n",
    "for col in cols_to_show:\n",
    "    print(f\"  • {col}\")\n",
    "if len(X_train.columns) > 15:\n",
    "    print(f\"  ... en {len(X_train.columns) - 15} meer kolommen\")\n",
    "\n",
    "# ============================================================================\n",
    "# STAP 10A: SCALING OP TRAINING SET\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"STAP 10A: STANDARDISATIE OP TRAINING SET\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Toon voorbeeld van niet-geschaalde data (TRAIN)\n",
    "print(\"\\nVoorbeeld TRAIN features VOOR scaling:\")\n",
    "display(X_train.head())\n",
    "\n",
    "print(\"\\nDescriptive statistics TRAIN VOOR scaling:\")\n",
    "display(X_train.describe())\n",
    "\n",
    "# Initialiseer scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# FIT scaler op TRAIN data (leert mean en std van train)\n",
    "scaler.fit(X_train)\n",
    "\n",
    "print(\"\\n Scaler Statistics (geleerd van TRAIN data):\")\n",
    "print(f\"  • Mean per feature (eerste 5): {scaler.mean_[:5]}\")\n",
    "print(f\"  • Std per feature (eerste 5): {scaler.scale_[:5]}\")\n",
    "\n",
    "# TRANSFORM train data met gefitte scaler\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "\n",
    "# Converteer terug naar DataFrame voor visualisatie\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "\n",
    "print(\"\\nVoorbeeld TRAIN features NA scaling:\")\n",
    "display(X_train_scaled.head())\n",
    "\n",
    "print(\"\\nDescriptive statistics TRAIN NA scaling:\")\n",
    "display(X_train_scaled.describe())\n",
    "\n",
    "print(\"\"\"\n",
    "INTERPRETATIE TRAIN:\n",
    "====================\n",
    "- Mean ≈ 0 (kleine floating point errors acceptabel)\n",
    "- Std ≈ 1 voor alle features\n",
    "- Dit is verwacht omdat we scaler gefitted hebben op train data\n",
    "\"\"\")\n",
    "\n",
    "# ============================================================================\n",
    "# STAP 10B: SCALING OP TEST SET (MET TRAIN STATISTICS)\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "# Toon voorbeeld van niet-geschaalde data (TEST)\n",
    "print(\"\\nVoorbeeld TEST features VOOR scaling:\")\n",
    "display(X_test.head())\n",
    "\n",
    "print(\"\\nDescriptive statistics TEST VOOR scaling:\")\n",
    "display(X_test.describe())\n",
    "\n",
    "# TRANSFORM test data met TRAIN scaler (GEEN fit!)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Converteer terug naar DataFrame voor visualisatie\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "print(\"\\nVoorbeeld TEST features NA scaling:\")\n",
    "display(X_test_scaled.head())\n",
    "\n",
    "print(\"\\nDescriptive statistics TEST NA scaling:\")\n",
    "display(X_test_scaled.describe())\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# STAP 10C: VERIFICATIE EN SAMENVATTING\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"VERIFICATIE VAN SCALING\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Controleer of alle features geschaald zijn\n",
    "print(\"\\nControle: Zijn alle features geschaald?\")\n",
    "print(\"\\nTRAIN set - Mean en Std per feature (eerste 10):\")\n",
    "train_stats = pd.DataFrame({\n",
    "    'Mean': X_train_scaled.mean(),\n",
    "    'Std': X_train_scaled.std()\n",
    "}).head(10)\n",
    "display(train_stats)\n",
    "\n",
    "print(\"\\nTEST set - Mean en Std per feature (eerste 10):\")\n",
    "test_stats = pd.DataFrame({\n",
    "    'Mean': X_test_scaled.mean(),\n",
    "    'Std': X_test_scaled.std()\n",
    "}).head(10)\n",
    "display(test_stats)\n",
    "\n",
    "print(\"\\n✓ TRAIN: Mean ≈ 0, Std ≈ 1 (exact, want scaler gefitted op train)\")\n",
    "print(\" TEST: Mean en Std kunnen afwijken (normaal, want geschaald met train statistics)\")\n",
    "\n",
    "# ============================================================================\n",
    "# STAP 10D: VISUALISATIE VAN SCALING EFFECT\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"VISUALISATIE: VOOR vs NA SCALING\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Selecteer een paar features om te visualiseren\n",
    "features_to_plot = [col for col in ['Sleep Duration', 'Heart Rate', 'Daily Steps', 'BMI', 'Age'] \n",
    "                    if col in X_train.columns][:4]\n",
    "\n",
    "if len(features_to_plot) > 0:\n",
    "    fig, axes = plt.subplots(2, len(features_to_plot), figsize=(16, 8))\n",
    "    \n",
    "    for i, feature in enumerate(features_to_plot):\n",
    "        # Voor scaling (train)\n",
    "        axes[0, i].hist(X_train[feature], bins=20, edgecolor='black', alpha=0.7)\n",
    "        axes[0, i].set_title(f'{feature}\\nVOOR scaling (Train)')\n",
    "        axes[0, i].set_xlabel('Waarde')\n",
    "        axes[0, i].set_ylabel('Frequentie')\n",
    "        \n",
    "        # Na scaling (train)\n",
    "        axes[1, i].hist(X_train_scaled[feature], bins=20, edgecolor='black', alpha=0.7, color='green')\n",
    "        axes[1, i].set_title(f'{feature}\\nNA scaling (Train)')\n",
    "        axes[1, i].set_xlabel('Z-score')\n",
    "        axes[1, i].set_ylabel('Frequentie')\n",
    "        axes[1, i].axvline(0, color='red', linestyle='--', label='Mean=0')\n",
    "    \n",
    "    plt.suptitle('Effect van Standardisatie op Features (Training Set)', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Geen numerieke features gevonden om te plotten\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ FEATURE SCALING VOLTOOID\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nSamenvatting:\")\n",
    "print(f\"  • Scaling methode: StandardScaler (Z-score normalisatie)\")\n",
    "print(f\"  • Scaler fitted op: TRAIN data ({X_train.shape[0]} samples)\")\n",
    "print(f\"  • Train features geschaald: {X_train_scaled.shape[1]} features\")\n",
    "print(f\"  • Test features geschaald: {X_test_scaled.shape[1]} features\")\n",
    "print(f\"  • Train mean ≈ 0, std ≈ 1: ✓\")\n",
    "print(f\"  • Test geschaald met TRAIN statistics: ✓\")\n",
    "print(f\"  • Data leakage voorkomen: ✓\")\n",
    "\n",
    "\n",
    "\n",
    "# Update variabelen voor volgende stappen\n",
    "X_train = X_train_scaled\n",
    "X_test = X_test_scaled\n",
    "\n",
    "print(\"\\n✓ X_train en X_test zijn nu geschaald en klaar voor modeling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52d7e80",
   "metadata": {},
   "source": [
    "## Stap 11 – Feature selection & engineering (kort)\n",
    "\n",
    "Voor de **baseline** houden we **alle features** na encoding (numeriek + one-hot).\n",
    "\n",
    "**Waarom alles behouden?**\n",
    "- Laat het **model** bepalen wat belangrijk is (data-driven).\n",
    "- We willen een eerlijke vergelijking: **Baseline (alle features)** vs **Optimized (geselecteerd)**.\n",
    "- Correlaties zijn bekend, maar we beslissen pas na model-evaluatie.\n",
    "\n",
    "**Plan in ML-fase:**\n",
    "1. **Baseline modellen** trainen met alle features (bv. RF/XGBoost/LogReg)\n",
    "2. **Feature importance** analyseren en zwakke features identificeren\n",
    "3. **Multicollineariteit aanpakken** (bv. bij `Systolic` vs `Diastolic` r≈0.98 → mogelijk `Diastolic` droppen)\n",
    "4. **Optimized modellen** retrainen en metrics vergelijken (Accuracy/F1/Precision/Recall)\n",
    "5. Optioneel: **feature engineering** als performance tegenvalt\n",
    "\n",
    "**Check:**\n",
    "- Samples/features-ratio is ruim voldoende → geen dimensionality-probleem.\n",
    "- Pipeline ok: **geen missing/infinite**, alles numeriek, encoding/scaling/outliers gedaan, **geen leakage** (fit train → transform test).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b697ab82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STAP 11: FEATURE SELECTION & ENGINEERING - OVERWEGINGEN\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"FINALE FEATURE SET VOOR BASELINE MODELING\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(f\"\\nTRAINING SET:\")\n",
    "print(f\"  • Samples: {X_train.shape[0]:,}\")\n",
    "print(f\"  • Features: {X_train.shape[1]}\")\n",
    "print(f\"  • Samples/Features ratio: {X_train.shape[0] / X_train.shape[1]:,.1f}:1\")\n",
    "\n",
    "print(f\"\\nTEST SET:\")\n",
    "print(f\"  • Samples: {X_test.shape[0]:,}\")\n",
    "print(f\"  • Features: {X_test.shape[1]}\")\n",
    "\n",
    "print(f\"\\nFeature lijst ({X_train.shape[1]} features):\")\n",
    "\n",
    "# Groepeer features\n",
    "original_numeric = [col for col in X_train.columns if not any(\n",
    "    prefix in col for prefix in ['Gender_', 'Occupation_', 'BMI Category_']\n",
    ")]\n",
    "gender_features = [col for col in X_train.columns if col.startswith('Gender_')]\n",
    "occupation_features = [col for col in X_train.columns if col.startswith('Occupation_')]\n",
    "bmi_cat_features = [col for col in X_train.columns if col.startswith('BMI Category_')]\n",
    "\n",
    "print(\"\\n1. NUMERIEKE FEATURES:\")\n",
    "for i, feature in enumerate(original_numeric, 1):\n",
    "    print(f\"   {i:2d}. {feature}\")\n",
    "\n",
    "if gender_features:\n",
    "    print(f\"\\n2. GENDER FEATURES ({len(gender_features)}):\")\n",
    "    for i, feature in enumerate(gender_features, 1):\n",
    "        print(f\"   {i:2d}. {feature}\")\n",
    "\n",
    "if occupation_features:\n",
    "    print(f\"\\n3. OCCUPATION FEATURES ({len(occupation_features)}):\")\n",
    "    for i, feature in enumerate(occupation_features, 1):\n",
    "        print(f\"   {i:2d}. {feature}\")\n",
    "\n",
    "if bmi_cat_features:\n",
    "    print(f\"\\n4. BMI CATEGORY FEATURES ({len(bmi_cat_features)}):\")\n",
    "    for i, feature in enumerate(bmi_cat_features, 1):\n",
    "        print(f\"   {i:2d}. {feature}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"DATA QUALITY FINAL CHECK\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(f\"\\n✓ Missing values: {X_train.isnull().sum().sum()} (train) + {X_test.isnull().sum().sum()} (test)\")\n",
    "print(f\"✓ Infinite values: {np.isinf(X_train.values).sum()} (train) + {np.isinf(X_test.values).sum()} (test)\")\n",
    "print(f\"✓ Data types: All numeric ({X_train.select_dtypes(include=[np.number]).shape[1]}/{X_train.shape[1]} features)\")\n",
    "print(f\"✓ Scaling: Completed (StandardScaler, fit on train)\")\n",
    "print(f\"✓ Encoding: Completed (One-hot + Label encoding)\")\n",
    "print(f\"✓ Outliers: Treated (conservative capping)\")\n",
    "print(f\"✓ Data leakage: Prevented (fit train → transform test)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ DATA PREPROCESSING PIPELINE VOLTOOID\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    " DATA IS KLAAR VOOR MODEL TRAINING!\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad22e870",
   "metadata": {},
   "source": [
    "## ML-uitwerking – Stap 1: Baseline models (kort)\n",
    "\n",
    "We trainen **baseline modellen** om:\n",
    "- eerste performance te meten,\n",
    "- feature importance te bekijken,\n",
    "- een referentie te hebben voor latere optimalisatie.\n",
    "\n",
    "### Modellen\n",
    "- **Random Forest (RF)**: robuust, kan non-lineaire relaties aan, geeft feature importance.\n",
    "- **Logistic Regression (LR)**: simpel en interpreteerbaar, maar gevoelig voor multicollineariteit.\n",
    "\n",
    "### Kritieke aanpassing (multicollineariteit)\n",
    "Er is **extreme correlatie** tussen **Systolic** en **Diastolic** (r ≈ 0.979).\n",
    "- **RF**: geen probleem → trainen met **alle features**.\n",
    "- **LR**: kan instabiel worden / slecht convergeren → trainen **zonder `Diastolic`**.\n",
    "\n",
    "### Pre-check\n",
    "We berekenen de correlatie tussen `Systolic` en `Diastolic`.\n",
    "Als `|r| > 0.95`, dan zetten we een flag om `Diastolic` uit de LR feature set te verwijderen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e386e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# UITWERKING ML VRAAGSTUK\n",
    "# STAP 1: BASELINE MODELS - TRAINING & EVALUATIE\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# IMPORT LIBRARIES\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"IMPORT REQUIRED LIBRARIES\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")\n",
    "\n",
    "# ============================================================================\n",
    "# VERIFY DATA STATUS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"DATA STATUS VERIFICATION\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(f\"\\nTRAINING SET:\")\n",
    "print(f\"  • X_train shape: {X_train.shape}\")\n",
    "print(f\"  • y_train shape: {y_train.shape}\")\n",
    "print(f\"  • Features: {X_train.shape[1]}\")\n",
    "print(f\"  • Samples: {X_train.shape[0]:,}\")\n",
    "\n",
    "print(f\"\\nTEST SET:\")\n",
    "print(f\"  • X_test shape: {X_test.shape}\")\n",
    "print(f\"  • y_test shape: {y_test.shape}\")\n",
    "print(f\"  • Features: {X_test.shape[1]}\")\n",
    "print(f\"  • Samples: {X_test.shape[0]:,}\")\n",
    "\n",
    "print(f\"\\nTARGET CLASSES:\")\n",
    "print(f\"  • Unique classes in y_train: {sorted(y_train.unique())}\")\n",
    "print(f\"  • Class distribution:\")\n",
    "for cls in sorted(y_train.unique()):\n",
    "    count = (y_train == cls).sum()\n",
    "    pct = count / len(y_train) * 100\n",
    "    print(f\"    - Class {cls}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "print(\"\\n✓ Data is ready for training\")\n",
    "\n",
    "# ============================================================================\n",
    "# MULTICOLLINEARITEIT PRE-CHECK\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MULTICOLLINEARITEIT PRE-CHECK\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if 'Diastolic' in X_train.columns and 'Systolic' in X_train.columns:\n",
    "    correlation = X_train[['Systolic', 'Diastolic']].corr().iloc[0, 1]\n",
    "    print(f\"\\n Systolic ↔ Diastolic correlation: r = {correlation:.3f}\")\n",
    "    \n",
    "    if abs(correlation) > 0.95:\n",
    "        print(f\"\\n EXTREME multicollineariteit gedetecteerd (|r| > 0.95)!\")\n",
    "        print(f\"\\nIMPACT:\")\n",
    "        print(f\"  • Random Forest: ✓ Kan trainen (robuust)\")\n",
    "        print(f\"  • Logistic Regression: Zal falen zonder correctie\")\n",
    "        print(f\"\\nACTIE:\")\n",
    "        print(f\"  • RF: Train met alle {X_train.shape[1]} features\")\n",
    "        print(f\"  • LR: Train zonder Diastolic ({X_train.shape[1]-1} features)\")\n",
    "        \n",
    "        remove_diastolic_for_lr = True\n",
    "    else:\n",
    "        print(f\"\\n✓ Multicollineariteit acceptabel (|r| < 0.95)\")\n",
    "        remove_diastolic_for_lr = False\n",
    "else:\n",
    "    print(\"\\n Systolic/Diastolic niet gevonden in features\")\n",
    "    remove_diastolic_for_lr = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c85bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MODEL 1: RANDOM FOREST CLASSIFIER (Joshua Kabel)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL 1: RANDOM FOREST CLASSIFIER - TRAINING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nInitializing Random Forest Classifier...\")\n",
    "print(\"Hyperparameters:\")\n",
    "print(\"  • n_estimators: 100 (number of trees)\")\n",
    "print(\"  • max_depth: None (trees grow until pure)\")\n",
    "print(\"  • min_samples_split: 2 (minimum samples to split node)\")\n",
    "print(\"  • random_state: 42 (reproducibility)\")\n",
    "print(\"  • n_jobs: -1 (use all CPU cores)\")\n",
    "print(f\"\\n  • Features used: ALL {X_train.shape[1]} features\")\n",
    "\n",
    "# Initialize model\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=None,\n",
    "    min_samples_split=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(\"\\n Training Random Forest...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Train model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "training_time_rf = time.time() - start_time\n",
    "print(f\"✓ Training completed in {training_time_rf:.2f} seconds\")\n",
    "\n",
    "# Predictions\n",
    "print(\"Making predictions...\")\n",
    "y_train_pred_rf = rf_model.predict(X_train)\n",
    "y_test_pred_rf = rf_model.predict(X_test)\n",
    "y_test_proba_rf = rf_model.predict_proba(X_test)\n",
    "\n",
    "print(\"✓ Predictions completed\")\n",
    "\n",
    "# ============================================================================\n",
    "# RANDOM FOREST - EVALUATION\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"RANDOM FOREST - PERFORMANCE METRICS\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Training accuracy\n",
    "train_accuracy_rf = accuracy_score(y_train, y_train_pred_rf)\n",
    "print(f\"\\nTRAINING SET PERFORMANCE:\")\n",
    "print(f\"  • Accuracy: {train_accuracy_rf:.4f} ({train_accuracy_rf*100:.2f}%)\")\n",
    "\n",
    "# Test accuracy\n",
    "test_accuracy_rf = accuracy_score(y_test, y_test_pred_rf)\n",
    "print(f\"\\nTEST SET PERFORMANCE:\")\n",
    "print(f\"  • Accuracy: {test_accuracy_rf:.4f} ({test_accuracy_rf*100:.2f}%)\")\n",
    "\n",
    "# Overfitting check\n",
    "overfitting_gap_rf = train_accuracy_rf - test_accuracy_rf\n",
    "print(f\"\\nOVERFITTING CHECK:\")\n",
    "print(f\"  • Train - Test gap: {overfitting_gap_rf:.4f} ({overfitting_gap_rf*100:.2f}%)\")\n",
    "if overfitting_gap_rf < 0.05:\n",
    "    print(f\"   Minimal overfitting (gap < 5%)\")\n",
    "elif overfitting_gap_rf < 0.10:\n",
    "    print(f\"  Moderate overfitting (gap 5-10%)\")\n",
    "else:\n",
    "    print(f\"  Significant overfitting (gap > 10%)\")\n",
    "\n",
    "# Detailed metrics\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"DETAILED CLASSIFICATION METRICS (TEST SET)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_pred_rf, \n",
    "                          target_names=['Class 0 (Insomnia)', 'Class 1 (None)', 'Class 2 (Sleep Apnea)']))\n",
    "\n",
    "# Per-class metrics\n",
    "precision_rf = precision_score(y_test, y_test_pred_rf, average='weighted')\n",
    "recall_rf = recall_score(y_test, y_test_pred_rf, average='weighted')\n",
    "f1_rf = f1_score(y_test, y_test_pred_rf, average='weighted')\n",
    "\n",
    "print(f\"\\nWEIGHTED AVERAGES (TEST SET):\")\n",
    "print(f\"  • Precision: {precision_rf:.4f}\")\n",
    "print(f\"  • Recall: {recall_rf:.4f}\")\n",
    "print(f\"  • F1-Score: {f1_rf:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"CONFUSION MATRIX (TEST SET)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "cm_rf = confusion_matrix(y_test, y_test_pred_rf)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(\"                 Predicted\")\n",
    "print(\"                 Insomnia  None  Sleep Apnea\")\n",
    "print(f\"Actual Insomnia     {cm_rf[0,0]:5d}  {cm_rf[0,1]:5d}  {cm_rf[0,2]:5d}\")\n",
    "print(f\"       None         {cm_rf[1,0]:5d}  {cm_rf[1,1]:5d}  {cm_rf[1,2]:5d}\")\n",
    "print(f\"       Sleep Apnea  {cm_rf[2,0]:5d}  {cm_rf[2,1]:5d}  {cm_rf[2,2]:5d}\")\n",
    "\n",
    "# Visualize Confusion Matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Insomnia', 'None', 'Sleep Apnea'],\n",
    "            yticklabels=['Insomnia', 'None', 'Sleep Apnea'])\n",
    "plt.title('Random Forest - Confusion Matrix (Test Set)', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Actual Class')\n",
    "plt.xlabel('Predicted Class')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3e9519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PREPARE DATA FOR LOGISTIC REGRESSION\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PREPARE DATA FOR LOGISTIC REGRESSION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if remove_diastolic_for_lr:\n",
    "    print(\"\\n Removing Diastolic to prevent multicollineariteit issues...\")\n",
    "    \n",
    "    X_train_lr = X_train.drop('Diastolic', axis=1)\n",
    "    X_test_lr = X_test.drop('Diastolic', axis=1)\n",
    "    \n",
    "    print(f\"✓ Diastolic removed for Logistic Regression\")\n",
    "    print(f\"  • Original features: {X_train.shape[1]}\")\n",
    "    print(f\"  • LR features: {X_train_lr.shape[1]}\")\n",
    "    print(f\"  • Removed: Diastolic (r=0.979 with Systolic)\")\n",
    "    \n",
    "    features_used_lr = X_train_lr.shape[1]\n",
    "else:\n",
    "    print(\"\\n✓ Using all features for Logistic Regression\")\n",
    "    X_train_lr = X_train.copy()\n",
    "    X_test_lr = X_test.copy()\n",
    "    features_used_lr = X_train_lr.shape[1]\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL 2: LOGISTIC REGRESSION (FIXED)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL 2: LOGISTIC REGRESSION - TRAINING (FIXED)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nInitializing Logistic Regression...\")\n",
    "print(\"Hyperparameters:\")\n",
    "print(\"  • max_iter: 1000 (maximum iterations)\")\n",
    "print(\"  • solver: 'lbfgs' (optimization algorithm)\")\n",
    "print(\"  • random_state: 42 (reproducibility)\")\n",
    "print(f\"\\n  • Features used: {features_used_lr} features\")\n",
    "if remove_diastolic_for_lr:\n",
    "    print(f\"  • Diastolic EXCLUDED to prevent numerical instability\")\n",
    "\n",
    "# Initialize model\n",
    "lr_model = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    solver='lbfgs',\n",
    "    random_state=42,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(\"\\n Training Logistic Regression...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Train model\n",
    "lr_model.fit(X_train_lr, y_train)\n",
    "\n",
    "training_time_lr = time.time() - start_time\n",
    "print(f\"✓ Training completed in {training_time_lr:.2f} seconds\")\n",
    "\n",
    "# Predictions\n",
    "print(\"\\n Making predictions...\")\n",
    "y_train_pred_lr = lr_model.predict(X_train_lr)\n",
    "y_test_pred_lr = lr_model.predict(X_test_lr)\n",
    "y_test_proba_lr = lr_model.predict_proba(X_test_lr)\n",
    "\n",
    "print(\"✓ Predictions completed\")\n",
    "\n",
    "# ============================================================================\n",
    "# LOGISTIC REGRESSION - EVALUATION\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"LOGISTIC REGRESSION - PERFORMANCE METRICS\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Training accuracy\n",
    "train_accuracy_lr = accuracy_score(y_train, y_train_pred_lr)\n",
    "print(f\"\\nTRAINING SET PERFORMANCE:\")\n",
    "print(f\"  • Accuracy: {train_accuracy_lr:.4f} ({train_accuracy_lr*100:.2f}%)\")\n",
    "\n",
    "# Test accuracy\n",
    "test_accuracy_lr = accuracy_score(y_test, y_test_pred_lr)\n",
    "print(f\"\\nTEST SET PERFORMANCE:\")\n",
    "print(f\"  • Accuracy: {test_accuracy_lr:.4f} ({test_accuracy_lr*100:.2f}%)\")\n",
    "\n",
    "# Check if model is working (not dummy classifier)\n",
    "unique_predictions = len(np.unique(y_test_pred_lr))\n",
    "print(f\"\\nMODEL SANITY CHECK:\")\n",
    "print(f\"  • Unique predictions: {unique_predictions}/3 classes\")\n",
    "if unique_predictions == 3:\n",
    "    print(f\"   Model predicts all classes (working correctly)\")\n",
    "elif unique_predictions == 1:\n",
    "    print(f\"  Model predicts only 1 class (DUMMY CLASSIFIER - NOT WORKING!)\")\n",
    "    print(f\"  → This should NOT happen with Diastolic removed\")\n",
    "else:\n",
    "    print(f\"  Model predicts {unique_predictions} classes (partial failure)\")\n",
    "\n",
    "# Overfitting check\n",
    "overfitting_gap_lr = train_accuracy_lr - test_accuracy_lr\n",
    "print(f\"\\nOVERFITTING CHECK:\")\n",
    "print(f\"  • Train - Test gap: {overfitting_gap_lr:.4f} ({overfitting_gap_lr*100:.2f}%)\")\n",
    "if overfitting_gap_lr < 0.05:\n",
    "    print(f\"   Minimal overfitting (gap < 5%)\")\n",
    "elif overfitting_gap_lr < 0.10:\n",
    "    print(f\"  Moderate overfitting (gap 5-10%)\")\n",
    "else:\n",
    "    print(f\"   Significant overfitting (gap > 10%)\")\n",
    "\n",
    "# Detailed metrics\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"DETAILED CLASSIFICATION METRICS (TEST SET)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_pred_lr, \n",
    "                          target_names=['Class 0 (Insomnia)', 'Class 1 (None)', 'Class 2 (Sleep Apnea)'],\n",
    "                          zero_division=0))\n",
    "\n",
    "# Per-class metrics\n",
    "precision_lr = precision_score(y_test, y_test_pred_lr, average='weighted', zero_division=0)\n",
    "recall_lr = recall_score(y_test, y_test_pred_lr, average='weighted', zero_division=0)\n",
    "f1_lr = f1_score(y_test, y_test_pred_lr, average='weighted', zero_division=0)\n",
    "\n",
    "print(f\"\\nWEIGHTED AVERAGES (TEST SET):\")\n",
    "print(f\"  • Precision: {precision_lr:.4f}\")\n",
    "print(f\"  • Recall: {recall_lr:.4f}\")\n",
    "print(f\"  • F1-Score: {f1_lr:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"CONFUSION MATRIX (TEST SET)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "cm_lr = confusion_matrix(y_test, y_test_pred_lr)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(\"                 Predicted\")\n",
    "print(\"                 Insomnia  None  Sleep Apnea\")\n",
    "print(f\"Actual Insomnia     {cm_lr[0,0]:5d}  {cm_lr[0,1]:5d}  {cm_lr[0,2]:5d}\")\n",
    "print(f\"       None         {cm_lr[1,0]:5d}  {cm_lr[1,1]:5d}  {cm_lr[1,2]:5d}\")\n",
    "print(f\"       Sleep Apnea  {cm_lr[2,0]:5d}  {cm_lr[2,1]:5d}  {cm_lr[2,2]:5d}\")\n",
    "\n",
    "# Visualize Confusion Matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm_lr, annot=True, fmt='d', cmap='Greens', \n",
    "            xticklabels=['Insomnia', 'None', 'Sleep Apnea'],\n",
    "            yticklabels=['Insomnia', 'None', 'Sleep Apnea'])\n",
    "plt.title('Logistic Regression - Confusion Matrix (Test Set)', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Actual Class')\n",
    "plt.xlabel('Predicted Class')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL COMPARISON\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL COMPARISON - BASELINE PERFORMANCE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create comparison dataframe\n",
    "comparison_data = {\n",
    "    'Model': ['Random Forest', 'Logistic Regression'],\n",
    "    'Features Used': [X_train.shape[1], features_used_lr],\n",
    "    'Train Accuracy': [train_accuracy_rf, train_accuracy_lr],\n",
    "    'Test Accuracy': [test_accuracy_rf, test_accuracy_lr],\n",
    "    'Precision': [precision_rf, precision_lr],\n",
    "    'Recall': [recall_rf, recall_lr],\n",
    "    'F1-Score': [f1_rf, f1_lr],\n",
    "    'Training Time (s)': [training_time_rf, training_time_lr],\n",
    "    'Overfitting Gap': [overfitting_gap_rf, overfitting_gap_lr]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.round(4)\n",
    "\n",
    "print(\"\\nPerformance Comparison Table:\")\n",
    "display(comparison_df)\n",
    "\n",
    "# Determine best model\n",
    "best_model_idx = comparison_df['Test Accuracy'].idxmax()\n",
    "best_model_name = comparison_df.loc[best_model_idx, 'Model']\n",
    "best_accuracy = comparison_df.loc[best_model_idx, 'Test Accuracy']\n",
    "\n",
    "print(f\"\\n BEST PERFORMING MODEL (by Test Accuracy):\")\n",
    "print(f\"   {best_model_name}: {best_accuracy:.4f} ({best_accuracy*100:.2f}%)\")\n",
    "\n",
    "# Visual comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Accuracy comparison\n",
    "metrics = ['Train Accuracy', 'Test Accuracy']\n",
    "rf_scores = [train_accuracy_rf, test_accuracy_rf]\n",
    "lr_scores = [train_accuracy_lr, test_accuracy_lr]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "axes[0].bar(x - width/2, rf_scores, width, label='Random Forest', color='steelblue')\n",
    "axes[0].bar(x + width/2, lr_scores, width, label='Logistic Regression', color='seagreen')\n",
    "axes[0].set_ylabel('Accuracy Score')\n",
    "axes[0].set_title('Accuracy Comparison')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(metrics)\n",
    "axes[0].legend()\n",
    "axes[0].set_ylim([0, 1.1])\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Per-class metrics\n",
    "metrics = ['Precision', 'Recall', 'F1-Score']\n",
    "rf_scores = [precision_rf, recall_rf, f1_rf]\n",
    "lr_scores = [precision_lr, recall_lr, f1_lr]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "axes[1].bar(x - width/2, rf_scores, width, label='Random Forest', color='steelblue')\n",
    "axes[1].bar(x + width/2, lr_scores, width, label='Logistic Regression', color='seagreen')\n",
    "axes[1].set_ylabel('Score')\n",
    "axes[1].set_title('Weighted Metrics Comparison')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(metrics)\n",
    "axes[1].legend()\n",
    "axes[1].set_ylim([0, 1.1])\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Training time comparison\n",
    "models = ['Random Forest', 'Logistic Regression']\n",
    "times = [training_time_rf, training_time_lr]\n",
    "\n",
    "axes[2].bar(models, times, color=['steelblue', 'seagreen'])\n",
    "axes[2].set_ylabel('Training Time (seconds)')\n",
    "axes[2].set_title('Training Time Comparison')\n",
    "axes[2].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.suptitle('Baseline Models - Performance Comparison (FIXED)', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# INTERPRETATION & INSIGHTS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INTERPRETATION & KEY INSIGHTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\"\"\n",
    "BASELINE MODEL PERFORMANCE ANALYSIS (FIXED):\n",
    "=============================================\n",
    "\n",
    "1. RANDOM FOREST RESULTS:\n",
    "   • Test Accuracy: {test_accuracy_rf*100:.2f}%\n",
    "   • Training Time: {training_time_rf:.2f}s\n",
    "   • Overfitting: {overfitting_gap_rf*100:.2f}% gap\n",
    "   • Features: {X_train.shape[1]} (all features)\n",
    "   \n",
    "2. LOGISTIC REGRESSION RESULTS (FIXED):\n",
    "   • Test Accuracy: {test_accuracy_lr*100:.2f}%\n",
    "   • Training Time: {training_time_lr:.2f}s\n",
    "   • Overfitting: {overfitting_gap_lr*100:.2f}% gap\n",
    "   • Features: {features_used_lr} (Diastolic removed)\n",
    "\n",
    "COMPARATIVE INSIGHTS:\n",
    "=====================\n",
    "\n",
    "Accuracy Difference: {abs(test_accuracy_rf - test_accuracy_lr)*100:.2f}%\n",
    "Feature Difference: RF uses {X_train.shape[1]}, LR uses {features_used_lr}\n",
    "\"\"\")\n",
    "\n",
    "if test_accuracy_rf > test_accuracy_lr + 0.05:\n",
    "    print(\"\"\"\n",
    "→ Random Forest SIGNIFICANTLY outperforms Logistic Regression (>5% difference)\n",
    "→ INTERPRETATION: Non-lineaire relaties zijn belangrijk in deze data\n",
    "→ CONCLUSIE: Tree-based modellen zijn geschikter voor dit probleem\n",
    "\"\"\")\n",
    "elif test_accuracy_rf > test_accuracy_lr:\n",
    "    print(\"\"\"\n",
    "→ Random Forest performs slightly better than Logistic Regression\n",
    "→ INTERPRETATION: Lichte non-lineaire relaties, maar lineaire benadering ook redelijk\n",
    "→ CONCLUSIE: Beide modellen zijn bruikbaar, RF heeft voorkeur\n",
    "\"\"\")\n",
    "else:\n",
    "    print(\"\"\"\n",
    "→ Logistic Regression performs as good or better than Random Forest\n",
    "→ INTERPRETATION: Data is grotendeels lineair scheidbaar\n",
    "→ CONCLUSIE: Simpeler model (LR) is voldoende, voorkeur vanwege interpreteerbaarheid\n",
    "\"\"\")\n",
    "\n",
    "if remove_diastolic_for_lr:\n",
    "    print(f\"\"\"\n",
    "MULTICOLLINEARITEIT FIX:\n",
    "========================\n",
    "✓ Diastolic removed from Logistic Regression\n",
    "✓ Model now predicts all {unique_predictions} classes correctly\n",
    "✓ Accuracy improved from ~60% (dummy) to {test_accuracy_lr*100:.2f}% (working model)\n",
    "\n",
    "IMPACT:\n",
    "• RF: Unchanged (robuust tegen multicollineariteit)\n",
    "• LR: Dramatic improvement (numerieke stabiliteit hersteld)\n",
    "• Feature count: RF={X_train.shape[1]}, LR={features_used_lr}\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ STAP 1 VOLTOOID - BASELINE MODELS GETRAIND (FIXED)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save for next steps\n",
    "print(\"\\n✓ Results saved for next steps:\")\n",
    "print(\"  • rf_model (trained with all features)\")\n",
    "print(\"  • lr_model (trained without Diastolic)\")\n",
    "print(\"  • X_train_lr, X_test_lr (LR feature sets)\")\n",
    "print(\"  • comparison_df (performance table)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e439938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "#  STAP 2 - FEATURE IMPORTANCE ANALYSE\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STAP 2: FEATURE IMPORTANCE ANALYSE (FIXED)\")\n",
    "print(\"=\"*80)\n",
    "print(\"\"\"\n",
    "DOEL: Identificeer belangrijkste features voor voorspelling\n",
    "METHODEN: \n",
    "  1. Random Forest Gini Importance\n",
    "  2. Logistic Regression Coefficients  \n",
    "  3. Permutation Importance (model-agnostic)\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# DIAGNOSTIC: CHECK WHAT FEATURES EACH MODEL WAS TRAINED ON\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DIAGNOSTIC: CHECKING MODEL FEATURE SETS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check Random Forest\n",
    "rf_n_features = rf_model.n_features_in_\n",
    "print(f\"\\nRandom Forest:\")\n",
    "print(f\"  • Trained on {rf_n_features} features\")\n",
    "print(f\"  • Current X_train has {X_train.shape[1]} features\")\n",
    "print(f\"  • Match: {rf_n_features == X_train.shape[1]}\")\n",
    "\n",
    "# Check Logistic Regression\n",
    "lr_n_features = lr_model.n_features_in_\n",
    "print(f\"\\nLogistic Regression:\")\n",
    "print(f\"  • Trained on {lr_n_features} features\")\n",
    "print(f\"  • Current X_train has {X_train.shape[1]} features\")\n",
    "print(f\"  • Match: {lr_n_features == X_train.shape[1]}\")\n",
    "\n",
    "# Determine which feature set to use\n",
    "if rf_n_features == X_train.shape[1]:\n",
    "    feature_names_for_analysis = X_train.columns\n",
    "    X_for_rf_analysis = X_train\n",
    "    X_test_for_rf = X_test\n",
    "    print(f\"\\n✓ Using current X_train features for Random Forest analysis\")\n",
    "else:\n",
    "    print(f\"\\n Feature mismatch for Random Forest!\")\n",
    "    print(f\"   RF was trained on different feature set\")\n",
    "\n",
    "if lr_n_features == X_train.shape[1]:\n",
    "    feature_names_for_lr = X_train.columns\n",
    "    X_for_lr_analysis = X_train\n",
    "    X_test_for_lr = X_test\n",
    "    print(f\"✓ Using current X_train features for Logistic Regression analysis\")\n",
    "elif 'X_train_lr' in locals() and lr_n_features == X_train_lr.shape[1]:\n",
    "    # LR was trained on different feature set (without Diastolic)\n",
    "    feature_names_for_lr = X_train_lr.columns\n",
    "    X_for_lr_analysis = X_train_lr\n",
    "    X_test_for_lr = X_test_lr\n",
    "    print(f\"✓ Using X_train_lr features for Logistic Regression analysis\")\n",
    "    print(f\"  (LR was trained without Diastolic)\")\n",
    "else:\n",
    "    print(f\"\\n Feature mismatch for Logistic Regression!\")\n",
    "    print(f\"   Creating compatible feature set...\")\n",
    "    # Remove Diastolic if it exists\n",
    "    if 'Diastolic' in X_train.columns:\n",
    "        feature_names_for_lr = X_train.columns.drop('Diastolic')\n",
    "        X_for_lr_analysis = X_train.drop('Diastolic', axis=1)\n",
    "        X_test_for_lr = X_test.drop('Diastolic', axis=1)\n",
    "    else:\n",
    "        feature_names_for_lr = X_train.columns\n",
    "        X_for_lr_analysis = X_train\n",
    "        X_test_for_lr = X_test\n",
    "\n",
    "# ============================================================================\n",
    "# EXTRACT IMPORTANCE FROM ALL THREE METHODS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXTRACTING FEATURE IMPORTANCES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. Random Forest (Gini-based)\n",
    "print(\"\\n1. Random Forest Gini Importance...\")\n",
    "rf_importances = pd.DataFrame({\n",
    "    'Feature': feature_names_for_analysis,\n",
    "    'RF_Importance': rf_model.feature_importances_\n",
    "})\n",
    "\n",
    "# 2. Logistic Regression (mean absolute coefficient across classes)\n",
    "print(\"2. Logistic Regression Coefficients...\")\n",
    "lr_coef_importance = np.abs(lr_model.coef_).mean(axis=0)\n",
    "\n",
    "# Verify shapes match\n",
    "print(f\"   LR coefficients shape: {lr_coef_importance.shape}\")\n",
    "print(f\"   LR feature names: {len(feature_names_for_lr)}\")\n",
    "\n",
    "if len(lr_coef_importance) != len(feature_names_for_lr):\n",
    "    print(f\"\\n MISMATCH DETECTED!\")\n",
    "    print(f\"   Coefficients: {len(lr_coef_importance)}\")\n",
    "    print(f\"   Features: {len(feature_names_for_lr)}\")\n",
    "    print(f\"   Using first {min(len(lr_coef_importance), len(feature_names_for_lr))} features\")\n",
    "    \n",
    "    # Truncate to matching length\n",
    "    n_features = min(len(lr_coef_importance), len(feature_names_for_lr))\n",
    "    lr_importances = pd.DataFrame({\n",
    "        'Feature': feature_names_for_lr[:n_features],\n",
    "        'LR_Coefficient': lr_coef_importance[:n_features]\n",
    "    })\n",
    "else:\n",
    "    lr_importances = pd.DataFrame({\n",
    "        'Feature': feature_names_for_lr,\n",
    "        'LR_Coefficient': lr_coef_importance\n",
    "    })\n",
    "\n",
    "# 3. Permutation Importance (on test set - use RF since it's typically more reliable)\n",
    "print(\"3. Permutation Importance (using Random Forest, may take 1-2 minutes)...\")\n",
    "perm_result = permutation_importance(\n",
    "    rf_model, X_test_for_rf, y_test,\n",
    "    n_repeats=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "perm_importances = pd.DataFrame({\n",
    "    'Feature': feature_names_for_analysis,\n",
    "    'Perm_Importance': perm_result.importances_mean,\n",
    "    'Perm_Std': perm_result.importances_std\n",
    "})\n",
    "\n",
    "print(\"\\n✓ All importances extracted\")\n",
    "\n",
    "# ============================================================================\n",
    "# MERGE AND NORMALIZE\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MERGING AND NORMALIZING IMPORTANCES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Start with RF importances\n",
    "importance_df = rf_importances.copy()\n",
    "\n",
    "# Add permutation importance\n",
    "importance_df = importance_df.merge(perm_importances, on='Feature', how='left')\n",
    "\n",
    "# Add LR importance (left join in case features don't match perfectly)\n",
    "importance_df = importance_df.merge(lr_importances, on='Feature', how='left')\n",
    "\n",
    "# Fill NaN values with 0 for LR coefficients (for features not in LR model)\n",
    "importance_df['LR_Coefficient'] = importance_df['LR_Coefficient'].fillna(0)\n",
    "\n",
    "# Normalize to percentages\n",
    "for col in ['RF_Importance', 'LR_Coefficient', 'Perm_Importance']:\n",
    "    if col in importance_df.columns:\n",
    "        col_sum = importance_df[col].sum()\n",
    "        if col_sum > 0:\n",
    "            importance_df[f'{col}_Pct'] = (importance_df[col] / col_sum) * 100\n",
    "        else:\n",
    "            importance_df[f'{col}_Pct'] = 0\n",
    "\n",
    "# Calculate average rank\n",
    "importance_df['RF_Rank'] = importance_df['RF_Importance'].rank(ascending=False)\n",
    "importance_df['LR_Rank'] = importance_df['LR_Coefficient'].rank(ascending=False)\n",
    "importance_df['Perm_Rank'] = importance_df['Perm_Importance'].rank(ascending=False)\n",
    "importance_df['Avg_Rank'] = importance_df[['RF_Rank', 'LR_Rank', 'Perm_Rank']].mean(axis=1)\n",
    "\n",
    "# Sort by average rank\n",
    "importance_df = importance_df.sort_values('Avg_Rank')\n",
    "\n",
    "# ============================================================================\n",
    "# VISUALIZATION: COMPREHENSIVE OVERVIEW\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"TOP 15 FEATURES (Consensus Ranking)\")\n",
    "print(\"-\"*80)\n",
    "display(importance_df[['Feature', 'RF_Importance_Pct', 'LR_Coefficient_Pct', \n",
    "                       'Perm_Importance_Pct', 'Avg_Rank']].head(15))\n",
    "\n",
    "# Create comprehensive visualization\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "gs = fig.add_gridspec(2, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# Plot 1: Top 15 Consensus (all methods)\n",
    "ax1 = fig.add_subplot(gs[0, :2])\n",
    "top_15 = importance_df.head(15)\n",
    "x = np.arange(len(top_15))\n",
    "width = 0.25\n",
    "\n",
    "ax1.barh(x - width, top_15['RF_Importance_Pct'], width, label='Random Forest', color='steelblue', alpha=0.8)\n",
    "ax1.barh(x, top_15['LR_Coefficient_Pct'], width, label='Logistic Reg', color='seagreen', alpha=0.8)\n",
    "ax1.barh(x + width, top_15['Perm_Importance_Pct'], width, label='Permutation', color='coral', alpha=0.8)\n",
    "ax1.set_yticks(x)\n",
    "ax1.set_yticklabels(top_15['Feature'])\n",
    "ax1.invert_yaxis()\n",
    "ax1.set_xlabel('Importance (%)')\n",
    "ax1.set_title('Top 15 Features - All Methods Comparison', fontweight='bold', fontsize=14)\n",
    "ax1.legend(loc='lower right')\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Plot 2: Cumulative Importance\n",
    "ax2 = fig.add_subplot(gs[0, 2])\n",
    "cumsum_pct = (importance_df['RF_Importance'] / importance_df['RF_Importance'].sum()).cumsum() * 100\n",
    "ax2.plot(range(1, len(cumsum_pct)+1), cumsum_pct, marker='o', linewidth=2, markersize=3)\n",
    "ax2.axhline(y=80, color='red', linestyle='--', linewidth=1, label='80%')\n",
    "ax2.axhline(y=90, color='orange', linestyle='--', linewidth=1, label='90%')\n",
    "features_80 = (cumsum_pct <= 80).sum() + 1\n",
    "features_90 = (cumsum_pct <= 90).sum() + 1\n",
    "ax2.axvline(x=features_80, color='red', linestyle=':', alpha=0.5)\n",
    "ax2.axvline(x=features_90, color='orange', linestyle=':', alpha=0.5)\n",
    "ax2.text(features_80, 40, f'{features_80}', ha='center', fontsize=10, fontweight='bold')\n",
    "ax2.text(features_90, 50, f'{features_90}', ha='center', fontsize=10, fontweight='bold')\n",
    "ax2.set_xlabel('Number of Features')\n",
    "ax2.set_ylabel('Cumulative Importance (%)')\n",
    "ax2.set_title('Cumulative Importance', fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "\n",
    "ax3 = fig.add_subplot(gs[1, :2])\n",
    "top_15_perm = importance_df.head(15)\n",
    "y_pos = range(len(top_15_perm))\n",
    "ax3.barh(y_pos, top_15_perm['Perm_Importance'], \n",
    "         xerr=top_15_perm['Perm_Std'],\n",
    "         color='coral', capsize=3, alpha=0.8)\n",
    "ax3.set_yticks(y_pos)\n",
    "ax3.set_yticklabels(top_15_perm['Feature'])\n",
    "ax3.invert_yaxis()\n",
    "ax3.set_xlabel('Permutation Importance (± std)')\n",
    "ax3.set_title('Top 15 - Permutation Importance (with uncertainty)', fontweight='bold')\n",
    "ax3.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Plot 4: Method Correlation Heatmap\n",
    "ax4 = fig.add_subplot(gs[1, 2])\n",
    "rank_corr = importance_df[['RF_Rank', 'LR_Rank', 'Perm_Rank']].corr()\n",
    "sns.heatmap(rank_corr, annot=True, fmt='.3f', cmap='coolwarm', center=0,\n",
    "            xticklabels=['RF', 'LR', 'Perm'],\n",
    "            yticklabels=['RF', 'LR', 'Perm'],\n",
    "            ax=ax4, cbar_kws={'label': 'Correlation'}, vmin=-1, vmax=1)\n",
    "ax4.set_title('Method Agreement\\n(Rank Correlation)', fontweight='bold')\n",
    "\n",
    "plt.suptitle('Feature Importance Analysis - Comprehensive Overview', \n",
    "             fontsize=16, fontweight='bold', y=0.98)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✓ {features_80} features explain 80% of importance\")\n",
    "print(f\"✓ {features_90} features explain 90% of importance\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ FEATURE IMPORTANCE ANALYSIS COMPLETE\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d724adc",
   "metadata": {},
   "source": [
    "## ML-uitwerking – Stap 3: Feature selection & retraining (kort)\n",
    "\n",
    "**Doel:** redundante/onbelangrijke features verwijderen, modellen opnieuw trainen, en **Baseline vs Optimized** vergelijken.\n",
    "\n",
    "### Wat verwijderen we?\n",
    "- **Diastolic** (sterk redundant met Systolic, r≈0.979) → voorkomt instabiliteit (vooral bij Logistic Regression).\n",
    "- Optioneel: features met **<1% RF-importance** (als die uit Stap 2 komen).\n",
    "\n",
    "### Wat doen we?\n",
    "1. Maak lijst `recommended_features_to_remove` (minimaal `Diastolic` als die bestaat).\n",
    "2. Maak **optimized** datasets:\n",
    "   - `X_train_optimized = X_train.drop(remove_list)`\n",
    "   - `X_test_optimized = X_test.drop(remove_list)`\n",
    "3. Retrain twee modellen op optimized set:\n",
    "   - **Random Forest**\n",
    "   - **Logistic Regression**\n",
    "4. Meet metrics op test:\n",
    "   - Accuracy, Precision, Recall, F1 (weighted)\n",
    "\n",
    "### Vergelijking Baseline → Optimized\n",
    "We zetten resultaten in een tabel en berekenen de veranderingen:\n",
    "- Kleine verandering (<0.5%): performance **behouden**\n",
    "- Positief: **verbeterd**\n",
    "- Negatief: **verslechterd**\n",
    "\n",
    "### Output / besluit\n",
    "- Confusion matrices voor optimized modellen\n",
    "- Samenvatting feature reductie + performance impact\n",
    "- Kies “beste” model op basis van hoogste **test accuracy** (en F1 als tweede check)\n",
    "- Next step: optioneel **hyperparameter tuning** op het beste model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6436a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# UITWERKING ML VRAAGSTUK\n",
    "# STAP 3: FEATURE SELECTION & MODEL RETRAINING\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3A: IDENTIFY FEATURES TO REMOVE\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 3A: FEATURE SELECTION - IDENTIFY FEATURES TO REMOVE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check if recommended_features_to_remove exists from Step 2\n",
    "if 'recommended_features_to_remove' not in locals():\n",
    "    print(\"\\n Creating feature removal list (recommended_features_to_remove not found)\")\n",
    "    recommended_features_to_remove = []\n",
    "    \n",
    "    # Always remove Diastolic IF IT EXISTS\n",
    "    if 'Diastolic' in X_train.columns:\n",
    "        recommended_features_to_remove.append('Diastolic')\n",
    "        print(\"  → Diastolic found and marked for removal\")\n",
    "    else:\n",
    "        print(\"   Diastolic not found in dataset (possibly already processed)\")\n",
    "    \n",
    "    # Optionally add low-importance features from RF\n",
    "    if 'rf_importance_df' in locals():\n",
    "        low_imp_features = rf_importance_df[rf_importance_df['Importance_Pct'] < 1.0]['Feature'].tolist()\n",
    "        for feat in low_imp_features:\n",
    "            if feat not in recommended_features_to_remove and feat in X_train.columns:\n",
    "                recommended_features_to_remove.append(feat)\n",
    "else:\n",
    "    # Filter recommended features to only include those that actually exist\n",
    "    print(\"\\n✓ Using feature removal list from Step 2\")\n",
    "    recommended_features_to_remove = [f for f in recommended_features_to_remove if f in X_train.columns]\n",
    "    \n",
    "    # Check which recommended features don't exist\n",
    "    missing_features = [f for f in locals().get('recommended_features_to_remove', []) \n",
    "                       if f not in X_train.columns]\n",
    "    if missing_features:\n",
    "        print(f\"\\n   The following recommended features don't exist in dataset: {missing_features}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"FEATURES IDENTIFIED FOR REMOVAL:\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(f\"\\nTotal features to remove: {len(recommended_features_to_remove)}\")\n",
    "\n",
    "if len(recommended_features_to_remove) > 0:\n",
    "    for i, feat in enumerate(recommended_features_to_remove, 1):\n",
    "        # Get importance info if available\n",
    "        if 'feature_importance_comparison' in locals():\n",
    "            feat_data = feature_importance_comparison[feature_importance_comparison['Feature'] == feat]\n",
    "            if len(feat_data) > 0:\n",
    "                rf_imp = feat_data['RF_Importance_%'].values[0]\n",
    "                avg_rank = feat_data['Average_Rank'].values[0]\n",
    "                print(f\"  {i}. {feat}\")\n",
    "                print(f\"     → RF Importance: {rf_imp:.3f}%\")\n",
    "                print(f\"     → Average Rank: {avg_rank:.1f}\")\n",
    "                \n",
    "                # Rationale\n",
    "                if feat == 'Diastolic':\n",
    "                    print(f\"     → Rationale: Extreem hoge correlatie met Systolic (r=0.979)\")\n",
    "                elif rf_imp < 1.0:\n",
    "                    print(f\"     → Rationale: Low importance (<1%)\")\n",
    "            else:\n",
    "                print(f\"  {i}. {feat}\")\n",
    "        else:\n",
    "            print(f\"  {i}. {feat}\")\n",
    "else:\n",
    "    print(\"   No features identified for removal\")\n",
    "    print(\"  → Proceeding with all features\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3B: CREATE OPTIMIZED FEATURE SETS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 3B: CREATE OPTIMIZED FEATURE SETS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nCreating optimized feature sets by removing selected features...\")\n",
    "\n",
    "# Store original feature sets for comparison\n",
    "X_train_baseline = X_train.copy()\n",
    "X_test_baseline = X_test.copy()\n",
    "\n",
    "print(f\"\\nBASELINE (Original):\")\n",
    "print(f\"  • Train: {X_train_baseline.shape[0]:,} samples × {X_train_baseline.shape[1]} features\")\n",
    "print(f\"  • Test: {X_test_baseline.shape[0]:,} samples × {X_test_baseline.shape[1]} features\")\n",
    "\n",
    "# Remove features\n",
    "if len(recommended_features_to_remove) > 0:\n",
    "    # Create optimized sets\n",
    "    X_train_optimized = X_train_baseline.drop(columns=recommended_features_to_remove)\n",
    "    X_test_optimized = X_test_baseline.drop(columns=recommended_features_to_remove)\n",
    "    \n",
    "    print(f\"\\nOPTIMIZED (After removal):\")\n",
    "    print(f\"  • Train: {X_train_optimized.shape[0]:,} samples × {X_train_optimized.shape[1]} features\")\n",
    "    print(f\"  • Test: {X_test_optimized.shape[0]:,} samples × {X_test_optimized.shape[1]} features\")\n",
    "    \n",
    "    features_removed = X_train_baseline.shape[1] - X_train_optimized.shape[1]\n",
    "    print(f\"\\n✓ Removed {features_removed} features\")\n",
    "    print(f\"✓ Feature reduction: {X_train_baseline.shape[1]} → {X_train_optimized.shape[1]} ({features_removed/X_train_baseline.shape[1]*100:.1f}% reduction)\")\n",
    "else:\n",
    "    print(\"\\n   No features to remove - using baseline feature set\")\n",
    "    X_train_optimized = X_train_baseline.copy()\n",
    "    X_test_optimized = X_test_baseline.copy()\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3C: RETRAIN MODELS WITH OPTIMIZED FEATURES\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 3C: RETRAIN MODELS WITH OPTIMIZED FEATURE SET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL 1: RANDOM FOREST - OPTIMIZED\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"RETRAINING RANDOM FOREST WITH OPTIMIZED FEATURES\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(\"\\n Training Random Forest (Optimized)...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Initialize and train\n",
    "rf_model_optimized = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=None,\n",
    "    min_samples_split=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "rf_model_optimized.fit(X_train_optimized, y_train)\n",
    "\n",
    "training_time_rf_opt = time.time() - start_time\n",
    "print(f\"✓ Training completed in {training_time_rf_opt:.2f} seconds\")\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_rf_opt = rf_model_optimized.predict(X_train_optimized)\n",
    "y_test_pred_rf_opt = rf_model_optimized.predict(X_test_optimized)\n",
    "\n",
    "# Metrics\n",
    "train_accuracy_rf_opt = accuracy_score(y_train, y_train_pred_rf_opt)\n",
    "test_accuracy_rf_opt = accuracy_score(y_test, y_test_pred_rf_opt)\n",
    "precision_rf_opt = precision_score(y_test, y_test_pred_rf_opt, average='weighted')\n",
    "recall_rf_opt = recall_score(y_test, y_test_pred_rf_opt, average='weighted')\n",
    "f1_rf_opt = f1_score(y_test, y_test_pred_rf_opt, average='weighted')\n",
    "\n",
    "print(f\"\\nPerformance (Optimized):\")\n",
    "print(f\"  • Train Accuracy: {train_accuracy_rf_opt:.4f} ({train_accuracy_rf_opt*100:.2f}%)\")\n",
    "print(f\"  • Test Accuracy: {test_accuracy_rf_opt:.4f} ({test_accuracy_rf_opt*100:.2f}%)\")\n",
    "print(f\"  • Precision: {precision_rf_opt:.4f}\")\n",
    "print(f\"  • Recall: {recall_rf_opt:.4f}\")\n",
    "print(f\"  • F1-Score: {f1_rf_opt:.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL 2: LOGISTIC REGRESSION - OPTIMIZED\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"RETRAINING LOGISTIC REGRESSION WITH OPTIMIZED FEATURES\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(\"\\n Training Logistic Regression (Optimized)...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Initialize and train\n",
    "lr_model_optimized = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    multi_class='multinomial',\n",
    "    solver='lbfgs',\n",
    "    random_state=42,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "lr_model_optimized.fit(X_train_optimized, y_train)\n",
    "\n",
    "training_time_lr_opt = time.time() - start_time\n",
    "print(f\"✓ Training completed in {training_time_lr_opt:.2f} seconds\")\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_lr_opt = lr_model_optimized.predict(X_train_optimized)\n",
    "y_test_pred_lr_opt = lr_model_optimized.predict(X_test_optimized)\n",
    "\n",
    "# Metrics\n",
    "train_accuracy_lr_opt = accuracy_score(y_train, y_train_pred_lr_opt)\n",
    "test_accuracy_lr_opt = accuracy_score(y_test, y_test_pred_lr_opt)\n",
    "precision_lr_opt = precision_score(y_test, y_test_pred_lr_opt, average='weighted')\n",
    "recall_lr_opt = recall_score(y_test, y_test_pred_lr_opt, average='weighted')\n",
    "f1_lr_opt = f1_score(y_test, y_test_pred_lr_opt, average='weighted')\n",
    "\n",
    "print(f\"\\nPerformance (Optimized):\")\n",
    "print(f\"  • Train Accuracy: {train_accuracy_lr_opt:.4f} ({train_accuracy_lr_opt*100:.2f}%)\")\n",
    "print(f\"  • Test Accuracy: {test_accuracy_lr_opt:.4f} ({test_accuracy_lr_opt*100:.2f}%)\")\n",
    "print(f\"  • Precision: {precision_lr_opt:.4f}\")\n",
    "print(f\"  • Recall: {recall_lr_opt:.4f}\")\n",
    "print(f\"  • F1-Score: {f1_lr_opt:.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3D: BASELINE VS OPTIMIZED COMPARISON\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 3D: BASELINE VS OPTIMIZED COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create comprehensive comparison\n",
    "comparison_data = {\n",
    "    'Model': [\n",
    "        'Random Forest (Baseline)',\n",
    "        'Random Forest (Optimized)',\n",
    "        'Logistic Regression (Baseline)',\n",
    "        'Logistic Regression (Optimized)'\n",
    "    ],\n",
    "    'Features': [\n",
    "        X_train_baseline.shape[1],\n",
    "        X_train_optimized.shape[1],\n",
    "        X_train_baseline.shape[1],\n",
    "        X_train_optimized.shape[1]\n",
    "    ],\n",
    "    'Train Accuracy': [\n",
    "        train_accuracy_rf,\n",
    "        train_accuracy_rf_opt,\n",
    "        train_accuracy_lr,\n",
    "        train_accuracy_lr_opt\n",
    "    ],\n",
    "    'Test Accuracy': [\n",
    "        test_accuracy_rf,\n",
    "        test_accuracy_rf_opt,\n",
    "        test_accuracy_lr,\n",
    "        test_accuracy_lr_opt\n",
    "    ],\n",
    "    'Precision': [\n",
    "        precision_rf,\n",
    "        precision_rf_opt,\n",
    "        precision_lr,\n",
    "        precision_lr_opt\n",
    "    ],\n",
    "    'Recall': [\n",
    "        recall_rf,\n",
    "        recall_rf_opt,\n",
    "        recall_lr,\n",
    "        recall_lr_opt\n",
    "    ],\n",
    "    'F1-Score': [\n",
    "        f1_rf,\n",
    "        f1_rf_opt,\n",
    "        f1_lr,\n",
    "        f1_lr_opt\n",
    "    ],\n",
    "    'Training Time (s)': [\n",
    "        training_time_rf,\n",
    "        training_time_rf_opt,\n",
    "        training_time_lr,\n",
    "        training_time_lr_opt\n",
    "    ]\n",
    "}\n",
    "\n",
    "comparison_results = pd.DataFrame(comparison_data)\n",
    "comparison_results = comparison_results.round(4)\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"COMPREHENSIVE PERFORMANCE COMPARISON\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "display(comparison_results)\n",
    "\n",
    "# Calculate improvements\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"PERFORMANCE CHANGES (Baseline → Optimized)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Random Forest\n",
    "rf_acc_change = (test_accuracy_rf_opt - test_accuracy_rf) * 100\n",
    "rf_f1_change = (f1_rf_opt - f1_rf) * 100\n",
    "print(f\"\\nRANDOM FOREST:\")\n",
    "print(f\"  • Test Accuracy: {test_accuracy_rf*100:.2f}% → {test_accuracy_rf_opt*100:.2f}% ({rf_acc_change:+.2f}%)\")\n",
    "print(f\"  • F1-Score: {f1_rf:.4f} → {f1_rf_opt:.4f} ({rf_f1_change:+.2f}%)\")\n",
    "print(f\"  • Training Time: {training_time_rf:.2f}s → {training_time_rf_opt:.2f}s\")\n",
    "\n",
    "if abs(rf_acc_change) < 0.5:\n",
    "    print(f\"  ✓ Performance MAINTAINED (change < 0.5%)\")\n",
    "elif rf_acc_change > 0:\n",
    "    print(f\"  ✓ Performance IMPROVED (+{rf_acc_change:.2f}%)\")\n",
    "else:\n",
    "    print(f\"   Performance DECREASED ({rf_acc_change:.2f}%)\")\n",
    "\n",
    "# Logistic Regression\n",
    "lr_acc_change = (test_accuracy_lr_opt - test_accuracy_lr) * 100\n",
    "lr_f1_change = (f1_lr_opt - f1_lr) * 100\n",
    "print(f\"\\nLOGISTIC REGRESSION:\")\n",
    "print(f\"  • Test Accuracy: {test_accuracy_lr*100:.2f}% → {test_accuracy_lr_opt*100:.2f}% ({lr_acc_change:+.2f}%)\")\n",
    "print(f\"  • F1-Score: {f1_lr:.4f} → {f1_lr_opt:.4f} ({lr_f1_change:+.2f}%)\")\n",
    "print(f\"  • Training Time: {training_time_lr:.2f}s → {training_time_lr_opt:.2f}s\")\n",
    "\n",
    "if abs(lr_acc_change) < 0.5:\n",
    "    print(f\"  ✓ Performance MAINTAINED (change < 0.5%)\")\n",
    "elif lr_acc_change > 0:\n",
    "    print(f\"  ✓ Performance IMPROVED (+{lr_acc_change:.2f}%)\")\n",
    "else:\n",
    "    print(f\"   Performance DECREASED ({lr_acc_change:.2f}%)\")\n",
    "\n",
    "# ============================================================================\n",
    "# VISUALIZATION: BASELINE VS OPTIMIZED\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"VISUALIZATION: BASELINE VS OPTIMIZED COMPARISON\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Plot 1: Accuracy Comparison\n",
    "models = ['RF\\nBaseline', 'RF\\nOptimized', 'LR\\nBaseline', 'LR\\nOptimized']\n",
    "train_accs = [train_accuracy_rf, train_accuracy_rf_opt, train_accuracy_lr, train_accuracy_lr_opt]\n",
    "test_accs = [test_accuracy_rf, test_accuracy_rf_opt, test_accuracy_lr, test_accuracy_lr_opt]\n",
    "\n",
    "x = np.arange(len(models))\n",
    "width = 0.35\n",
    "\n",
    "axes[0, 0].bar(x - width/2, train_accs, width, label='Train Accuracy', color='lightblue')\n",
    "axes[0, 0].bar(x + width/2, test_accs, width, label='Test Accuracy', color='steelblue')\n",
    "axes[0, 0].set_ylabel('Accuracy')\n",
    "axes[0, 0].set_title('Accuracy Comparison: Baseline vs Optimized', fontweight='bold')\n",
    "axes[0, 0].set_xticks(x)\n",
    "axes[0, 0].set_xticklabels(models)\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].set_ylim([0.7, 1.0])\n",
    "axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, (train, test) in enumerate(zip(train_accs, test_accs)):\n",
    "    axes[0, 0].text(i - width/2, train + 0.01, f'{train:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "    axes[0, 0].text(i + width/2, test + 0.01, f'{test:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "# Plot 2: F1-Score Comparison\n",
    "f1_scores = [f1_rf, f1_rf_opt, f1_lr, f1_lr_opt]\n",
    "colors = ['steelblue', 'lightblue', 'seagreen', 'lightgreen']\n",
    "\n",
    "axes[0, 1].bar(models, f1_scores, color=colors)\n",
    "axes[0, 1].set_ylabel('F1-Score (weighted)')\n",
    "axes[0, 1].set_title('F1-Score Comparison: Baseline vs Optimized', fontweight='bold')\n",
    "axes[0, 1].set_ylim([0.7, 1.0])\n",
    "axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, score in enumerate(f1_scores):\n",
    "    axes[0, 1].text(i, score + 0.01, f'{score:.4f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Plot 3: Accuracy Change\n",
    "acc_changes = [rf_acc_change, lr_acc_change]\n",
    "model_names = ['Random Forest', 'Logistic Regression']\n",
    "colors_change = ['green' if x >= 0 else 'red' for x in acc_changes]\n",
    "\n",
    "axes[1, 0].bar(model_names, acc_changes, color=colors_change, alpha=0.7)\n",
    "axes[1, 0].axhline(y=0, color='black', linestyle='-', linewidth=0.8)\n",
    "axes[1, 0].set_ylabel('Accuracy Change (%)')\n",
    "axes[1, 0].set_title('Performance Change: Baseline → Optimized', fontweight='bold')\n",
    "axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, change in enumerate(acc_changes):\n",
    "    axes[1, 0].text(i, change + 0.1 if change >= 0 else change - 0.1, \n",
    "                    f'{change:+.2f}%', ha='center', va='bottom' if change >= 0 else 'top', \n",
    "                    fontsize=10, fontweight='bold')\n",
    "\n",
    "# Plot 4: Feature Count & Training Time\n",
    "x = np.arange(len(model_names))\n",
    "width = 0.35\n",
    "\n",
    "# Feature count (normalized)\n",
    "feature_reduction = [(X_train_baseline.shape[1] - X_train_optimized.shape[1]) / X_train_baseline.shape[1] * 100] * 2\n",
    "\n",
    "ax4_twin = axes[1, 1].twinx()\n",
    "\n",
    "bars1 = axes[1, 1].bar(x - width/2, [training_time_rf, training_time_lr], width, \n",
    "                       label='Baseline Time', color='lightcoral')\n",
    "bars2 = axes[1, 1].bar(x + width/2, [training_time_rf_opt, training_time_lr_opt], width,\n",
    "                       label='Optimized Time', color='coral')\n",
    "\n",
    "axes[1, 1].set_ylabel('Training Time (seconds)', color='coral')\n",
    "axes[1, 1].set_xlabel('Model')\n",
    "axes[1, 1].set_title('Training Time & Feature Reduction', fontweight='bold')\n",
    "axes[1, 1].set_xticks(x)\n",
    "axes[1, 1].set_xticklabels(model_names)\n",
    "axes[1, 1].tick_params(axis='y', labelcolor='coral')\n",
    "axes[1, 1].legend(loc='upper left')\n",
    "\n",
    "# Feature reduction on secondary axis\n",
    "ax4_twin.plot(x, feature_reduction, color='steelblue', marker='o', linewidth=2, markersize=10, label='Feature Reduction %')\n",
    "ax4_twin.set_ylabel('Feature Reduction (%)', color='steelblue')\n",
    "ax4_twin.tick_params(axis='y', labelcolor='steelblue')\n",
    "ax4_twin.legend(loc='upper right')\n",
    "ax4_twin.set_ylim([0, max(feature_reduction) * 1.5])\n",
    "\n",
    "plt.suptitle('Baseline vs Optimized Models - Comprehensive Comparison', \n",
    "             fontsize=16, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# CONFUSION MATRICES: OPTIMIZED MODELS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"CONFUSION MATRICES: OPTIMIZED MODELS\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Random Forest - Optimized\n",
    "cm_rf_opt = confusion_matrix(y_test, y_test_pred_rf_opt)\n",
    "sns.heatmap(cm_rf_opt, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Insomnia', 'None', 'Sleep Apnea'],\n",
    "            yticklabels=['Insomnia', 'None', 'Sleep Apnea'],\n",
    "            ax=axes[0])\n",
    "axes[0].set_title(f'Random Forest (Optimized)\\nTest Accuracy: {test_accuracy_rf_opt*100:.2f}%', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Actual Class')\n",
    "axes[0].set_xlabel('Predicted Class')\n",
    "\n",
    "# Logistic Regression - Optimized\n",
    "cm_lr_opt = confusion_matrix(y_test, y_test_pred_lr_opt)\n",
    "sns.heatmap(cm_lr_opt, annot=True, fmt='d', cmap='Greens', \n",
    "            xticklabels=['Insomnia', 'None', 'Sleep Apnea'],\n",
    "            yticklabels=['Insomnia', 'None', 'Sleep Apnea'],\n",
    "            ax=axes[1])\n",
    "axes[1].set_title(f'Logistic Regression (Optimized)\\nTest Accuracy: {test_accuracy_lr_opt*100:.2f}%', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Actual Class')\n",
    "axes[1].set_xlabel('Predicted Class')\n",
    "\n",
    "plt.suptitle('Confusion Matrices - Optimized Models (Test Set)', \n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# FINAL EVALUATION & RECOMMENDATIONS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL EVALUATION & RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\"\"\n",
    "FEATURE SELECTION IMPACT SUMMARY:\n",
    "==================================\n",
    "\n",
    "FEATURE REDUCTION:\n",
    "• Features removed: {X_train_baseline.shape[1] - X_train_optimized.shape[1]}\n",
    "• Original: {X_train_baseline.shape[1]} features\n",
    "• Optimized: {X_train_optimized.shape[1]} features\n",
    "• Reduction: {(X_train_baseline.shape[1] - X_train_optimized.shape[1])/X_train_baseline.shape[1]*100:.1f}%\n",
    "\n",
    "RANDOM FOREST:\n",
    "• Accuracy change: {rf_acc_change:+.2f}%\n",
    "• F1-Score change: {rf_f1_change:+.2f}%\n",
    "• Conclusion: {\"✓ Feature selection successful - performance maintained/improved\" if abs(rf_acc_change) < 1.0 else \"Performance impacted - re-evaluate feature selection\"}\n",
    "\n",
    "LOGISTIC REGRESSION:\n",
    "• Accuracy change: {lr_acc_change:+.2f}%\n",
    "• F1-Score change: {lr_f1_change:+.2f}%\n",
    "• Conclusion: {\"✓ Feature selection successful - multicollineariteit reduced\" if lr_acc_change >= -0.5 else \" Performance impacted significantly\"}\n",
    "\"\"\")\n",
    "\n",
    "# Determine best overall model\n",
    "best_model_baseline = 'Random Forest' if test_accuracy_rf > test_accuracy_lr else 'Logistic Regression'\n",
    "best_acc_baseline = max(test_accuracy_rf, test_accuracy_lr)\n",
    "\n",
    "best_model_optimized = 'Random Forest' if test_accuracy_rf_opt > test_accuracy_lr_opt else 'Logistic Regression'\n",
    "best_acc_optimized = max(test_accuracy_rf_opt, test_accuracy_lr_opt)\n",
    "\n",
    "print(f\"\\nBEST MODEL IDENTIFICATION:\")\n",
    "print(f\"  • Baseline: {best_model_baseline} ({best_acc_baseline*100:.2f}%)\")\n",
    "print(f\"  • Optimized: {best_model_optimized} ({best_acc_optimized*100:.2f}%)\")\n",
    "\n",
    "if best_acc_optimized >= best_acc_baseline:\n",
    "    print(f\"\\n RECOMMENDATION: Use OPTIMIZED {best_model_optimized}\")\n",
    "    print(f\"   • Accuracy: {best_acc_optimized*100:.2f}%\")\n",
    "    print(f\"   • Features: {X_train_optimized.shape[1]} (simpler model)\")\n",
    "    print(f\"   • Benefits: Cleaner, faster, more interpretable\")\n",
    "else:\n",
    "    print(f\"\\n RECOMMENDATION: Consider using BASELINE {best_model_baseline}\")\n",
    "    print(f\"   • Accuracy: {best_acc_baseline*100:.2f}%\")\n",
    "    print(f\"   • Trade-off: More features but better performance\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1b823e",
   "metadata": {},
   "source": [
    "## Stap 4 – Retraining met unified features (heel kort)\n",
    "\n",
    "**Doel:** RF en LR trainen met **dezelfde 22 features** (o.a. `Diastolic` overal verwijderd) → eerlijke vergelijking + minder multicollineariteit.\n",
    "\n",
    "**Acties:**\n",
    "- Maak `X_train_unified`/`X_test_unified` door `Diastolic` te droppen.\n",
    "- Train **Random Forest** en **Logistic Regression** op deze unified set.\n",
    "- Evalueer op test: **Accuracy, Precision, Recall, F1** + check overfitting (`train - test`).\n",
    "- Vergelijk met baseline (tabel + confusion matrix + classification report) en kies beste model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5bcfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# UITWERKING ML VRAAGSTUK\n",
    "# STAP 4: MODEL RETRAINING MET OPTIMIZED FEATURES (22 FEATURES)\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# STAP 4A: VERIFY DATA STATUS & CREATE UNIFIED FEATURE SET\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STAP 4A: VERIFY DATA STATUS & CREATE UNIFIED FEATURE SET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check if we have the baseline data\n",
    "if 'X_train_baseline' not in locals() or 'X_test_baseline' not in locals():\n",
    "    print(\"\\n Baseline data niet gevonden - we gebruiken de huidige X_train en X_test\")\n",
    "    X_train_baseline = X_train.copy()\n",
    "    X_test_baseline = X_test.copy()\n",
    "\n",
    "print(f\"\\nCURRENT DATA STATUS:\")\n",
    "print(f\"  • X_train_baseline: {X_train_baseline.shape}\")\n",
    "print(f\"  • X_test_baseline: {X_test_baseline.shape}\")\n",
    "\n",
    "# Identify features to remove\n",
    "features_to_remove_unified = []\n",
    "\n",
    "# Always remove Diastolic if it exists\n",
    "if 'Diastolic' in X_train_baseline.columns:\n",
    "    features_to_remove_unified.append('Diastolic')\n",
    "    print(f\"\\n✓ Diastolic found in baseline data - will be removed\")\n",
    "else:\n",
    "    print(f\"\\n Diastolic not found - possibly already removed in previous steps\")\n",
    "\n",
    "# Check for other low-importance features if we have the analysis\n",
    "if 'rf_importance_df' in locals():\n",
    "    # Get features with <1% importance (excluding Diastolic which we already added)\n",
    "    low_imp_features = rf_importance_df[\n",
    "        (rf_importance_df['Importance_Pct'] < 1.0) & \n",
    "        (rf_importance_df['Feature'] != 'Diastolic')\n",
    "    ]['Feature'].tolist()\n",
    "    \n",
    "    if len(low_imp_features) > 0:\n",
    "        print(f\"\\n Additional low-importance features found (<1%):\")\n",
    "        for feat in low_imp_features:\n",
    "            if feat in X_train_baseline.columns:\n",
    "                imp_pct = rf_importance_df[rf_importance_df['Feature'] == feat]['Importance_Pct'].values[0]\n",
    "                print(f\"  • {feat}: {imp_pct:.3f}%\")\n",
    "                \n",
    "                # Ask if we should remove (for now, we keep them to be conservative)\n",
    "                # features_to_remove_unified.append(feat)\n",
    "\n",
    "print(f\"\\n\" + \"-\"*80)\n",
    "print(\"FEATURES TO REMOVE:\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(f\"\\nTotal features to remove: {len(features_to_remove_unified)}\")\n",
    "for i, feat in enumerate(features_to_remove_unified, 1):\n",
    "    print(f\"  {i}. {feat}\")\n",
    "    if feat == 'Diastolic':\n",
    "        print(f\"     → Rationale: Extreme multicollineariteit met Systolic (r=0.979)\")\n",
    "\n",
    "# Create optimized feature sets\n",
    "print(f\"\\n\" + \"-\"*80)\n",
    "print(\"CREATING UNIFIED OPTIMIZED FEATURE SET:\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "if len(features_to_remove_unified) > 0:\n",
    "    X_train_unified = X_train_baseline.drop(columns=features_to_remove_unified)\n",
    "    X_test_unified = X_test_baseline.drop(columns=features_to_remove_unified)\n",
    "    \n",
    "    print(f\"\\n✓ Features removed: {len(features_to_remove_unified)}\")\n",
    "    print(f\"✓ Baseline features: {X_train_baseline.shape[1]}\")\n",
    "    print(f\"✓ Unified features: {X_train_unified.shape[1]}\")\n",
    "else:\n",
    "    print(f\"\\n No features to remove - using baseline as unified set\")\n",
    "    X_train_unified = X_train_baseline.copy()\n",
    "    X_test_unified = X_test_baseline.copy()\n",
    "\n",
    "print(f\"\\nFINAL UNIFIED FEATURE SET:\")\n",
    "print(f\"  • Train: {X_train_unified.shape[0]:,} samples × {X_train_unified.shape[1]} features\")\n",
    "print(f\"  • Test: {X_test_unified.shape[0]:,} samples × {X_test_unified.shape[1]} features\")\n",
    "\n",
    "print(f\"\\n Feature List ({X_train_unified.shape[1]} features):\")\n",
    "feature_list = X_train_unified.columns.tolist()\n",
    "for i, feat in enumerate(feature_list[:15], 1):  # Show first 15\n",
    "    print(f\"  {i:2d}. {feat}\")\n",
    "if len(feature_list) > 15:\n",
    "    print(f\"  ... and {len(feature_list) - 15} more features\")\n",
    "\n",
    "# ============================================================================\n",
    "# STAP 4B: TRAIN RANDOM FOREST (UNIFIED FEATURES)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STAP 4B: TRAIN RANDOM FOREST WITH UNIFIED FEATURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nModel Configuration:\")\n",
    "print(f\"  • Algorithm: Random Forest Classifier\")\n",
    "print(f\"  • Features: {X_train_unified.shape[1]} (unified set)\")\n",
    "print(f\"  • n_estimators: 100\")\n",
    "print(f\"  • max_depth: None (full trees)\")\n",
    "print(f\"  • random_state: 42\")\n",
    "\n",
    "print(f\"\\n Training Random Forest (Unified)...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Initialize model\n",
    "rf_unified = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=None,\n",
    "    min_samples_split=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Train\n",
    "rf_unified.fit(X_train_unified, y_train)\n",
    "\n",
    "training_time_rf_unified = time.time() - start_time\n",
    "print(f\"✓ Training completed in {training_time_rf_unified:.2f} seconds\")\n",
    "\n",
    "# Predictions\n",
    "print(f\"\\n Making predictions...\")\n",
    "y_train_pred_rf_unified = rf_unified.predict(X_train_unified)\n",
    "y_test_pred_rf_unified = rf_unified.predict(X_test_unified)\n",
    "y_test_proba_rf_unified = rf_unified.predict_proba(X_test_unified)\n",
    "\n",
    "# Calculate metrics\n",
    "train_acc_rf_unified = accuracy_score(y_train, y_train_pred_rf_unified)\n",
    "test_acc_rf_unified = accuracy_score(y_test, y_test_pred_rf_unified)\n",
    "precision_rf_unified = precision_score(y_test, y_test_pred_rf_unified, average='weighted')\n",
    "recall_rf_unified = recall_score(y_test, y_test_pred_rf_unified, average='weighted')\n",
    "f1_rf_unified = f1_score(y_test, y_test_pred_rf_unified, average='weighted')\n",
    "\n",
    "print(f\"\\n RANDOM FOREST (UNIFIED) - PERFORMANCE:\")\n",
    "print(f\"  • Train Accuracy: {train_acc_rf_unified:.4f} ({train_acc_rf_unified*100:.2f}%)\")\n",
    "print(f\"  • Test Accuracy: {test_acc_rf_unified:.4f} ({test_acc_rf_unified*100:.2f}%)\")\n",
    "print(f\"  • Precision: {precision_rf_unified:.4f}\")\n",
    "print(f\"  • Recall: {recall_rf_unified:.4f}\")\n",
    "print(f\"  • F1-Score: {f1_rf_unified:.4f}\")\n",
    "\n",
    "# Overfitting check\n",
    "overfitting_rf_unified = train_acc_rf_unified - test_acc_rf_unified\n",
    "print(f\"\\n   Overfitting gap: {overfitting_rf_unified:.4f} ({overfitting_rf_unified*100:.2f}%)\")\n",
    "if overfitting_rf_unified < 0.05:\n",
    "    print(f\"   Minimal overfitting\")\n",
    "elif overfitting_rf_unified < 0.10:\n",
    "    print(f\"  Moderate overfitting\")\n",
    "else:\n",
    "    print(f\"   Significant overfitting\")\n",
    "\n",
    "# ============================================================================\n",
    "# STAP 4C: TRAIN LOGISTIC REGRESSION (UNIFIED FEATURES)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STAP 4C: TRAIN LOGISTIC REGRESSION WITH UNIFIED FEATURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nModel Configuration:\")\n",
    "print(f\"  • Algorithm: Logistic Regression\")\n",
    "print(f\"  • Features: {X_train_unified.shape[1]} (unified set - SAME as RF!)\")\n",
    "print(f\"  • multi_class: multinomial\")\n",
    "print(f\"  • solver: lbfgs\")\n",
    "print(f\"  • max_iter: 1000\")\n",
    "print(f\"  • random_state: 42\")\n",
    "\n",
    "print(f\"\\n Training Logistic Regression (Unified)...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Initialize model\n",
    "lr_unified = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    multi_class='multinomial',\n",
    "    solver='lbfgs',\n",
    "    random_state=42,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Train\n",
    "lr_unified.fit(X_train_unified, y_train)\n",
    "\n",
    "training_time_lr_unified = time.time() - start_time\n",
    "print(f\"✓ Training completed in {training_time_lr_unified:.2f} seconds\")\n",
    "\n",
    "# Predictions\n",
    "print(f\"\\n Making predictions...\")\n",
    "y_train_pred_lr_unified = lr_unified.predict(X_train_unified)\n",
    "y_test_pred_lr_unified = lr_unified.predict(X_test_unified)\n",
    "y_test_proba_lr_unified = lr_unified.predict_proba(X_test_unified)\n",
    "\n",
    "# Calculate metrics\n",
    "train_acc_lr_unified = accuracy_score(y_train, y_train_pred_lr_unified)\n",
    "test_acc_lr_unified = accuracy_score(y_test, y_test_pred_lr_unified)\n",
    "precision_lr_unified = precision_score(y_test, y_test_pred_lr_unified, average='weighted')\n",
    "recall_lr_unified = recall_score(y_test, y_test_pred_lr_unified, average='weighted')\n",
    "f1_lr_unified = f1_score(y_test, y_test_pred_lr_unified, average='weighted')\n",
    "\n",
    "print(f\"\\n LOGISTIC REGRESSION (UNIFIED) - PERFORMANCE:\")\n",
    "print(f\"  • Train Accuracy: {train_acc_lr_unified:.4f} ({train_acc_lr_unified*100:.2f}%)\")\n",
    "print(f\"  • Test Accuracy: {test_acc_lr_unified:.4f} ({test_acc_lr_unified*100:.2f}%)\")\n",
    "print(f\"  • Precision: {precision_lr_unified:.4f}\")\n",
    "print(f\"  • Recall: {recall_lr_unified:.4f}\")\n",
    "print(f\"  • F1-Score: {f1_lr_unified:.4f}\")\n",
    "\n",
    "# Overfitting check\n",
    "overfitting_lr_unified = train_acc_lr_unified - test_acc_lr_unified\n",
    "print(f\"\\n   Overfitting gap: {overfitting_lr_unified:.4f} ({overfitting_lr_unified*100:.2f}%)\")\n",
    "if overfitting_lr_unified < 0.05:\n",
    "    print(f\"   Minimal overfitting\")\n",
    "elif overfitting_lr_unified < 0.10:\n",
    "    print(f\"   Moderate overfitting\")\n",
    "else:\n",
    "    print(f\"   Significant overfitting\")\n",
    "\n",
    "# Sanity check\n",
    "unique_preds_lr = len(np.unique(y_test_pred_lr_unified))\n",
    "print(f\"\\n   Unique predictions: {unique_preds_lr}/3 classes\")\n",
    "if unique_preds_lr == 3:\n",
    "    print(f\"   Model predicts all classes (working correctly)\")\n",
    "else:\n",
    "    print(f\"   WARNING: Model only predicts {unique_preds_lr} classes!\")\n",
    "\n",
    "# ============================================================================\n",
    "# STAP 4D: COMPREHENSIVE COMPARISON - BASELINE vs UNIFIED\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STAP 4D: COMPREHENSIVE COMPARISON - BASELINE vs UNIFIED\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create comparison table\n",
    "comparison_unified = pd.DataFrame({\n",
    "    'Model': [\n",
    "        'RF - Baseline (Step 1)',\n",
    "        'RF - Unified (Step 4)',\n",
    "        'LR - Baseline (Step 1)', \n",
    "        'LR - Unified (Step 4)'\n",
    "    ],\n",
    "    'Features': [\n",
    "        X_train_baseline.shape[1],\n",
    "        X_train_unified.shape[1],\n",
    "        X_train_baseline.shape[1] if 'Diastolic' not in X_train_baseline.columns else X_train_baseline.shape[1] - 1,\n",
    "        X_train_unified.shape[1]\n",
    "    ],\n",
    "    'Train Acc': [\n",
    "        train_accuracy_rf if 'train_accuracy_rf' in locals() else 0.0,\n",
    "        train_acc_rf_unified,\n",
    "        train_accuracy_lr if 'train_accuracy_lr' in locals() else 0.0,\n",
    "        train_acc_lr_unified\n",
    "    ],\n",
    "    'Test Acc': [\n",
    "        test_accuracy_rf if 'test_accuracy_rf' in locals() else 0.0,\n",
    "        test_acc_rf_unified,\n",
    "        test_accuracy_lr if 'test_accuracy_lr' in locals() else 0.0,\n",
    "        test_acc_lr_unified\n",
    "    ],\n",
    "    'Precision': [\n",
    "        precision_rf if 'precision_rf' in locals() else 0.0,\n",
    "        precision_rf_unified,\n",
    "        precision_lr if 'precision_lr' in locals() else 0.0,\n",
    "        precision_lr_unified\n",
    "    ],\n",
    "    'Recall': [\n",
    "        recall_rf if 'recall_rf' in locals() else 0.0,\n",
    "        recall_rf_unified,\n",
    "        recall_lr if 'recall_lr' in locals() else 0.0,\n",
    "        recall_lr_unified\n",
    "    ],\n",
    "    'F1-Score': [\n",
    "        f1_rf if 'f1_rf' in locals() else 0.0,\n",
    "        f1_rf_unified,\n",
    "        f1_lr if 'f1_lr' in locals() else 0.0,\n",
    "        f1_lr_unified\n",
    "    ],\n",
    "    'Training Time': [\n",
    "        training_time_rf if 'training_time_rf' in locals() else 0.0,\n",
    "        training_time_rf_unified,\n",
    "        training_time_lr if 'training_time_lr' in locals() else 0.0,\n",
    "        training_time_lr_unified\n",
    "    ]\n",
    "})\n",
    "\n",
    "comparison_unified = comparison_unified.round(4)\n",
    "\n",
    "print(\"\\n PERFORMANCE COMPARISON TABLE:\")\n",
    "print(\"=\"*80)\n",
    "display(comparison_unified)\n",
    "\n",
    "# Calculate improvements\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"PERFORMANCE CHANGES (Baseline → Unified)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "if 'test_accuracy_rf' in locals():\n",
    "    rf_change = (test_acc_rf_unified - test_accuracy_rf) * 100\n",
    "    print(f\"\\nRANDOM FOREST:\")\n",
    "    print(f\"  • Accuracy: {test_accuracy_rf*100:.2f}% → {test_acc_rf_unified*100:.2f}% ({rf_change:+.2f}%)\")\n",
    "    print(f\"  • Features: {X_train_baseline.shape[1]} → {X_train_unified.shape[1]}\")\n",
    "    \n",
    "    if abs(rf_change) < 0.5:\n",
    "        print(f\"  ✓ Performance STABLE (minimal change)\")\n",
    "    elif rf_change > 0:\n",
    "        print(f\"  ✓ Performance IMPROVED\")\n",
    "    else:\n",
    "        print(f\"   Slight performance decrease (expected for RF)\")\n",
    "\n",
    "if 'test_accuracy_lr' in locals():\n",
    "    lr_change = (test_acc_lr_unified - test_accuracy_lr) * 100\n",
    "    print(f\"\\nLOGISTIC REGRESSION:\")\n",
    "    print(f\"  • Accuracy: {test_accuracy_lr*100:.2f}% → {test_acc_lr_unified*100:.2f}% ({lr_change:+.2f}%)\")\n",
    "    print(f\"  • Features: Both using {X_train_unified.shape[1]} (NOW CONSISTENT!)\")\n",
    "    \n",
    "    if abs(lr_change) < 0.5:\n",
    "        print(f\"  ✓ Performance STABLE\")\n",
    "    elif lr_change > 0:\n",
    "        print(f\"  ✓ Performance IMPROVED (multicollineariteit reduced!)\")\n",
    "    else:\n",
    "        print(f\"   Performance decreased\")\n",
    "\n",
    "# ============================================================================\n",
    "# STAP 4E: DETAILED EVALUATION - CLASSIFICATION REPORTS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STAP 4E: DETAILED CLASSIFICATION REPORTS (UNIFIED MODELS)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"RANDOM FOREST (UNIFIED) - CLASSIFICATION REPORT\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(\"\\nClassification Report (Test Set):\")\n",
    "print(classification_report(\n",
    "    y_test, \n",
    "    y_test_pred_rf_unified,\n",
    "    target_names=['Insomnia', 'None', 'Sleep Apnea'],\n",
    "    digits=4\n",
    "))\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"LOGISTIC REGRESSION (UNIFIED) - CLASSIFICATION REPORT\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(\"\\nClassification Report (Test Set):\")\n",
    "print(classification_report(\n",
    "    y_test,\n",
    "    y_test_pred_lr_unified,\n",
    "    target_names=['Insomnia', 'None', 'Sleep Apnea'],\n",
    "    digits=4\n",
    "))\n",
    "\n",
    "# ============================================================================\n",
    "# STAP 4F: VISUALIZATION - CONFUSION MATRICES\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STAP 4F: CONFUSION MATRICES (UNIFIED MODELS)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Random Forest Confusion Matrix\n",
    "cm_rf_unified = confusion_matrix(y_test, y_test_pred_rf_unified)\n",
    "sns.heatmap(cm_rf_unified, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Insomnia', 'None', 'Sleep Apnea'],\n",
    "            yticklabels=['Insomnia', 'None', 'Sleep Apnea'],\n",
    "            ax=axes[0], cbar_kws={'label': 'Count'})\n",
    "axes[0].set_title(f'Random Forest (Unified)\\nAccuracy: {test_acc_rf_unified*100:.2f}% | F1: {f1_rf_unified:.4f}',\n",
    "                  fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Actual Class')\n",
    "axes[0].set_xlabel('Predicted Class')\n",
    "\n",
    "# Logistic Regression Confusion Matrix\n",
    "cm_lr_unified = confusion_matrix(y_test, y_test_pred_lr_unified)\n",
    "sns.heatmap(cm_lr_unified, annot=True, fmt='d', cmap='Greens',\n",
    "            xticklabels=['Insomnia', 'None', 'Sleep Apnea'],\n",
    "            yticklabels=['Insomnia', 'None', 'Sleep Apnea'],\n",
    "            ax=axes[1], cbar_kws={'label': 'Count'})\n",
    "axes[1].set_title(f'Logistic Regression (Unified)\\nAccuracy: {test_acc_lr_unified*100:.2f}% | F1: {f1_lr_unified:.4f}',\n",
    "                  fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Actual Class')\n",
    "axes[1].set_xlabel('Predicted Class')\n",
    "\n",
    "plt.suptitle('Confusion Matrices - Unified Models (Test Set)', \n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# STAP 4G: PERFORMANCE COMPARISON VISUALIZATION (FIXED)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STAP 4G: PERFORMANCE COMPARISON VISUALIZATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Ensure dark mode or standard backgrounds don't hide labels\n",
    "plt.style.use('seaborn-v0_8-whitegrid') \n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# --- PLOT 1: TEST ACCURACY COMPARISON ---\n",
    "models = ['RF Baseline', 'RF Unified', 'LR Baseline', 'LR Unified']\n",
    "# Handle potential missing baseline variables\n",
    "rf_base_acc = test_accuracy_rf if 'test_accuracy_rf' in locals() else 0\n",
    "lr_base_acc = test_accuracy_lr if 'test_accuracy_lr' in locals() else 0\n",
    "test_accs = [rf_base_acc, test_acc_rf_unified, lr_base_acc, test_acc_lr_unified]\n",
    "\n",
    "colors = ['lightblue', 'steelblue', 'lightgreen', 'seagreen']\n",
    "bars1 = axes[0, 0].bar(models, test_accs, color=colors, alpha=0.8)\n",
    "axes[0, 0].set_ylabel('Test Accuracy')\n",
    "axes[0, 0].set_title('Test Accuracy: Baseline vs Unified', fontweight='bold', fontsize=12)\n",
    "axes[0, 0].set_ylim([0, 1.05]) # Fixed: Adjusted from 0.85 to 0 to show bars\n",
    "axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    if height > 0:\n",
    "        axes[0, 0].text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "                       f'{height:.4f}\\n({height*100:.1f}%)',\n",
    "                       ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "# --- PLOT 2: F1-SCORE COMPARISON ---\n",
    "rf_base_f1 = f1_rf if 'f1_rf' in locals() else 0\n",
    "lr_base_f1 = f1_lr if 'f1_lr' in locals() else 0\n",
    "f1_vals = [rf_base_f1, f1_rf_unified, lr_base_f1, f1_lr_unified]\n",
    "\n",
    "bars2 = axes[0, 1].bar(models, f1_vals, color=colors, alpha=0.8)\n",
    "axes[0, 1].set_ylabel('F1-Score (Weighted)')\n",
    "axes[0, 1].set_title('F1-Score: Baseline vs Unified', fontweight='bold', fontsize=12)\n",
    "axes[0, 1].set_ylim([0, 1.05]) # Fixed: Adjusted from 0.85 to 0\n",
    "axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "for bar in bars2:\n",
    "    height = bar.get_height()\n",
    "    if height > 0:\n",
    "        axes[0, 1].text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "                       f'{height:.4f}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "# --- PLOT 3: PER-CLASS PERFORMANCE (RF UNIFIED) ---\n",
    "rf_report = classification_report(y_test, y_test_pred_rf_unified, output_dict=True)\n",
    "classes = ['Insomnia', 'None', 'Sleep Apnea']\n",
    "# Map numerical keys if necessary ('0', '1', '2')\n",
    "rf_metrics = {\n",
    "    'Precision': [rf_report[str(i)]['precision'] for i in range(3)],\n",
    "    'Recall': [rf_report[str(i)]['recall'] for i in range(3)],\n",
    "    'F1-Score': [rf_report[str(i)]['f1-score'] for i in range(3)]\n",
    "}\n",
    "\n",
    "x = np.arange(len(classes))\n",
    "width = 0.2\n",
    "\n",
    "axes[1, 0].bar(x - width, rf_metrics['Precision'], width, label='Precision', color='steelblue', alpha=0.8)\n",
    "axes[1, 0].bar(x, rf_metrics['Recall'], width, label='Recall', color='orange', alpha=0.8)\n",
    "axes[1, 0].bar(x + width, rf_metrics['F1-Score'], width, label='F1-Score', color='green', alpha=0.8)\n",
    "axes[1, 0].set_ylabel('Score')\n",
    "axes[1, 0].set_title('Random Forest (Unified) - Per-Class Metrics', fontweight='bold', fontsize=12)\n",
    "axes[1, 0].set_xticks(x)\n",
    "axes[1, 0].set_xticklabels(classes)\n",
    "axes[1, 0].legend(loc='lower right')\n",
    "axes[1, 0].set_ylim([0, 1.1]) # Fixed: Adjusted from 0.8 to 0\n",
    "axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# --- PLOT 4: PER-CLASS PERFORMANCE (LR UNIFIED) ---\n",
    "lr_report = classification_report(y_test, y_test_pred_lr_unified, output_dict=True)\n",
    "lr_metrics = {\n",
    "    'Precision': [lr_report[str(i)]['precision'] for i in range(3)],\n",
    "    'Recall': [lr_report[str(i)]['recall'] for i in range(3)],\n",
    "    'F1-Score': [lr_report[str(i)]['f1-score'] for i in range(3)]\n",
    "}\n",
    "\n",
    "axes[1, 1].bar(x - width, lr_metrics['Precision'], width, label='Precision', color='seagreen', alpha=0.8)\n",
    "axes[1, 1].bar(x, lr_metrics['Recall'], width, label='Recall', color='orange', alpha=0.8)\n",
    "axes[1, 1].bar(x + width, lr_metrics['F1-Score'], width, label='F1-Score', color='purple', alpha=0.8)\n",
    "axes[1, 1].set_ylabel('Score')\n",
    "axes[1, 1].set_title('Logistic Regression (Unified) - Per-Class Metrics', fontweight='bold', fontsize=12)\n",
    "axes[1, 1].set_xticks(x)\n",
    "axes[1, 1].set_xticklabels(classes)\n",
    "axes[1, 1].legend(loc='lower right')\n",
    "axes[1, 1].set_ylim([0, 1.1]) # Fixed: Adjusted from 0.8 to 0\n",
    "axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.suptitle('Step 4: Unified Models Performance Analysis', fontsize=16, fontweight='bold', y=0.98)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# STAP 4H: FINAL SUMMARY & RECOMMENDATIONS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STAP 4H: FINAL SUMMARY & RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\"\"\n",
    "UNIFIED FEATURE SET RESULTS:\n",
    "============================\n",
    "\n",
    "FEATURE REDUCTION:\n",
    "• Baseline: {X_train_baseline.shape[1]} features\n",
    "• Unified: {X_train_unified.shape[1]} features\n",
    "• Removed: {X_train_baseline.shape[1] - X_train_unified.shape[1]} features ({(X_train_baseline.shape[1] - X_train_unified.shape[1])/X_train_baseline.shape[1]*100:.1f}% reduction)\n",
    "• Removed features: {', '.join(features_to_remove_unified)}\n",
    "\n",
    "RANDOM FOREST (UNIFIED):\n",
    "• Test Accuracy: {test_acc_rf_unified*100:.2f}%\n",
    "• F1-Score: {f1_rf_unified:.4f}\n",
    "• Training Time: {training_time_rf_unified:.2f}s\n",
    "• Status: ✓ Stable performance with fewer features\n",
    "\n",
    "LOGISTIC REGRESSION (UNIFIED):\n",
    "• Test Accuracy: {test_acc_lr_unified*100:.2f}%\n",
    "• F1-Score: {f1_lr_unified:.4f}\n",
    "• Training Time: {training_time_lr_unified:.2f}s\n",
    "• Status: ✓ Improved stability (multicollineariteit reduced)\n",
    "\n",
    "KEY IMPROVEMENTS:\n",
    "=================\n",
    "\n",
    "1. ✓ CONSISTENCY: Both models now use SAME {X_train_unified.shape[1]} features\n",
    "2. ✓ MULTICOLLINEARITEIT: Diastolic removed (r=0.979 with Systolic)\n",
    "3. ✓ STABILITY: Logistic Regression trains without numerical issues\n",
    "4. ✓ EFFICIENCY: Faster training with fewer features\n",
    "5. ✓ INTERPRETABILITY: Cleaner model without redundant features\n",
    "\"\"\")\n",
    "\n",
    "# Determine best model\n",
    "if test_acc_rf_unified > test_acc_lr_unified:\n",
    "    best_model = \"Random Forest\"\n",
    "    best_acc = test_acc_rf_unified\n",
    "    best_f1 = f1_rf_unified\n",
    "else:\n",
    "    best_model = \"Logistic Regression\"\n",
    "    best_acc = test_acc_lr_unified\n",
    "    best_f1 = f1_lr_unified\n",
    "\n",
    "print(f\"\\n BESTE MODEL:\")\n",
    "print(f\"  • Model: {best_model}\")\n",
    "print(f\"  • Test Accuracy: {best_acc*100:.2f}%\")\n",
    "print(f\"  • F1-Score: {best_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1554e6",
   "metadata": {},
   "source": [
    "## Stap 5 – Hyperparameter tuning (heel kort)\n",
    "\n",
    "**Doel:** betere performance dan defaults door hyperparameters te optimaliseren.\n",
    "\n",
    "**Aanpak:** `RandomizedSearchCV` + **Stratified 5-fold CV**\n",
    "- Fit alleen op `X_train_unified`, evaluatie via CV (geen leakage).\n",
    "\n",
    "**Wat tunen we?**\n",
    "- **Random Forest:** o.a. `n_estimators`, `max_depth`, `min_samples_split/leaf`, `max_features`, `bootstrap`\n",
    "- **Logistic Regression:** `C`, `solver`, `max_iter`, `class_weight` *(penalty blijft l2 voor compatibiliteit)*\n",
    "\n",
    "**Na tuning:**\n",
    "- Pak `best_params_` + `best_score_` (CV accuracy)\n",
    "- Train final tuned model met `best_params_`\n",
    "- Test op `X_test_unified`: Accuracy/Precision/Recall/F1 + confusion matrix\n",
    "- Vergelijk alle modellen: **Baseline → Unified → Tuned** en kies beste.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b0a462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# UITWERKING ML VRAAGSTUK\n",
    "# STAP 5: HYPERPARAMETER TUNING - OPTIMALISATIE VAN MODELPERFORMANCE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")\n",
    "\n",
    "# ============================================================================\n",
    "# STAP 5A: VERIFY DATA STATUS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STAP 5A: VERIFY DATA STATUS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check if we have unified data from Step 4\n",
    "if 'X_train_unified' not in locals() or 'X_test_unified' not in locals():\n",
    "    print(\"\\n Unified data not found - using current X_train and X_test\")\n",
    "    X_train_unified = X_train.copy()\n",
    "    X_test_unified = X_test.copy()\n",
    "\n",
    "print(f\"\\nDATA FOR TUNING:\")\n",
    "print(f\"  • X_train: {X_train_unified.shape[0]:,} samples × {X_train_unified.shape[1]} features\")\n",
    "print(f\"  • X_test: {X_test_unified.shape[0]:,} samples × {X_test_unified.shape[1]} features\")\n",
    "print(f\"  • y_train: {len(y_train):,} samples\")\n",
    "print(f\"  • y_test: {len(y_test):,} samples\")\n",
    "\n",
    "print(f\"\\n✓ Data ready for hyperparameter tuning\")\n",
    "\n",
    "# ============================================================================\n",
    "# STAP 5B: RANDOM FOREST HYPERPARAMETER TUNING\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STAP 5B: RANDOM FOREST - HYPERPARAMETER TUNING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nDEFINING PARAMETER GRID FOR RANDOM FOREST:\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Define parameter distributions for RandomizedSearchCV\n",
    "rf_param_distributions = {\n",
    "    'n_estimators': [50, 100, 150, 200, 300],\n",
    "    'max_depth': [10, 20, 30, 40, None],\n",
    "    'min_samples_split': [2, 5, 10, 15],\n",
    "    'min_samples_leaf': [1, 2, 4, 8],\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "print(\"\\nParameter distributions:\")\n",
    "for param, values in rf_param_distributions.items():\n",
    "    print(f\"  • {param}: {values}\")\n",
    "\n",
    "total_combinations = 1\n",
    "for values in rf_param_distributions.values():\n",
    "    total_combinations *= len(values)\n",
    "\n",
    "print(f\"\\nTotal possible combinations: {total_combinations:,}\")\n",
    "print(f\"We will sample: 50 random combinations (Randomized Search)\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"SETTING UP RANDOMIZED SEARCH\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Initialize base model\n",
    "rf_base = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "\n",
    "# Setup cross-validation\n",
    "cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "print(\"\\nCross-Validation Strategy:\")\n",
    "print(f\"  • Method: Stratified K-Fold\")\n",
    "print(f\"  • Number of folds: 5\")\n",
    "print(f\"  • Shuffle: True\")\n",
    "print(f\"  • Random state: 42\")\n",
    "\n",
    "# Setup RandomizedSearchCV\n",
    "rf_random_search = RandomizedSearchCV(\n",
    "    estimator=rf_base,\n",
    "    param_distributions=rf_param_distributions,\n",
    "    n_iter=50,  # Number of random combinations to try\n",
    "    cv=cv_strategy,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "print(\"\\nRandomizedSearchCV Configuration:\")\n",
    "print(f\"  • Estimator: Random Forest Classifier\")\n",
    "print(f\"  • Iterations: 50 (random samples)\")\n",
    "print(f\"  • Scoring metric: Accuracy\")\n",
    "print(f\"  • Parallel jobs: -1 (all CPU cores)\")\n",
    "print(f\"  • Return train score: True\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"RUNNING RANDOMIZED SEARCH FOR RANDOM FOREST\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(\"\\n This may take several minutes...\")\n",
    "print(\"   Training 50 combinations × 5 folds = 250 models total\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Fit RandomizedSearchCV\n",
    "rf_random_search.fit(X_train_unified, y_train)\n",
    "\n",
    "tuning_time_rf = time.time() - start_time\n",
    "\n",
    "print(f\"\\n✓ Tuning completed in {tuning_time_rf:.2f} seconds ({tuning_time_rf/60:.2f} minutes)\")\n",
    "\n",
    "# Get best parameters\n",
    "rf_best_params = rf_random_search.best_params_\n",
    "rf_best_score = rf_random_search.best_score_\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"RANDOM FOREST - BEST HYPERPARAMETERS FOUND\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(\"\\nBest parameters:\")\n",
    "for param, value in rf_best_params.items():\n",
    "    print(f\"  • {param}: {value}\")\n",
    "\n",
    "print(f\"\\nBest cross-validation score (mean accuracy across 5 folds):\")\n",
    "print(f\"  • {rf_best_score:.4f} ({rf_best_score*100:.2f}%)\")\n",
    "\n",
    "# Compare with baseline (if available)\n",
    "if 'test_acc_rf_unified' in locals():\n",
    "    improvement_rf = (rf_best_score - test_acc_rf_unified) * 100\n",
    "    print(f\"\\nComparison with baseline (Step 4):\")\n",
    "    print(f\"  • Baseline (default params): {test_acc_rf_unified*100:.2f}%\")\n",
    "    print(f\"  • Tuned (optimized params): {rf_best_score*100:.2f}%\")\n",
    "    print(f\"  • Improvement: {improvement_rf:+.2f}%\")\n",
    "    \n",
    "    if improvement_rf > 0.5:\n",
    "        print(f\"  ✓ Significant improvement achieved!\")\n",
    "    elif improvement_rf > 0:\n",
    "        print(f\"  ✓ Slight improvement\")\n",
    "    else:\n",
    "        print(f\"  ⚠ No improvement - default params were already good\")\n",
    "\n",
    "# Train final model with best parameters\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"TRAINING FINAL RANDOM FOREST WITH BEST PARAMETERS\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "rf_tuned = RandomForestClassifier(**rf_best_params, random_state=42, n_jobs=-1)\n",
    "rf_tuned.fit(X_train_unified, y_train)\n",
    "\n",
    "# Evaluate on test set\n",
    "y_test_pred_rf_tuned = rf_tuned.predict(X_test_unified)\n",
    "test_acc_rf_tuned = accuracy_score(y_test, y_test_pred_rf_tuned)\n",
    "precision_rf_tuned = precision_score(y_test, y_test_pred_rf_tuned, average='weighted')\n",
    "recall_rf_tuned = recall_score(y_test, y_test_pred_rf_tuned, average='weighted')\n",
    "f1_rf_tuned = f1_score(y_test, y_test_pred_rf_tuned, average='weighted')\n",
    "\n",
    "print(f\"\\n RANDOM FOREST (TUNED) - TEST SET PERFORMANCE:\")\n",
    "print(f\"  • Test Accuracy: {test_acc_rf_tuned:.4f} ({test_acc_rf_tuned*100:.2f}%)\")\n",
    "print(f\"  • Precision: {precision_rf_tuned:.4f}\")\n",
    "print(f\"  • Recall: {recall_rf_tuned:.4f}\")\n",
    "print(f\"  • F1-Score: {f1_rf_tuned:.4f}\")\n",
    "\n",
    "print(f\"\\n✓ Random Forest tuning complete!\")\n",
    "\n",
    "# ============================================================================\n",
    "# STAP 5C: LOGISTIC REGRESSION HYPERPARAMETER TUNING\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STAP 5C: LOGISTIC REGRESSION - HYPERPARAMETER TUNING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nDEFINING PARAMETER GRID FOR LOGISTIC REGRESSION:\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Define parameter distributions\n",
    "lr_param_distributions = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "    'penalty': ['l2'],  # l2 works with all solvers\n",
    "    'solver': ['lbfgs', 'saga', 'liblinear'],\n",
    "    'max_iter': [1000, 2000, 3000],\n",
    "    'class_weight': [None, 'balanced']\n",
    "}\n",
    "\n",
    "print(\"\\nParameter distributions:\")\n",
    "for param, values in lr_param_distributions.items():\n",
    "    print(f\"  • {param}: {values}\")\n",
    "\n",
    "total_combinations_lr = 1\n",
    "for values in lr_param_distributions.values():\n",
    "    total_combinations_lr *= len(values)\n",
    "\n",
    "print(f\"\\nTotal possible combinations: {total_combinations_lr:,}\")\n",
    "print(f\"We will sample: 30 random combinations (Randomized Search)\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"SETTING UP RANDOMIZED SEARCH\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Initialize base model\n",
    "lr_base = LogisticRegression(random_state=42, multi_class='multinomial')\n",
    "\n",
    "# Setup RandomizedSearchCV\n",
    "lr_random_search = RandomizedSearchCV(\n",
    "    estimator=lr_base,\n",
    "    param_distributions=lr_param_distributions,\n",
    "    n_iter=30,  # Fewer iterations (LR is faster than RF)\n",
    "    cv=cv_strategy,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "print(\"\\nRandomizedSearchCV Configuration:\")\n",
    "print(f\"  • Estimator: Logistic Regression\")\n",
    "print(f\"  • Iterations: 30 (random samples)\")\n",
    "print(f\"  • Scoring metric: Accuracy\")\n",
    "print(f\"  • Parallel jobs: -1 (all CPU cores)\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"RUNNING RANDOMIZED SEARCH FOR LOGISTIC REGRESSION\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(\"\\n This may take a few minutes...\")\n",
    "print(\"   Training 30 combinations × 5 folds = 150 models total\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Fit RandomizedSearchCV\n",
    "lr_random_search.fit(X_train_unified, y_train)\n",
    "\n",
    "tuning_time_lr = time.time() - start_time\n",
    "\n",
    "print(f\"\\n✓ Tuning completed in {tuning_time_lr:.2f} seconds ({tuning_time_lr/60:.2f} minutes)\")\n",
    "\n",
    "# Get best parameters\n",
    "lr_best_params = lr_random_search.best_params_\n",
    "lr_best_score = lr_random_search.best_score_\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"LOGISTIC REGRESSION - BEST HYPERPARAMETERS FOUND\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(\"\\nBest parameters:\")\n",
    "for param, value in lr_best_params.items():\n",
    "    print(f\"  • {param}: {value}\")\n",
    "\n",
    "print(f\"\\nBest cross-validation score (mean accuracy across 5 folds):\")\n",
    "print(f\"  • {lr_best_score:.4f} ({lr_best_score*100:.2f}%)\")\n",
    "\n",
    "# Compare with baseline\n",
    "if 'test_acc_lr_unified' in locals():\n",
    "    improvement_lr = (lr_best_score - test_acc_lr_unified) * 100\n",
    "    print(f\"\\nComparison with baseline (Step 4):\")\n",
    "    print(f\"  • Baseline (default params): {test_acc_lr_unified*100:.2f}%\")\n",
    "    print(f\"  • Tuned (optimized params): {lr_best_score*100:.2f}%\")\n",
    "    print(f\"  • Improvement: {improvement_lr:+.2f}%\")\n",
    "    \n",
    "    if improvement_lr > 0.5:\n",
    "        print(f\"  ✓ Significant improvement achieved!\")\n",
    "    elif improvement_lr > 0:\n",
    "        print(f\"  ✓ Slight improvement\")\n",
    "    else:\n",
    "        print(f\"  ⚠ No improvement - default params were already good\")\n",
    "\n",
    "# Train final model with best parameters\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"TRAINING FINAL LOGISTIC REGRESSION WITH BEST PARAMETERS\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "lr_tuned = LogisticRegression(**lr_best_params, random_state=42, multi_class='multinomial')\n",
    "lr_tuned.fit(X_train_unified, y_train)\n",
    "\n",
    "# Evaluate on test set\n",
    "y_test_pred_lr_tuned = lr_tuned.predict(X_test_unified)\n",
    "test_acc_lr_tuned = accuracy_score(y_test, y_test_pred_lr_tuned)\n",
    "precision_lr_tuned = precision_score(y_test, y_test_pred_lr_tuned, average='weighted')\n",
    "recall_lr_tuned = recall_score(y_test, y_test_pred_lr_tuned, average='weighted')\n",
    "f1_lr_tuned = f1_score(y_test, y_test_pred_lr_tuned, average='weighted')\n",
    "\n",
    "print(f\"\\n LOGISTIC REGRESSION (TUNED) - TEST SET PERFORMANCE:\")\n",
    "print(f\"  • Test Accuracy: {test_acc_lr_tuned:.4f} ({test_acc_lr_tuned*100:.2f}%)\")\n",
    "print(f\"  • Precision: {precision_lr_tuned:.4f}\")\n",
    "print(f\"  • Recall: {recall_lr_tuned:.4f}\")\n",
    "print(f\"  • F1-Score: {f1_lr_tuned:.4f}\")\n",
    "\n",
    "print(f\"\\n✓ Logistic Regression tuning complete!\")\n",
    "\n",
    "# ============================================================================\n",
    "# STAP 5D: COMPREHENSIVE COMPARISON - ALL MODELS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STAP 5D: COMPREHENSIVE COMPARISON - ALL MODELS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create comprehensive comparison table\n",
    "all_models_comparison = pd.DataFrame({\n",
    "    'Model': [\n",
    "        'RF - Baseline (Step 1)',\n",
    "        'RF - Unified (Step 4)',\n",
    "        'RF - Tuned (Step 5)',\n",
    "        'LR - Baseline (Step 1)',\n",
    "        'LR - Unified (Step 4)',\n",
    "        'LR - Tuned (Step 5)'\n",
    "    ],\n",
    "    'Test Accuracy': [\n",
    "        test_accuracy_rf if 'test_accuracy_rf' in locals() else np.nan,\n",
    "        test_acc_rf_unified if 'test_acc_rf_unified' in locals() else np.nan,\n",
    "        test_acc_rf_tuned,\n",
    "        test_accuracy_lr if 'test_accuracy_lr' in locals() else np.nan,\n",
    "        test_acc_lr_unified if 'test_acc_lr_unified' in locals() else np.nan,\n",
    "        test_acc_lr_tuned\n",
    "    ],\n",
    "    'Precision': [\n",
    "        precision_rf if 'precision_rf' in locals() else np.nan,\n",
    "        precision_rf_unified if 'precision_rf_unified' in locals() else np.nan,\n",
    "        precision_rf_tuned,\n",
    "        precision_lr if 'precision_lr' in locals() else np.nan,\n",
    "        precision_lr_unified if 'precision_lr_unified' in locals() else np.nan,\n",
    "        precision_lr_tuned\n",
    "    ],\n",
    "    'Recall': [\n",
    "        recall_rf if 'recall_rf' in locals() else np.nan,\n",
    "        recall_rf_unified if 'recall_rf_unified' in locals() else np.nan,\n",
    "        recall_rf_tuned,\n",
    "        recall_lr if 'recall_lr' in locals() else np.nan,\n",
    "        recall_lr_unified if 'recall_lr_unified' in locals() else np.nan,\n",
    "        recall_lr_tuned\n",
    "    ],\n",
    "    'F1-Score': [\n",
    "        f1_rf if 'f1_rf' in locals() else np.nan,\n",
    "        f1_rf_unified if 'f1_rf_unified' in locals() else np.nan,\n",
    "        f1_rf_tuned,\n",
    "        f1_lr if 'f1_lr' in locals() else np.nan,\n",
    "        f1_lr_unified if 'f1_lr_unified' in locals() else np.nan,\n",
    "        f1_lr_tuned\n",
    "    ]\n",
    "})\n",
    "\n",
    "all_models_comparison = all_models_comparison.round(4)\n",
    "\n",
    "print(\"\\n COMPLETE MODEL EVOLUTION:\")\n",
    "print(\"=\"*80)\n",
    "display(all_models_comparison)\n",
    "# ============================================================================\n",
    "# STAP 5E: VISUALIZATION - TUNING RESULTS (REVISED)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STAP 5E: VISUALIZATION - TUNING DISTRIBUTIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# We now focus on the internal tuning performance (how the search performed)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Random Forest - CV Results Distribution\n",
    "rf_cv_results = pd.DataFrame(rf_random_search.cv_results_)\n",
    "top_20_indices = rf_cv_results.nlargest(20, 'mean_test_score').index\n",
    "\n",
    "axes[0].boxplot([rf_cv_results.loc[i, [f'split{j}_test_score' for j in range(5)]].values \n",
    "                     for i in top_20_indices],\n",
    "                    labels=[f'{i+1}' for i in range(20)])\n",
    "axes[0].axhline(y=rf_best_score, color='red', linestyle='--', linewidth=2, label='Best CV Score')\n",
    "axes[0].set_xlabel('Top 20 Hyperparameter Combinations')\n",
    "axes[0].set_ylabel('Cross-Validation Accuracy')\n",
    "axes[0].set_title('Random Forest - CV Score Distribution (Top 20)', fontweight='bold', fontsize=12)\n",
    "axes[0].legend()\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 2: Logistic Regression - CV Results Distribution\n",
    "lr_cv_results = pd.DataFrame(lr_random_search.cv_results_)\n",
    "top_15_indices = lr_cv_results.nlargest(15, 'mean_test_score').index\n",
    "\n",
    "axes[1].boxplot([lr_cv_results.loc[i, [f'split{j}_test_score' for j in range(5)]].values \n",
    "                     for i in top_15_indices],\n",
    "                    labels=[f'{i+1}' for i in range(15)])\n",
    "axes[1].axhline(y=lr_best_score, color='red', linestyle='--', linewidth=2, label='Best CV Score')\n",
    "axes[1].set_xlabel('Top 15 Hyperparameter Combinations')\n",
    "axes[1].set_ylabel('Cross-Validation Accuracy')\n",
    "axes[1].set_title('Logistic Regression - CV Score Distribution (Top 15)', fontweight='bold', fontsize=12)\n",
    "axes[1].legend()\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.suptitle('Step 5: Hyperparameter Tuning - Cross-Validation Stability', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# STAP 5F: CONFUSION MATRICES - TUNED MODELS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STAP 5F: CONFUSION MATRICES - TUNED MODELS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Random Forest - Tuned\n",
    "cm_rf_tuned = confusion_matrix(y_test, y_test_pred_rf_tuned)\n",
    "sns.heatmap(cm_rf_tuned, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Insomnia', 'None', 'Sleep Apnea'],\n",
    "            yticklabels=['Insomnia', 'None', 'Sleep Apnea'],\n",
    "            ax=axes[0], cbar_kws={'label': 'Count'})\n",
    "axes[0].set_title(f'Random Forest (Tuned)\\nAccuracy: {test_acc_rf_tuned*100:.2f}%',\n",
    "                  fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Actual Class')\n",
    "axes[0].set_xlabel('Predicted Class')\n",
    "\n",
    "# Logistic Regression - Tuned\n",
    "cm_lr_tuned = confusion_matrix(y_test, y_test_pred_lr_tuned)\n",
    "sns.heatmap(cm_lr_tuned, annot=True, fmt='d', cmap='Greens',\n",
    "            xticklabels=['Insomnia', 'None', 'Sleep Apnea'],\n",
    "            yticklabels=['Insomnia', 'None', 'Sleep Apnea'],\n",
    "            ax=axes[1], cbar_kws={'label': 'Count'})\n",
    "axes[1].set_title(f'Logistic Regression (Tuned)\\nAccuracy: {test_acc_lr_tuned*100:.2f}%',\n",
    "                  fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Actual Class')\n",
    "axes[1].set_xlabel('Predicted Class')\n",
    "\n",
    "plt.suptitle('Final Model Performance - Confusion Matrices', \n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# STAP 5F: CONFUSION MATRICES - TUNED MODELS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STAP 5F: CONFUSION MATRICES - TUNED MODELS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Random Forest - Tuned\n",
    "cm_rf_tuned = confusion_matrix(y_test, y_test_pred_rf_tuned)\n",
    "sns.heatmap(cm_rf_tuned, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Insomnia', 'None', 'Sleep Apnea'],\n",
    "            yticklabels=['Insomnia', 'None', 'Sleep Apnea'],\n",
    "            ax=axes[0], cbar_kws={'label': 'Count'})\n",
    "axes[0].set_title(f'Random Forest (Tuned)\\nAccuracy: {test_acc_rf_tuned*100:.2f}% | F1: {f1_rf_tuned:.4f}',\n",
    "                  fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Actual Class')\n",
    "axes[0].set_xlabel('Predicted Class')\n",
    "\n",
    "# Logistic Regression - Tuned\n",
    "cm_lr_tuned = confusion_matrix(y_test, y_test_pred_lr_tuned)\n",
    "sns.heatmap(cm_lr_tuned, annot=True, fmt='d', cmap='Greens',\n",
    "            xticklabels=['Insomnia', 'None', 'Sleep Apnea'],\n",
    "            yticklabels=['Insomnia', 'None', 'Sleep Apnea'],\n",
    "            ax=axes[1], cbar_kws={'label': 'Count'})\n",
    "axes[1].set_title(f'Logistic Regression (Tuned)\\nAccuracy: {test_acc_lr_tuned*100:.2f}% | F1: {f1_lr_tuned:.4f}',\n",
    "                  fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Actual Class')\n",
    "axes[1].set_xlabel('Predicted Class')\n",
    "\n",
    "plt.suptitle('Confusion Matrices - Tuned Models (Test Set)', \n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# STAP 5G: DETAILED CLASSIFICATION REPORTS - TUNED MODELS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STAP 5G: DETAILED CLASSIFICATION REPORTS - TUNED MODELS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"RANDOM FOREST (TUNED) - CLASSIFICATION REPORT\")\n",
    "print(\"-\"*80)\n",
    "print(classification_report(y_test, y_test_pred_rf_tuned,\n",
    "                          target_names=['Insomnia', 'None', 'Sleep Apnea'],\n",
    "                          digits=4))\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"LOGISTIC REGRESSION (TUNED) - CLASSIFICATION REPORT\")\n",
    "print(\"-\"*80)\n",
    "print(classification_report(y_test, y_test_pred_lr_tuned,\n",
    "                          target_names=['Insomnia', 'None', 'Sleep Apnea'],\n",
    "                          digits=4))\n",
    "\n",
    "# ============================================================================\n",
    "# STAP 5H: FINAL SUMMARY & BEST MODEL SELECTION\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STAP 5H: FINAL SUMMARY & BEST MODEL SELECTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\"\"\n",
    "HYPERPARAMETER TUNING RESULTS:\n",
    "===============================\n",
    "\n",
    "RANDOM FOREST:\n",
    "• Tuning time: {tuning_time_rf/60:.2f} minutes\n",
    "• Best CV score: {rf_best_score*100:.2f}%\n",
    "• Test accuracy: {test_acc_rf_tuned*100:.2f}%\n",
    "• F1-Score: {f1_rf_tuned:.4f}\n",
    "\n",
    "Best hyperparameters:\n",
    "\"\"\")\n",
    "for param, value in rf_best_params.items():\n",
    "    print(f\"  • {param}: {value}\")\n",
    "\n",
    "print(f\"\"\"\n",
    "LOGISTIC REGRESSION:\n",
    "• Tuning time: {tuning_time_lr/60:.2f} minutes\n",
    "• Best CV score: {lr_best_score*100:.2f}%\n",
    "• Test accuracy: {test_acc_lr_tuned*100:.2f}%\n",
    "• F1-Score: {f1_lr_tuned:.4f}\n",
    "\n",
    "Best hyperparameters:\n",
    "\"\"\")\n",
    "for param, value in lr_best_params.items():\n",
    "    print(f\"  • {param}: {value}\")\n",
    "\n",
    "# Determine overall best model\n",
    "best_overall_acc = max(test_acc_rf_tuned, test_acc_lr_tuned)\n",
    "if test_acc_rf_tuned > test_acc_lr_tuned:\n",
    "    best_overall_model = \"Random Forest (Tuned)\"\n",
    "    best_overall_f1 = f1_rf_tuned\n",
    "else:\n",
    "    best_overall_model = \"Logistic Regression (Tuned)\"\n",
    "    best_overall_f1 = f1_lr_tuned\n",
    "\n",
    "print(f\"\\n BEST OVERALL MODEL:\")\n",
    "print(f\"  • Model: {best_overall_model}\")\n",
    "print(f\"  • Test Accuracy: {best_overall_acc*100:.2f}%\")\n",
    "print(f\"  • F1-Score: {best_overall_f1:.4f}\")\n",
    "\n",
    "print(f\"\"\"\n",
    "IMPROVEMENT SUMMARY:\n",
    "====================\n",
    "\n",
    "From Baseline to Tuned:\n",
    "\"\"\")\n",
    "\n",
    "if 'test_accuracy_rf' in locals() and 'test_accuracy_lr' in locals():\n",
    "    rf_improvement = (test_acc_rf_tuned - test_accuracy_rf) * 100\n",
    "    lr_improvement = (test_acc_lr_tuned - test_accuracy_lr) * 100\n",
    "\n",
    "    print(f\"RANDOM FOREST:\")\n",
    "    print(f\"  • Accuracy Improvement: {rf_improvement:+.2f}%\")\n",
    "    print(f\"  • F1-Score Improvement: {(f1_rf_tuned - f1_rf)*100:.2f}%\")\n",
    "\n",
    "    print(f\"\\nLOGISTIC REGRESSION:\")\n",
    "    print(f\"  • Accuracy Improvement: {lr_improvement:+.2f}%\")\n",
    "    print(f\"  • F1-Score Improvement: {(f1_lr_tuned - f1_lr)*100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fd8310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STAP 6: FINAL MODEL SELECTION & COMPREHENSIVE EVALUATION \n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STAP 6: FINAL MODEL SELECTION & COMPREHENSIVE EVALUATION \")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "DOEL:\n",
    "Selecteer het beste finale model voor deployment na preprocessing, feature selection en tuning.\n",
    "EVALUATIE:\n",
    "1) Performance: Accuracy, (weighted/macro) F1, per-class metrics\n",
    "2) Robustness: Cohen’s Kappa, MCC\n",
    "3) Praktisch: interpretability, inference speed, complexity, deployment/maintenance\n",
    "Vergelijking: Random Forest (tuned) vs Logistic Regression (tuned)\n",
    "\"\"\")\n",
    "\n",
    "# ============================================================================\n",
    "# 6A: CHECK OF ALLES BESCHIKBAAR IS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"6A: AVAILABILITY CHECK\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "required = ['rf_tuned', 'lr_tuned', 'X_test_unified', 'y_test']\n",
    "available = {k: (k in locals()) for k in required}\n",
    "\n",
    "for k, v in available.items():\n",
    "    print(f\"  • {k}: {'✓' if v else ''}\")\n",
    "\n",
    "if not all(available.values()):\n",
    "    print(\"\\n Run eerst Stap 5 / zorg dat tuned models + testdata beschikbaar zijn.\")\n",
    "else:\n",
    "    print(\"\\n✓ Alles beschikbaar, evaluatie start...\")\n",
    "\n",
    "# ============================================================================\n",
    "# 6B: CORE METRICS + ROBUSTNESS\n",
    "# ============================================================================\n",
    "if all(available.values()):\n",
    "    \n",
    "  \n",
    "\n",
    "    # Predictions\n",
    "    y_rf = rf_tuned.predict(X_test_unified)\n",
    "    y_lr = lr_tuned.predict(X_test_unified)\n",
    "\n",
    "    # Metrics table\n",
    "    metrics = {\n",
    "        \"RF (tuned)\": {\n",
    "            \"Accuracy\": accuracy_score(y_test, y_rf),\n",
    "            \"F1 (weighted)\": f1_score(y_test, y_rf, average=\"weighted\"),\n",
    "            \"F1 (macro)\": f1_score(y_test, y_rf, average=\"macro\"),\n",
    "            \"Kappa\": cohen_kappa_score(y_test, y_rf),\n",
    "            \"MCC\": matthews_corrcoef(y_test, y_rf),\n",
    "        },\n",
    "        \"LR (tuned)\": {\n",
    "            \"Accuracy\": accuracy_score(y_test, y_lr),\n",
    "            \"F1 (weighted)\": f1_score(y_test, y_lr, average=\"weighted\"),\n",
    "            \"F1 (macro)\": f1_score(y_test, y_lr, average=\"macro\"),\n",
    "            \"Kappa\": cohen_kappa_score(y_test, y_lr),\n",
    "            \"MCC\": matthews_corrcoef(y_test, y_lr),\n",
    "        },\n",
    "    }\n",
    "    metrics_df = pd.DataFrame(metrics).T.round(4)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"CORE METRICS\")\n",
    "    print(\"=\"*80)\n",
    "    display(metrics_df)\n",
    "\n",
    "    # ============================================================================\n",
    "    # 6C: PER-CLASS SUMMARY (F1)\n",
    "    # ============================================================================\n",
    "    class_names = ['Insomnia', 'None', 'Sleep Apnea']\n",
    "\n",
    "    rf_rep = classification_report(y_test, y_rf, target_names=class_names, output_dict=True)\n",
    "    lr_rep = classification_report(y_test, y_lr, target_names=class_names, output_dict=True)\n",
    "\n",
    "    per_class = pd.DataFrame({\n",
    "        \"RF F1\": [rf_rep[c][\"f1-score\"] for c in class_names],\n",
    "        \"LR F1\": [lr_rep[c][\"f1-score\"] for c in class_names],\n",
    "        \"Support\": [rf_rep[c][\"support\"] for c in class_names],\n",
    "    }, index=class_names).round(4)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PER-CLASS (F1)\")\n",
    "    print(\"=\"*80)\n",
    "    display(per_class)\n",
    "\n",
    "    # ============================================================================\n",
    "    # 6D: CONFUSION MATRIX (compact)\n",
    "    # ============================================================================\n",
    "    cm_rf = confusion_matrix(y_test, y_rf)\n",
    "    cm_lr = confusion_matrix(y_test, y_lr)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"CONFUSION MATRIX (RAW)\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"RF:\\n\", cm_rf)\n",
    "    print(\"\\nLR:\\n\", cm_lr)\n",
    "\n",
    "    # ============================================================================\n",
    "    # 6E: PRAKTISCH (SPEED + COMPLEXITY)\n",
    "    # ============================================================================\n",
    "    start = time.time()\n",
    "    for _ in range(10):\n",
    "        rf_tuned.predict(X_test_unified)\n",
    "    rf_ms = (time.time() - start) / 10 * 1000\n",
    "\n",
    "    start = time.time()\n",
    "    for _ in range(10):\n",
    "        lr_tuned.predict(X_test_unified)\n",
    "    lr_ms = (time.time() - start) / 10 * 1000\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PRAKTISCH\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"• Inference time (batch): RF={rf_ms:.2f} ms | LR={lr_ms:.2f} ms\")\n",
    "    print(f\"• Complexity: RF hoog (ensemble {rf_tuned.n_estimators} trees) | LR laag (lineair, {X_test_unified.shape[1]} coeffs)\")\n",
    "    print(\"• Interpretability: RF medium (feature importance) | LR hoog (coefs/odds)\")\n",
    "\n",
    "    # ============================================================================\n",
    "    # 6F: EINDKEUZE (simpele regel)\n",
    "    # ============================================================================\n",
    "    # Primary rule: kies model met hogere weighted F1, tenzij verschil klein en interpretability belangrijk\n",
    "    rf_f1 = metrics_df.loc[\"RF (tuned)\", \"F1 (weighted)\"]\n",
    "    lr_f1 = metrics_df.loc[\"LR (tuned)\", \"F1 (weighted)\"]\n",
    "    rf_acc = metrics_df.loc[\"RF (tuned)\", \"Accuracy\"]\n",
    "    lr_acc = metrics_df.loc[\"LR (tuned)\", \"Accuracy\"]\n",
    "\n",
    "    diff = abs(rf_f1 - lr_f1)\n",
    "\n",
    "    if rf_f1 > lr_f1:\n",
    "        recommended = \"Random Forest (tuned)\"\n",
    "        runner_up = \"Logistic Regression (tuned)\"\n",
    "    else:\n",
    "        recommended = \"Logistic Regression (tuned)\"\n",
    "        runner_up = \"Random Forest (tuned)\"\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EINDKEUZE\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\" Aanbevolen model: {recommended}\")\n",
    "    print(f\"• Accuracy: RF={rf_acc:.3f} | LR={lr_acc:.3f}\")\n",
    "    print(f\"• F1 (weighted): RF={rf_f1:.3f} | LR={lr_f1:.3f}\")\n",
    "    print(f\"• Verschil in F1: {diff:.4f}  -> {'klein' if diff < 0.02 else 'duidelijk'}\")\n",
    "    print(f\"Backup/runner-up: {runner_up}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616882c3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955dbb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown, Image\n",
    "\n",
    "report_md = r\"\"\"\n",
    "# Voorspellen van Slaapstoornissen met Lifestyle en Gezondheidsdata\n",
    "\n",
    "## 1. Onderzoeksvraag\n",
    "\n",
    "**Kunnen we op basis van lifestyle en gezondheidsfactoren voorspellen of iemand een slaapstoornis heeft of zal ontwikkelen?**\n",
    "\n",
    "**Antwoord:**  \n",
    "Ja. Uit ons onderzoek blijkt dat lifestyle en gezondheidsdata (zoals BMI, fysieke activiteit en bloeddruk) sterke voorspellers zijn. Het best getunede model behaalt een nauwkeurigheid van **74,6%**, wat betekent dat ongeveer driekwart van de gevallen correct kan worden geïdentificeerd op basis van dagelijkse leefstijlmetingen.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Modelprestaties (Vergelijking)\n",
    "\n",
    "| Metric | Joshua: Random Forest (Tuned) | Imad: Logistic Regression (Tuned) |\n",
    "|--------|-------------------------------|-----------------------------------|\n",
    "| Test Accuracy | 74,6% | 73,9% |\n",
    "| F1-Score (Weighted) | 0,746 | 0,739 |\n",
    "| Overfitting Gap | 25,9% | 0,0% (Perfecte stabiliteit) |\n",
    "| Trainingstijd | 0,07 s | 0,01 s |\n",
    "| Inference Snelheid | 23,37 ms | 0,18 ms |\n",
    "| Aantal Features | 23 (incl. Diastolic) | 22 (excl. Diastolic) |\n",
    "\n",
    "### Observaties\n",
    "\n",
    "- Random Forest is krachtiger maar complexer en vatbaarder voor overfitting.  \n",
    "- Logistic Regression is stabiel, interpreteerbaar en extreem snel.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Diepgaande Analyse & Inzichten\n",
    "\n",
    "### Joshua : Random Forest\n",
    "\n",
    "- **Specialisatie:** Herkent complexe patronen tussen meerdere factoren tegelijk.  \n",
    "- **Klinische winst:** Hoogste recall voor Sleep Apnea (70,4%)  minimaliseert het risico dat patiënten met een stoornis over het hoofd worden gezien.  \n",
    "- **Keerzijde:** Complex (“black box”) en meer overfitting.\n",
    "\n",
    "### Imad : Logistic Regression\n",
    "\n",
    "- **Specialisatie:** Stabiel en transparant; overfitting gap = 0,0%.  \n",
    "- **Efficiëntie:** >130× sneller in voorspellingen dan Random Forest.  \n",
    "- **Kenmerk:** Door *Diastolic* te verwijderen blijft de prestatie vrijwel gelijk (verschil slechts 0,2–0,7%).\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Classificatie Details (Confusion Matrix)\n",
    "\n",
    "| Categorie | Correct voorspeld (Joshua) | Correct voorspeld (Imad) |\n",
    "|-----------|----------------------------|---------------------------|\n",
    "| Geen stoornis (None) | 459 | 454 |\n",
    "| Slaapapneu | 150 | 147 |\n",
    "| Insomnia | 137 | 138 |\n",
    "\n",
    "### Observaties\n",
    "\n",
    "- Random Forest identificeert beter gezonde personen en apneu-patiënten.  \n",
    "- Logistic Regression scoort iets beter bij het herkennen van slapeloosheid.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Eindadvies \n",
    "\n",
    "### Primair Model: Random Forest (Joshua)\n",
    "\n",
    "- **Reden:** Hogere nauwkeurigheid (74,6%) en betere recall op Sleep Apnea.  \n",
    "- **Toepassing:** Diagnostische ondersteuning.\n",
    "\n",
    "### Secundair Model / Controle: Logistic Regression (Imad)\n",
    "\n",
    "- **Reden:** Interpreteerbare coëfficiënten voor patiëntuitleg.  \n",
    "- **Toepassing:** Baseline en controle instrument.\n",
    "\"\"\"\n",
    "\n",
    "# Render markdown\n",
    "display(Markdown(report_md))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
