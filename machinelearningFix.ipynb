{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e1989b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "# Basisstijl voor plots\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
    "\n",
    "plt.rcParams[\"axes.grid\"] = True\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "\n",
    "# STAP 1: DATA INLADEN EN EERSTE VERKENNING\n",
    "\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"STAP 1: DATA INLADEN EN EERSTE VERKENNING\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "\n",
    "\n",
    "# Laad de dataset\n",
    "\n",
    "df = pd.read_csv(\"ML-sleep_health_lifestyle_dataset.csv\")\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\nDataset geladen met {df.shape[0]} rijen en {df.shape[1]} kolommen\")\n",
    "\n",
    "print(\"\\nEerste 5 rijen:\")\n",
    "\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9769f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STAP 2: DATA INSPECTIE EN KWALITEITSCONTROLE\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STAP 2: DATA INSPECTIE EN KWALITEITSCONTROLE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nInformatie over datatypes en geheugengebruik:\")\n",
    "df.info()\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"CONTROLE OP ONTBREKENDE WAARDEN\")\n",
    "print(\"-\"*80)\n",
    "missing_values = df.isnull().sum()\n",
    "missing_pct = 100 * df.isnull().sum() / len(df)\n",
    "missing_table = pd.DataFrame({\n",
    "    'Aantal Missing': missing_values,\n",
    "    'Percentage': missing_pct\n",
    "})\n",
    "print(missing_table[missing_table['Aantal Missing'] > 0])\n",
    "\n",
    "if missing_table['Aantal Missing'].sum() == 0:\n",
    "    print(\"\\n‚úì Geen ontbrekende waarden gevonden in de dataset\")\n",
    "    print(\"Dit is gunstig voor modelontwikkeling omdat we geen imputatiestrategie√´n nodig hebben.\")\n",
    "else:\n",
    "    print(\"\\n‚ö† Er zijn ontbrekende waarden die behandeld moeten worden\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"BESCHRIJVENDE STATISTIEKEN - NUMERIEKE VARIABELEN\")\n",
    "print(\"-\"*80)\n",
    "display(df.describe())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3504e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STAP 3: TARGET VARIABELE ANALYSE\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STAP 3: TARGET VARIABELE ANALYSE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nDe target variabele 'Sleep Disorder' vormt de basis voor ons classificatieprobleem.\")\n",
    "print(\"We onderzoeken de verdeling van klassen om te bepalen of we te maken hebben met\")\n",
    "print(\"class imbalance en of dit speciale aandacht vereist tijdens modeltraining.\\n\")\n",
    "\n",
    "# Vervang NaN in Sleep Disorder met 'None'\n",
    "df['Sleep Disorder'] = df['Sleep Disorder'].fillna('None')\n",
    "\n",
    "print(\"Verdeling van Sleep Disorder:\")\n",
    "class_distribution = df['Sleep Disorder'].value_counts()\n",
    "print(class_distribution)\n",
    "print(f\"\\nPercentages:\")\n",
    "print(df['Sleep Disorder'].value_counts(normalize=True) * 100)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "df['Sleep Disorder'].value_counts().plot(kind='bar', color=['#2ecc71', '#e74c3c', '#3498db'])\n",
    "plt.title(\"Verdeling van Sleep Disorder Klassen\", fontsize=14, fontweight='bold')\n",
    "plt.xlabel(\"Klasse\")\n",
    "plt.ylabel(\"Aantal observaties\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"INTERPRETATIE VAN KLASSENBALANS\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Bereken imbalance ratio\n",
    "min_class = class_distribution.min()\n",
    "max_class = class_distribution.max()\n",
    "imbalance_ratio = max_class / min_class\n",
    "\n",
    "print(f\"\\nImbalance ratio (grootste/kleinste klasse): {imbalance_ratio:.2f}\")\n",
    "\n",
    "if imbalance_ratio < 1.5:\n",
    "    print(\" De klassen zijn redelijk gebalanceerd. Standaard modeltraining is geschikt.\")\n",
    "elif imbalance_ratio < 3:\n",
    "    print(\"Er is enige klassenonevenwichtigheid. Overweeg stratified sampling en\")\n",
    "    print(\"  class_weight='balanced' parameter bij sommige modellen.\")\n",
    "else:\n",
    "    print(\"Significante klassenonevenwichtigheid gedetecteerd!\")\n",
    "    print(\"  Overweeg: SMOTE, class weighting, of stratified k-fold cross-validation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff8c5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# STAP 4: MULTICLASS VS BINARY CLASSIFICATIE - ONDERBOUWING\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STAP 4: MODELTYPE KEUZE - MULTICLASS VS BINARY CLASSIFICATIE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "RATIONALE VOOR MULTICLASS CLASSIFICATIE:\n",
    "-----------------------------------------\n",
    "\n",
    "Onze target variabele heeft drie categorie√´n:\n",
    "1. None (geen slaapstoornis)\n",
    "2. Insomnia (slapeloosheid)\n",
    "3. Sleep Apnea (slaapapneu)\n",
    "\n",
    "WAAROM MULTICLASS IN PLAATS VAN BINARY?\n",
    "========================================\n",
    "\n",
    "1. KLINISCHE RELEVANTIE:\n",
    "   ‚Ä¢ Insomnia en Sleep Apnea hebben verschillende oorzaken, symptomen en behandelingen\n",
    "   ‚Ä¢ Een binair model (wel/geen stoornis) zou deze cruciale distinctie verliezen\n",
    "   ‚Ä¢ Voor medisch personeel is het essentieel om het TYPE stoornis te identificeren\n",
    "\n",
    "2. BEHANDELINGSIMPLICATIES:\n",
    "   ‚Ä¢ Insomnia ‚Üí vaak cognitieve gedragstherapie, slaaphygi√´ne, medicatie\n",
    "   ‚Ä¢ Sleep Apnea ‚Üí CPAP-apparaat, gewichtsreductie, operatieve ingrepen\n",
    "   ‚Ä¢ De aanpak verschilt fundamenteel\n",
    "\n",
    "3. DIAGNOSTISCHE WAARDE:\n",
    "   ‚Ä¢ Verschillende risicoprofielen: Sleep Apnea correleert met BMI en hartslag,\n",
    "     Insomnia vaak met stress en levensstijlfactoren\n",
    "   ‚Ä¢ Een multiclass model kan deze subtiele patronen onderscheiden\n",
    "\n",
    "4. MODELCOMPLEXITEIT VS INFORMATIEBEHOUD:\n",
    "   ‚Ä¢ Trade-off: multiclass is complexer, maar behoudt essenti√´le informatie\n",
    "   ‚Ä¢ In medische context weegt informatieverlies zwaarder dan modelcomplexiteit\n",
    "\n",
    "\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbce46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# STAP 5: FEATURE TYPE IDENTIFICATIE EN CATEGORISATIE\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STAP 5: FEATURE TYPE IDENTIFICATIE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Numerieke kolommen\n",
    "num_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "# Verwijder Person ID uit numerieke features (is geen predictive feature)\n",
    "num_cols = [col for col in num_cols if col != 'Person ID']\n",
    "\n",
    "# Categorische kolommen\n",
    "cat_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "# Verwijder target uit categorische features\n",
    "cat_feature_cols = [col for col in cat_cols if col != 'Sleep Disorder']\n",
    "\n",
    "print(f\"\\nNumerieke features ({len(num_cols)}):\")\n",
    "for col in num_cols:\n",
    "    print(f\"  ‚Ä¢ {col}\")\n",
    "\n",
    "print(f\"\\nCategorische features ({len(cat_feature_cols)}):\")\n",
    "for col in cat_feature_cols:\n",
    "    unique_values = df[col].nunique()\n",
    "    print(f\"  ‚Ä¢ {col} ({unique_values} unieke waarden)\")\n",
    "    print(f\"    Waarden: {df[col].unique()[:5].tolist()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615b9d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STAP 6: TRAIN-TEST SPLIT - DATA PARTITIONERING\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STAP 6: TRAIN-TEST SPLIT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "DOEL VAN TRAIN-TEST SPLIT:\n",
    "===========================\n",
    "\n",
    "We splitsen de data in twee sets:\n",
    "1. TRAINING SET: Model leert patronen uit deze data\n",
    "2. TEST SET: Model wordt ge√´valueerd op ongeziene data\n",
    "\n",
    "Dit voorkomt OVERFITTING en geeft realistische performantie schatting.\n",
    "\n",
    "KRITIEK: SPLIT VOOR ALLE TRANSFORMATIES!\n",
    "=========================================\n",
    "\n",
    "We splitsen op de ONBEWERKTE data (na feature identificatie).\n",
    "Alle bewerkingen (outlier behandeling, encoding, scaling) gebeuren DAARNA,\n",
    "APART op train en test set.\n",
    "\n",
    "WAAROM?\n",
    "- Voorkomt data leakage (test info lekt niet naar train)\n",
    "- Scaler fit op train, transform op test\n",
    "- Encoder fit op train, transform op test\n",
    "- Outlier grenzen bepaald op train, toegepast op test\n",
    "\n",
    "SPLIT RATIO OVERWEGINGEN:\n",
    "==========================\n",
    "\n",
    "OPTIES:\n",
    "- 90/10: Maximaal trainingsdata, maar kleine test set (minder betrouwbare metrics)\n",
    "- 80/20: Goede balans (ONZE KEUZE)\n",
    "- 70/30: Meer test data, maar minder trainingsdata\n",
    "- 60/40: Bij zeer kleine datasets\n",
    "\n",
    "ONZE KEUZE: 80/20 SPLIT\n",
    "========================\n",
    "\n",
    "Rationale:\n",
    "- Dataset heeft ~374 samples ‚Üí 80% = ~299 train, 20% = ~75 test\n",
    "- 80% training is genoeg voor model learning\n",
    "- 20% test is acceptabel voor betrouwbare evaluatie (hoewel aan de kleine kant)\n",
    "- Breed geaccepteerd in ML literatuur (Hastie et al., 2009)\n",
    "\n",
    "STRATIFIED SAMPLING:\n",
    "====================\n",
    "\n",
    "stratify=y zorgt dat de klassenverhouding gelijk blijft in train √©n test.\n",
    "\n",
    "Voorbeeld: Als originele data 50% None, 30% Insomnia, 20% Sleep Apnea heeft,\n",
    "dan heeft ZOWEL train ALS test deze verhoudingen.\n",
    "\n",
    "WAAROM STRATIFICATION CRUCIAAL IS:\n",
    "- Voorkomt dat √©√©n klasse oververtegenwoordigd is in test set\n",
    "- Zorgt voor representatieve evaluatie\n",
    "- Essentieel bij (lichte) class imbalance\n",
    "\n",
    "RANDOM STATE:\n",
    "=============\n",
    "random_state=42 zorgt voor reproduceerbaarheid.\n",
    "Elke run geeft identieke split ‚Üí belangrijk voor:\n",
    "- Vergelijking tussen modellen\n",
    "- Rapportage van resultaten\n",
    "- Samenwerking in team\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"UITVOEREN VAN SPLIT OP ONBEWERKTE DATA\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Scheiding van features en target VOOR encoding/scaling\n",
    "X = df.drop(['Sleep Disorder', 'Person ID'], axis=1)  # ‚úì originele data\n",
    "y = df['Sleep Disorder']  # nog als string\n",
    "\n",
    "print(f\"\\nOriginele dataset:\")\n",
    "print(f\"  ‚Ä¢ X shape: {X.shape}\")\n",
    "print(f\"  ‚Ä¢ y shape: {y.shape}\")\n",
    "\n",
    "# Split op onbewerkte data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,  # ‚úì onbewerkte features\n",
    "    y,  # ‚úì onbewerkte target (nog strings)\n",
    "    test_size=0.20,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTRAINING SET:\")\n",
    "print(f\"  ‚Ä¢ X_train shape: {X_train.shape}\")\n",
    "print(f\"  ‚Ä¢ y_train shape: {y_train.shape}\")\n",
    "print(f\"  ‚Ä¢ Percentage: {100 * len(X_train) / len(X):.1f}%\")\n",
    "\n",
    "print(f\"\\nTEST SET:\")\n",
    "print(f\"  ‚Ä¢ X_test shape: {X_test.shape}\")\n",
    "print(f\"  ‚Ä¢ y_test shape: {y_test.shape}\")\n",
    "print(f\"  ‚Ä¢ Percentage: {100 * len(X_test) / len(X):.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"VERIFICATIE VAN STRATIFICATIE\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(\"\\nKlassenverdeling in ORIGINELE data:\")\n",
    "print(y.value_counts(normalize=True).sort_index())\n",
    "\n",
    "print(\"\\nKlassenverdeling in TRAINING set:\")\n",
    "print(y_train.value_counts(normalize=True).sort_index())\n",
    "\n",
    "print(\"\\nKlassenverdeling in TEST set:\")\n",
    "print(y_test.value_counts(normalize=True).sort_index())\n",
    "\n",
    "# Visualisatie van stratificatie\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "y.value_counts(normalize=True).sort_index().plot(kind='bar', ax=axes[0], color='skyblue')\n",
    "axes[0].set_title('Original Data')\n",
    "axes[0].set_ylabel('Proportion')\n",
    "axes[0].set_xlabel('Class')\n",
    "axes[0].set_ylim(0, 0.6)\n",
    "\n",
    "y_train.value_counts(normalize=True).sort_index().plot(kind='bar', ax=axes[1], color='lightgreen')\n",
    "axes[1].set_title('Training Set (80%)')\n",
    "axes[1].set_ylabel('Proportion')\n",
    "axes[1].set_xlabel('Class')\n",
    "axes[1].set_ylim(0, 0.6)\n",
    "\n",
    "y_test.value_counts(normalize=True).sort_index().plot(kind='bar', ax=axes[2], color='lightcoral')\n",
    "axes[2].set_title('Test Set (20%)')\n",
    "axes[2].set_ylabel('Proportion')\n",
    "axes[2].set_xlabel('Class')\n",
    "axes[2].set_ylim(0, 0.6)\n",
    "\n",
    "plt.suptitle('Verificatie Stratified Split - Klassenverhouding blijft gelijk', \n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì Stratificatie succesvol: klassenverhouding identiek in train en test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8a445e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STAP 7: EXPLORATIEVE DATA ANALYSE (EDA)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STAP 7: EXPLORATIEVE DATA ANALYSE (EDA)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "BELANGRIJK: EDA wordt uitgevoerd op de TRAINING SET\n",
    "====================================================\n",
    "\n",
    "Na de train-test split in stap 6, voeren we alle analyses uit op de training data.\n",
    "De test set blijft 'ongezien' om data leakage te voorkomen.\n",
    "\n",
    "Dit betekent:\n",
    "- Distributies ‚Üí berekend op train\n",
    "- Correlaties ‚Üí berekend op train  \n",
    "- Outlier grenzen ‚Üí bepaald op train (en toegepast op test in stap 8)\n",
    "- Feature statistics ‚Üí berekend op train\n",
    "\"\"\")\n",
    "\n",
    "# Combineer X_train en y_train voor analyse\n",
    "df_train = X_train.copy()\n",
    "df_train['Sleep Disorder'] = y_train.values\n",
    "\n",
    "print(f\"\\nTraining set voor EDA: {df_train.shape[0]} samples √ó {df_train.shape[1]} kolommen\")\n",
    "\n",
    "# ============================================================================\n",
    "# FEATURE IMPORTANCE - WAAROM ZIJN DEZE FEATURES BELANGRIJK?\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURE IMPORTANCE - KLINISCHE RATIONALE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "WAAROM ZIJN DEZE FEATURES BELANGRIJK VOOR SLAAPSTOORNIS CLASSIFICATIE?\n",
    "========================================================================\n",
    "\n",
    "1. SLEEP DURATION (Slaapduur) \n",
    "   ‚Ä¢ Primaire indicator van slaapstoornissen\n",
    "   ‚Ä¢ Insomnia: <6 uur per nacht (chronisch slaaptekort)\n",
    "   ‚Ä¢ Sleep Apnea: vaak normale duur, maar gefragmenteerde slaap\n",
    "   ‚Ä¢ Verwachting: Duidelijk verschil tussen klassen\n",
    "\n",
    "2. QUALITY OF SLEEP (Slaapkwaliteit) \n",
    "   ‚Ä¢ Subjectieve maat (self-reported, schaal 1-10)\n",
    "   ‚Ä¢ Capteert ervaren slaapkwaliteit die objectieve metingen kunnen missen\n",
    "   ‚Ä¢ Lage score bij BEIDE insomnia en sleep apnea\n",
    "   ‚Ä¢ Verwachting: Sterkste predictor voor aanwezigheid slaapstoornis\n",
    "\n",
    "3. STRESS LEVEL (Stressniveau) - \n",
    "   ‚Ä¢ Chronische stress ‚Üí verhoogd cortisol ‚Üí verstoord circadiaans ritme\n",
    "   ‚Ä¢ Bidirectionele relatie: stress ‚Üí slechte slaap ‚Üí meer stress\n",
    "   ‚Ä¢ Vooral relevant voor insomnia (hyperarousal)\n",
    "   ‚Ä¢ Verwachting: Hoog bij insomnia, gemiddeld bij sleep apnea\n",
    "\n",
    "4. PHYSICAL ACTIVITY LEVEL (Fysieke Activiteit) - \n",
    "   ‚Ä¢ Regelmatige beweging verbetert slaapkwaliteit (adenosine opbouw)\n",
    "   ‚Ä¢ Te weinig activiteit ‚Üí verstoorde slaap-waak cyclus\n",
    "   ‚Ä¢ Te veel (overtraining) ‚Üí verhoogde cortisol\n",
    "   ‚Ä¢ Verwachting: Lagere waarden geassocieerd met slaapstoornissen\n",
    "\n",
    "5. HEART RATE (Hartslag) -\n",
    "   ‚Ä¢ Verhoogd bij sleep apnea: repetitieve zuurstoftekort episodes ‚Üí sympathische activatie\n",
    "   ‚Ä¢ Normaal tot laag bij insomnia (compensatiemechanisme)\n",
    "   ‚Ä¢ Discriminerende feature: kan sleep apnea onderscheiden van insomnia\n",
    "   ‚Ä¢ Verwachting: Significant hogere HR bij sleep apnea pati√´nten\n",
    "\n",
    "6. DAILY STEPS (Dagelijkse Stappen) -\n",
    "   ‚Ä¢ Proxy voor algemene fysieke activiteit en lifestyle\n",
    "   ‚Ä¢ Correleert met Physical Activity Level (mogelijke multicollineariteit)\n",
    "   ‚Ä¢ Sedentair gedrag geassocieerd met slaapproblemen\n",
    "   ‚Ä¢ Verwachting: Lagere stappen bij beide slaapstoornissen\n",
    "\n",
    "7. BMI (Body Mass Index)\n",
    "   ‚Ä¢ Obesitas (BMI >30) = grootste risicofactor voor obstructieve sleep apnea\n",
    "   ‚Ä¢ Mechanisme: verhoogd weefsel in luchtwegen ‚Üí obstructie\n",
    "   ‚Ä¢ Minder relevant voor insomnia (meer psychologische factoren)\n",
    "   ‚Ä¢ Verwachting: Sterk verhoogd specifiek in sleep apnea groep\n",
    "\n",
    "8. BLOOD PRESSURE (Bloeddruk) - \n",
    "   ‚Ä¢ Hypertensie sterk geassocieerd met sleep apnea\n",
    "   ‚Ä¢ Chronische zuurstoftekort ‚Üí chronische sympathische activatie\n",
    "   ‚Ä¢ Kan ook verhoogd zijn bij chronische stress (insomnia)\n",
    "   ‚Ä¢ Verwachting: Hogere waarden bij sleep apnea, ook mogelijk bij insomnia\n",
    "\n",
    "9. AGE (Leeftijd) -\n",
    "   ‚Ä¢ Prevalentie sleep apnea neemt toe met leeftijd (weefselverlies, verminderde spierspanning)\n",
    "   ‚Ä¢ Insomnia prevalentie meer variabel over levensfasen\n",
    "   ‚Ä¢ Hormonale veranderingen (menopauze) be√Ønvloeden slaap\n",
    "   ‚Ä¢ Verwachting: Sleep apnea groep gemiddeld ouder\n",
    "\n",
    "10. GENDER (Geslacht) -\n",
    "    ‚Ä¢ Mannen: 2-3x hogere kans op sleep apnea (anatomische verschillen)\n",
    "    ‚Ä¢ Vrouwen: hogere prevalentie insomnia (hormonale factoren, stress)\n",
    "    ‚Ä¢ Belangrijke stratificatie variabele\n",
    "    ‚Ä¢ Verwachting: Gender als moderator voor type slaapstoornis\n",
    "\n",
    "11. OCCUPATION (Beroep) - \n",
    "    ‚Ä¢ Shift werk ‚Üí verstoord circadiaans ritme ‚Üí insomnia\n",
    "    ‚Ä¢ Stress-intensive beroepen ‚Üí hogere insomnia prevalentie\n",
    "    ‚Ä¢ Sedentair werk ‚Üí risico op obesitas ‚Üí sleep apnea\n",
    "    ‚Ä¢ Verwachting: Bepaalde beroepen correleren met specifieke stoornissen\n",
    "\n",
    "12. BMI CATEGORY (BMI Categorie) - \n",
    "    ‚Ä¢ Ordinale versie van BMI (Normal/Overweight/Obese)\n",
    "    ‚Ä¢ Mogelijk redundant ‚Üí overweeg feature selection\n",
    "    ‚Ä¢ Overweight/Obese: directe link met sleep apnea\n",
    "    ‚Ä¢ Verwachting: Mogelijk te verwijderen vanwege multicollineariteit met BMI\n",
    "\n",
    "SAMENVATTING TOP PREDICTORS:\n",
    "- Quality of Sleep: algemene indicator voor ALLE slaapstoornissen\n",
    "- BMI: specifiek voor sleep apnea\n",
    "- Heart Rate: discrimineert tussen sleep apnea vs insomnia\n",
    "- Sleep Duration: algemene indicator voor ernst van problematiek\n",
    "- Stress Level: specifiek voor insomnia\n",
    "\"\"\")\n",
    "\n",
    "# ============================================================================\n",
    "# GEMIDDELDE FEATURE WAARDEN PER SLEEP DISORDER KLASSE\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PER-KLASSE ANALYSE - VERSCHILLEN TUSSEN GROEPEN\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nDeze analyse toont gemiddelde feature waarden per Sleep Disorder klasse.\")\n",
    "print(\"Grote verschillen tussen klassen duiden op discriminerende features.\\n\")\n",
    "\n",
    "for col in num_cols:\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(f\"FEATURE: {col}\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    # Bereken statistics per klasse\n",
    "    class_stats = df_train.groupby('Sleep Disorder')[col].agg([\n",
    "        ('Mean', 'mean'),\n",
    "        ('Std', 'std'),\n",
    "        ('Min', 'min'),\n",
    "        ('Max', 'max'),\n",
    "        ('Count', 'count')\n",
    "    ]).round(2)\n",
    "    \n",
    "    display(class_stats)\n",
    "    \n",
    "    # Interpretatie\n",
    "    means = df_train.groupby('Sleep Disorder')[col].mean()\n",
    "    max_class = means.idxmax()\n",
    "    min_class = means.idxmin()\n",
    "    difference = means.max() - means.min()\n",
    "    \n",
    "    print(f\"\\n INTERPRETATIE:\")\n",
    "    print(f\"  ‚Ä¢ Hoogste gemiddelde: '{max_class}' = {means[max_class]:.2f}\")\n",
    "    print(f\"  ‚Ä¢ Laagste gemiddelde: '{min_class}' = {means[min_class]:.2f}\")\n",
    "    print(f\"  ‚Ä¢ Verschil: {difference:.2f} ({(difference/means.mean())*100:.1f}% van gemiddelde)\")\n",
    "    \n",
    "    # Conclusie\n",
    "    if difference / means.mean() > 0.2:  # >20% verschil\n",
    "        print(f\"  ‚úì STERKE DISCRIMINERENDE FEATURE (>20% verschil)\")\n",
    "    elif difference / means.mean() > 0.1:  # 10-20% verschil\n",
    "        print(f\"  ‚Üí Matige discriminerende feature (10-20% verschil)\")\n",
    "    else:\n",
    "        print(f\"  ‚ö† Zwakke discriminerende feature (<10% verschil)\")\n",
    "\n",
    "# ============================================================================\n",
    "# DISTRIBUTIE VAN NUMERIEKE VARIABELEN\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DISTRIBUTIE VAN NUMERIEKE VARIABELEN (TRAINING SET)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Maak histogrammen van TRAIN data\n",
    "axes = df_train[num_cols].hist(figsize=(15, 12), bins=20, edgecolor='black')\n",
    "plt.suptitle(\"Histogrammen van Numerieke Variabelen (Training Set)\", \n",
    "             fontsize=16, fontweight='bold')\n",
    "\n",
    "try:\n",
    "    ax_list = axes.flatten()\n",
    "except Exception:\n",
    "    ax_list = [axes] if hasattr(axes, 'get_axes') else list(axes)\n",
    "    \n",
    "for ax in ax_list:\n",
    "    ax.set_xlabel('Waarde')\n",
    "    ax.set_ylabel('Frequentie')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\"\"\n",
    "INTERPRETATIE VAN DISTRIBUTIES:\n",
    "================================\n",
    "\n",
    "- Sleep Duration: \n",
    "  - Normalish verdeling rond 7-8 uur (verwacht patroon)\n",
    "  - Mogelijk bimodaal (twee pieken: normale slapers vs insomnia)\n",
    "  \n",
    "- Quality of Sleep: \n",
    "  - Concentratie rond hogere waarden (7-9)\n",
    "  - Linker staart: slaapstoornis pati√´nten\n",
    "  \n",
    "- Stress Level: \n",
    "  - Spreiding over gehele schaal (1-10)\n",
    "  - Mogelijk uniform of licht rechts-scheef\n",
    "  \n",
    "- Physical Activity Level: \n",
    "  - Variatie in activiteitsniveaus\n",
    "  - Check of correlatie met Daily Steps (multicollineariteit)\n",
    "  \n",
    "- Heart Rate: \n",
    "  - Concentratie rond 70-80 bpm (normaal rustritme)\n",
    "  - Rechter staart: mogelijk sleep apnea pati√´nten\n",
    "  \n",
    "- Daily Steps: \n",
    "  - Rechtse scheefheid (veel lage waarden, enkele zeer hoge)\n",
    "  - Extreme waarden (>15000) mogelijk tracking fouten of atleten\n",
    "  \n",
    "- BMI:\n",
    "  - Rechts-scheve verdeling (normale populatie + obese groep)\n",
    "  - Verwacht patroon voor algemene populatie\n",
    "  \n",
    "- Age:\n",
    "  - Spreiding over volwassen leeftijden\n",
    "  - Check of ouderen oververtegenwoordigd in sleep apnea groep\n",
    "\"\"\")\n",
    "\n",
    "# ============================================================================\n",
    "# CORRELATIE ANALYSE\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CORRELATIE ANALYSE (TRAINING SET)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nCorrelatiematrix geeft inzicht in lineaire relaties tussen variabelen.\")\n",
    "print(\"Hoge correlaties tussen features kunnen wijzen op multicollineariteit.\\n\")\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "correlation_matrix = df_train[num_cols].corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt='.2f', \n",
    "            square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title(\"Correlatie Heatmap - Numerieke Variabelen (Training Set)\", \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Features')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identificeer sterke correlaties (|r| > 0.7, exclusief diagonaal)\n",
    "print(\"\\nSterk gecorreleerde variabelen (|r| > 0.7):\")\n",
    "strong_correlations = []\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.7:\n",
    "            strong_correlations.append({\n",
    "                'Var1': correlation_matrix.columns[i],\n",
    "                'Var2': correlation_matrix.columns[j],\n",
    "                'Correlation': correlation_matrix.iloc[i, j]\n",
    "            })\n",
    "\n",
    "if strong_correlations:\n",
    "    print(\"\\n‚ö†Ô∏è MULTICOLLINEARITEIT GEDETECTEERD:\")\n",
    "    for corr in strong_correlations:\n",
    "        print(f\"  ‚Ä¢ {corr['Var1']} ‚Üî {corr['Var2']}: r = {corr['Correlation']:.3f}\")\n",
    "    print(\"\\nüí° AANBEVELING: Overweeg √©√©n van deze features te verwijderen in feature selection fase.\")\n",
    "else:\n",
    "    print(\"  ‚úì Geen sterke correlaties gevonden (geen multicollineariteit issues)\")\n",
    "\n",
    "# ============================================================================\n",
    "# PAIRPLOT - RELATIES TUSSEN BELANGRIJKE VARIABELEN\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PAIRPLOT - RELATIES TUSSEN BELANGRIJKE VARIABELEN (TRAINING SET)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Selecteer subset voor pairplot (anders te groot)\n",
    "cols_for_pairplot = [\n",
    "    'Sleep Duration',\n",
    "    'Quality of Sleep',\n",
    "    'Stress Level',\n",
    "    'Physical Activity Level',\n",
    "    'Heart Rate',\n",
    "    'Daily Steps'\n",
    "]\n",
    "cols_for_pairplot = [c for c in cols_for_pairplot if c in df_train.columns]\n",
    "\n",
    "print(f\"\\nPairplot van {len(cols_for_pairplot)} belangrijkste variabelen\")\n",
    "print(\"Dit helpt om non-lineaire relaties en clusters te identificeren.\")\n",
    "print(\"Kleuren tonen verschillende Sleep Disorder klassen.\\n\")\n",
    "\n",
    "sns.pairplot(df_train[cols_for_pairplot + ['Sleep Disorder']], \n",
    "             hue='Sleep Disorder', diag_kind='kde', palette='Set2')\n",
    "plt.suptitle(\"Pairplot - Relaties tussen Features (Training Set)\", \n",
    "             y=1.01, fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\"\"\n",
    "INTERPRETATIE PAIRPLOT:\n",
    "======================\n",
    "\n",
    "LET OP:\n",
    "- Clusters per kleur: duidelijke scheiding tussen klassen = goede feature\n",
    "- Overlap tussen kleuren: moeilijk te onderscheiden klassen\n",
    "- Diagonaal (KDE plots): distributie per klasse\n",
    "- Off-diagonal scatter plots: bivariate relaties\n",
    "\n",
    "VERWACHT:\n",
    "- BMI vs Heart Rate: mogelijk cluster voor sleep apnea (hoog BMI + hoge HR)\n",
    "- Quality of Sleep vs Sleep Duration: positieve correlatie\n",
    "- Stress Level vs Quality of Sleep: negatieve correlatie\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úì EDA VOLTOOID OP TRAINING SET\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nBelangrijkste bevindingen worden gebruikt voor:\")\n",
    "print(\"  ‚Ä¢ Outlier behandeling (stap 8)\")\n",
    "print(\"  ‚Ä¢ Feature selection (stap 11)\")\n",
    "print(\"  ‚Ä¢ Model keuze en interpretatie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc56110f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STAP 8: OUTLIER DETECTIE EN BEHANDELING - GEFUNDEERDE AANPAK \n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STAP 8: OUTLIER DETECTIE EN BEHANDELING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "METHODOLOGIE: IQR (INTERQUARTILE RANGE) METHODE\n",
    "================================================\n",
    "\n",
    "De IQR-methode definieert outliers als waarden die buiten de volgende grenzen vallen:\n",
    "- Lower bound = Q1 - 1.5 √ó IQR\n",
    "- Upper bound = Q3 + 1.5 √ó IQR\n",
    "\n",
    "KRITISCHE VRAAG: MOETEN WE OUTLIERS VERWIJDEREN?\n",
    "==================================================\n",
    "\n",
    "In dit project verwijderen we outliers niet automatisch. In plaats daarvan passen we een conservatieve\n",
    "capping/winsorization toe: zeer hoge of lage waarden worden begrensd tot een minimum of maximum.\n",
    "Op deze manier behouden we alle observaties (belangrijk in medische datasets), maar verminderen\n",
    "we de invloed van onrepresentatieve uitschieters op het model.\n",
    "\n",
    "BELANGRIJK: DATA LEAKAGE PREVENTIE\n",
    "===================================\n",
    "\n",
    "We berekenen outlier grenzen (Q1, Q3, IQR) ALLEEN op TRAINING data.\n",
    "Deze grenzen worden vervolgens toegepast op ZOWEL train ALS test data.\n",
    "\n",
    "Werkwijze:\n",
    "1. Bereken Q1, Q3, IQR op X_train\n",
    "2. Bepaal lower/upper bounds op basis van train statistics\n",
    "3. Pas capping toe op X_train met deze grenzen\n",
    "4. Pas DEZELFDE grenzen toe op X_test (geen herberekening!)\n",
    "\n",
    "Dit voorkomt data leakage: test set informatie be√Ønvloedt niet de train set.\n",
    "\"\"\")\n",
    "\n",
    "# ============================================================================\n",
    "# STAP 8A: OUTLIER DETECTIE OP TRAINING SET\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"OUTLIER IDENTIFICATIE PER VARIABELE (TRAINING SET)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Maak kopie√´n om originele data te behouden\n",
    "X_train_clean = X_train.copy()\n",
    "X_test_clean = X_test.copy()\n",
    "\n",
    "# Identificeer numerieke kolommen in X_train\n",
    "num_cols_train = X_train_clean.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "print(f\"\\nNumerieke features in training set: {len(num_cols_train)}\")\n",
    "print(f\"Features: {num_cols_train}\\n\")\n",
    "\n",
    "# Bereken outlier statistics op TRAIN data\n",
    "outlier_info = {}\n",
    "outlier_bounds = {}  # Opslaan voor toepassen op test set\n",
    "\n",
    "for col in num_cols_train:\n",
    "    # Bereken op TRAIN\n",
    "    q1 = X_train_clean[col].quantile(0.25)\n",
    "    q3 = X_train_clean[col].quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    lower = q1 - 1.5 * iqr\n",
    "    upper = q3 + 1.5 * iqr\n",
    "    \n",
    "    # Identificeer outliers in train\n",
    "    mask = (X_train_clean[col] < lower) | (X_train_clean[col] > upper)\n",
    "    outlier_count = int(mask.sum())\n",
    "    outlier_pct = 100 * outlier_count / len(X_train_clean)\n",
    "    \n",
    "    # Opslaan voor rapportage\n",
    "    outlier_info[col] = {\n",
    "        'count': outlier_count,\n",
    "        'pct': float(outlier_pct),\n",
    "        'lower': float(lower),\n",
    "        'upper': float(upper),\n",
    "        'Q1': float(q1),\n",
    "        'Q3': float(q3),\n",
    "        'IQR': float(iqr)\n",
    "    }\n",
    "    \n",
    "    # Opslaan bounds voor test set\n",
    "    outlier_bounds[col] = {'lower': lower, 'upper': upper}\n",
    "\n",
    "outlier_summary = pd.DataFrame.from_dict(outlier_info, orient='index')\n",
    "outlier_summary = outlier_summary.sort_values('pct', ascending=False)\n",
    "\n",
    "print(\"\\nOutlier Samenvatting Training Set (gesorteerd op percentage):\")\n",
    "display(outlier_summary)\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"VISUALISATIE: BOXPLOTS VOOR OUTLIER DETECTIE (TRAINING SET)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "n_cols_plot = len(num_cols_train)\n",
    "n_rows = (n_cols_plot + 2) // 3  # Bereken aantal rijen nodig\n",
    "n_cols = min(3, n_cols_plot)\n",
    "\n",
    "plt.figure(figsize=(16, 4 * n_rows))\n",
    "for i, col in enumerate(num_cols_train, 1):\n",
    "    plt.subplot(n_rows, n_cols, i)\n",
    "    sns.boxplot(x=X_train_clean[col], color='skyblue')\n",
    "    title = f\"{col}\\n({outlier_summary.loc[col, 'count']:.0f} outliers, {outlier_summary.loc[col, 'pct']:.1f}%)\"\n",
    "    plt.title(title, fontsize=10)\n",
    "    plt.xlabel('')\n",
    "plt.suptitle(\"Boxplots - Outlier Detectie (Training Set)\", fontsize=14, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# STAP 8B: OUTLIER BEHANDELING - CAPPING/WINSORIZATION\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"OUTLIER BEHANDELING STRATEGIE - GEFUNDEERDE BESLISSINGEN (CAPPING)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "PRINCIPE: CONSERVATIEVE AANPAK\n",
    "=============================\n",
    "\n",
    "- We verwijderen geen rijen puur op basis van IQR-outliers.\n",
    "- We gebruiken winsorization/clipping (capping) voor sterk scheve/dispersed variabelen\n",
    "  en voor duidelijk onrealistische waarden.\n",
    "- Dit behoudt klinisch relevante extreme cases, maar vermindert de invloed van meetfouten\n",
    "  en ongebruikelijke uitschieters op modeltraining.\n",
    "\n",
    "IMPLEMENTATIE:\n",
    "==============\n",
    "1. Bepaal capping strategie per feature op basis van domeinkennis\n",
    "2. Bereken grenzen op TRAIN set\n",
    "3. Pas toe op TRAIN set\n",
    "4. Pas DEZELFDE grenzen toe op TEST set\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"IMPLEMENTATIE VAN BEHANDELING (CAPPING / WINSORIZATION)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Strategie 1: Winsorization voor Daily Steps (1% - 99%)\n",
    "if 'Daily Steps' in X_train_clean.columns:\n",
    "    print(\"\\n1. DAILY STEPS - Winsorization / Capping (1% - 99%)\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # TRAIN: bereken percentiles\n",
    "    low_p_train = X_train_clean['Daily Steps'].quantile(0.01)\n",
    "    high_p_train = X_train_clean['Daily Steps'].quantile(0.99)\n",
    "    \n",
    "    # Toon originele ranges\n",
    "    original_min_train = X_train_clean['Daily Steps'].min()\n",
    "    original_max_train = X_train_clean['Daily Steps'].max()\n",
    "    original_min_test = X_test_clean['Daily Steps'].min()\n",
    "    original_max_test = X_test_clean['Daily Steps'].max()\n",
    "    \n",
    "    print(f\"   TRAIN originele range: {original_min_train:.0f} - {original_max_train:.0f} stappen\")\n",
    "    print(f\"   TEST originele range: {original_min_test:.0f} - {original_max_test:.0f} stappen\")\n",
    "    print(f\"   Capping grenzen (van train): {low_p_train:.0f} - {high_p_train:.0f} stappen\")\n",
    "    \n",
    "    # Pas capping toe op TRAIN\n",
    "    X_train_clean['Daily Steps'] = np.clip(X_train_clean['Daily Steps'], low_p_train, high_p_train)\n",
    "    \n",
    "    # Pas DEZELFDE grenzen toe op TEST\n",
    "    X_test_clean['Daily Steps'] = np.clip(X_test_clean['Daily Steps'], low_p_train, high_p_train)\n",
    "    \n",
    "    print(f\"   ‚úì Capping toegepast op train ({len(X_train_clean)} samples)\")\n",
    "    print(f\"   ‚úì DEZELFDE grenzen toegepast op test ({len(X_test_clean)} samples)\")\n",
    "    print(\"   Rationale: Extreme waarden kunnen tracking fouten zijn. Winsorization behoudt data maar beperkt invloed.\")\n",
    "\n",
    "# Strategie 2: Heart Rate - capping naar plausibele range (40 - 120 bpm)\n",
    "if 'Heart Rate' in X_train_clean.columns:\n",
    "    print(\"\\n2. HEART RATE - Capping naar plausibele range (40 - 120 bpm)\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Medische plausibiliteitsgrenzen (domeinkennis)\n",
    "    hr_lower, hr_upper = 40, 120\n",
    "    \n",
    "    # Toon originele ranges\n",
    "    original_hr_min_train = X_train_clean['Heart Rate'].min()\n",
    "    original_hr_max_train = X_train_clean['Heart Rate'].max()\n",
    "    original_hr_min_test = X_test_clean['Heart Rate'].min()\n",
    "    original_hr_max_test = X_test_clean['Heart Rate'].max()\n",
    "    \n",
    "    print(f\"   TRAIN originele hartslag: {original_hr_min_train:.0f} - {original_hr_max_train:.0f} bpm\")\n",
    "    print(f\"   TEST originele hartslag: {original_hr_min_test:.0f} - {original_hr_max_test:.0f} bpm\")\n",
    "    print(f\"   Capping grenzen (medisch plausibel): {hr_lower} - {hr_upper} bpm\")\n",
    "    \n",
    "    # Pas capping toe op TRAIN\n",
    "    X_train_clean['Heart Rate'] = X_train_clean['Heart Rate'].clip(lower=hr_lower, upper=hr_upper)\n",
    "    \n",
    "    # Pas DEZELFDE grenzen toe op TEST\n",
    "    X_test_clean['Heart Rate'] = X_test_clean['Heart Rate'].clip(lower=hr_lower, upper=hr_upper)\n",
    "    \n",
    "    print(f\"   ‚úì Capping toegepast op train ({len(X_train_clean)} samples)\")\n",
    "    print(f\"   ‚úì DEZELFDE grenzen toegepast op test ({len(X_test_clean)} samples)\")\n",
    "    print(\"   Rationale: Clipping behoudt observaties maar vermindert invloed van onrealistische uitschieters.\")\n",
    "\n",
    "# Strategie 3: Plausibiliteitscontroles (geen verwijdering, enkel rapportage)\n",
    "print(\"\\n3. ANDERE VARIABELEN - Plausibiliteitscheck (geen verwijdering)\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Sleep Duration (we behouden, want korte/lange slaap kan klinisch relevant zijn)\n",
    "if 'Sleep Duration' in X_train_clean.columns:\n",
    "    invalid_sleep_train = (X_train_clean['Sleep Duration'] < 0) | (X_train_clean['Sleep Duration'] > 24)\n",
    "    invalid_sleep_test = (X_test_clean['Sleep Duration'] < 0) | (X_test_clean['Sleep Duration'] > 24)\n",
    "    \n",
    "    if invalid_sleep_train.sum() > 0 or invalid_sleep_test.sum() > 0:\n",
    "        print(f\"   ‚ö† Sleep Duration: {invalid_sleep_train.sum()} train + {invalid_sleep_test.sum()} test waarden buiten 0-24 uur\")\n",
    "    else:\n",
    "        print(\"   ‚úì Sleep Duration: alle waarden binnen 0-24 uur (train + test)\")\n",
    "\n",
    "# BMI (optioneel)\n",
    "if 'BMI' in X_train_clean.columns:\n",
    "    invalid_bmi_train = (X_train_clean['BMI'] < 10) | (X_train_clean['BMI'] > 70)\n",
    "    invalid_bmi_test = (X_test_clean['BMI'] < 10) | (X_test_clean['BMI'] > 70)\n",
    "    \n",
    "    if invalid_bmi_train.sum() > 0 or invalid_bmi_test.sum() > 0:\n",
    "        print(f\"   ‚ö† BMI: {invalid_bmi_train.sum()} train + {invalid_bmi_test.sum()} test waarden buiten 10-70\")\n",
    "    else:\n",
    "        print(\"   ‚úì BMI waarden binnen plausibel bereik (train + test)\")\n",
    "\n",
    "# Age\n",
    "if 'Age' in X_train_clean.columns:\n",
    "    invalid_age_train = (X_train_clean['Age'] < 18) | (X_train_clean['Age'] > 100)\n",
    "    invalid_age_test = (X_test_clean['Age'] < 18) | (X_test_clean['Age'] > 100)\n",
    "    \n",
    "    if invalid_age_train.sum() > 0 or invalid_age_test.sum() > 0:\n",
    "        print(f\"   ‚ö† Age: {invalid_age_train.sum()} train + {invalid_age_test.sum()} test waarden buiten 18-100 jaar\")\n",
    "    else:\n",
    "        print(\"   ‚úì Age waarden binnen plausibel bereik (train + test)\")\n",
    "\n",
    "# ============================================================================\n",
    "# STAP 8C: RESULTAAT VAN OUTLIER BEHANDELING\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"RESULTAAT VAN OUTLIER BEHANDELING (CAPPING / WINSORIZATION)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# We hebben geen rijen verwijderd; we hebben waarden gecapped/winsorized\n",
    "rows_removed = 0\n",
    "pct_removed = 0.0\n",
    "\n",
    "print(f\"\\nTRAINING SET:\")\n",
    "print(f\"  ‚Ä¢ Originele grootte: {X_train.shape[0]} rijen √ó {X_train.shape[1]} kolommen\")\n",
    "print(f\"  ‚Ä¢ Na behandeling: {X_train_clean.shape[0]} rijen √ó {X_train_clean.shape[1]} kolommen\")\n",
    "print(f\"  ‚Ä¢ Verwijderd: {rows_removed} rijen ({pct_removed:.2f}%)\")\n",
    "\n",
    "print(f\"\\nTEST SET:\")\n",
    "print(f\"  ‚Ä¢ Originele grootte: {X_test.shape[0]} rijen √ó {X_test.shape[1]} kolommen\")\n",
    "print(f\"  ‚Ä¢ Na behandeling: {X_test_clean.shape[0]} rijen √ó {X_test_clean.shape[1]} kolommen\")\n",
    "print(f\"  ‚Ä¢ Verwijderd: {rows_removed} rijen ({pct_removed:.2f}%)\")\n",
    "\n",
    "print(\"\\n‚úì Geen rijen verwijderd - we hebben capping toegepast op extreme waarden\")\n",
    "print(\"‚úì Outlier grenzen bepaald op TRAIN en toegepast op BEIDE sets (geen data leakage)\")\n",
    "print(\"‚úì Extreme waarden beperkt via winsorization/clipping\")\n",
    "\n",
    "# Update de variabelen voor gebruik in volgende stappen\n",
    "X_train = X_train_clean\n",
    "X_test = X_test_clean\n",
    "\n",
    "print(\"\\n X_train en X_test zijn nu updated met outlier behandeling\")\n",
    "print(\"   Deze worden gebruikt in volgende stappen (encoding, scaling)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf95802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STAP 9: FEATURE ENCODING - CATEGORISCHE VARIABELEN TRANSFORMEREN\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STAP 9: FEATURE ENCODING - CATEGORISCHE VARIABELEN\")\n",
    "print(\"=\"*80)\n",
    "print(\"\"\"\n",
    "WAAROM ENCODING NODIG IS:\n",
    "=========================\n",
    "Machine learning modellen werken met numerieke waarden. Categorische variabelen\n",
    "zoals 'Gender' (Male/Female) of 'BMI Category' (Normal/Overweight/Obese) moeten\n",
    "worden omgezet naar een numerieke representatie.\n",
    "\n",
    "ENCODING METHODEN:\n",
    "==================\n",
    "1. LABEL ENCODING:\n",
    "   ‚Ä¢ Ordinal categorie√´n: 'Low' ‚Üí 0, 'Medium' ‚Üí 1, 'High' ‚Üí 2\n",
    "   ‚Ä¢ Nadeel: impliceert orde/rangorde (2 > 1 > 0)\n",
    "   ‚Ä¢ Gebruik alleen voor ordinale variabelen of target variabele\n",
    "\n",
    "2. ONE-HOT ENCODING:\n",
    "   ‚Ä¢ Nominale categorie√´n: 'Male' ‚Üí [1,0], 'Female' ‚Üí [0,1]\n",
    "   ‚Ä¢ Voordeel: geen kunstmatige ordinale relatie\n",
    "   ‚Ä¢ Nadeel: verhoogt dimensionaliteit (curse of dimensionality bij veel categorie√´n)\n",
    "   ‚Ä¢ Drop_first=True om multicollineariteit te voorkomen (dummy variable trap)\n",
    "\n",
    "3. NUMERIEKE SPLITS:\n",
    "   ‚Ä¢ Blood Pressure: \"120/80\" ‚Üí Systolic=120, Diastolic=80\n",
    "   ‚Ä¢ Voorkomt explosie van dummy variabelen\n",
    "   ‚Ä¢ Behoudt numerieke relatie\n",
    "\n",
    "ONZE AANPAK MET TRAIN-TEST SPLIT:\n",
    "==================================\n",
    "CRITICAL: Data Leakage Preventie\n",
    "---------------------------------\n",
    "- One-hot encoding: fit op TRAIN, transform op TEST\n",
    "- Label encoding (target): fit op TRAIN, transform op TEST\n",
    "- Encoder leert categorie√´n alleen van train data\n",
    "\n",
    "Waarom?\n",
    "- Test set kan nieuwe categorie√´n hebben die train niet heeft gezien\n",
    "- Encoder moet consistent zijn tussen train en test\n",
    "- Voorkomt dat test informatie lekt naar train\n",
    "\"\"\")\n",
    "\n",
    "# ============================================================================\n",
    "# STAP 9A: BLOOD PRESSURE SPLITS IN SYSTOLIC EN DIASTOLIC\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"BLOOD PRESSURE TRANSFORMATIE\")\n",
    "print(\"-\"*80)\n",
    "print(\"\"\"\n",
    "PROBLEEM: Blood Pressure is categorisch met ~100+ unieke waarden\n",
    "-----------------------------------------------------------------\n",
    "Elke waarde zoals \"120/80\", \"130/85\", etc. zou een aparte dummy kolom krijgen.\n",
    "Dit resulteert in 100+ features ‚Üí curse of dimensionality!\n",
    "\n",
    "OPLOSSING: Split in 2 numerieke features\n",
    "-----------------------------------------\n",
    "\"120/80\" ‚Üí Systolic = 120, Diastolic = 80\n",
    "\n",
    "Voordelen:\n",
    "‚úì Reduceert 100+ dummies naar 2 numerieke features\n",
    "‚úì Behoudt medische betekenis (systolische/diastolische druk)\n",
    "‚úì Modellen kunnen numerieke relaties leren\n",
    "\"\"\")\n",
    "\n",
    "def split_blood_pressure(df):\n",
    "    \"\"\"Split Blood Pressure kolom in Systolic en Diastolic\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Split \"120/80\" in twee delen\n",
    "    bp_split = df['Blood Pressure'].str.split('/', expand=True)\n",
    "    \n",
    "    # Converteer naar integers\n",
    "    df['Systolic'] = bp_split[0].astype(int)\n",
    "    df['Diastolic'] = bp_split[1].astype(int)\n",
    "    \n",
    "    # Verwijder originele Blood Pressure kolom\n",
    "    df = df.drop('Blood Pressure', axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(f\"\\nVOOR transformatie:\")\n",
    "print(f\"  ‚Ä¢ X_train shape: {X_train.shape}\")\n",
    "print(f\"  ‚Ä¢ X_test shape: {X_test.shape}\")\n",
    "print(f\"  ‚Ä¢ Blood Pressure unieke waarden in train: {X_train['Blood Pressure'].nunique()}\")\n",
    "print(f\"  ‚Ä¢ Blood Pressure unieke waarden in test: {X_test['Blood Pressure'].nunique()}\")\n",
    "\n",
    "print(\"\\nVoorbeeld Blood Pressure waarden:\")\n",
    "print(X_train['Blood Pressure'].head(10).tolist())\n",
    "\n",
    "# Voer transformatie uit\n",
    "X_train = split_blood_pressure(X_train)\n",
    "X_test = split_blood_pressure(X_test)\n",
    "\n",
    "print(f\"\\nNA transformatie:\")\n",
    "print(f\"  ‚Ä¢ X_train shape: {X_train.shape}\")\n",
    "print(f\"  ‚Ä¢ X_test shape: {X_test.shape}\")\n",
    "\n",
    "print(\"\\nNieuwe kolommen toegevoegd:\")\n",
    "print(f\"  ‚Ä¢ Systolic - Range train: [{X_train['Systolic'].min()}, {X_train['Systolic'].max()}]\")\n",
    "print(f\"  ‚Ä¢ Systolic - Range test: [{X_test['Systolic'].min()}, {X_test['Systolic'].max()}]\")\n",
    "print(f\"  ‚Ä¢ Diastolic - Range train: [{X_train['Diastolic'].min()}, {X_train['Diastolic'].max()}]\")\n",
    "print(f\"  ‚Ä¢ Diastolic - Range test: [{X_test['Diastolic'].min()}, {X_test['Diastolic'].max()}]\")\n",
    "\n",
    "print(\"\\nVoorbeeld eerste 5 rijen:\")\n",
    "print(X_train[['Systolic', 'Diastolic']].head())\n",
    "\n",
    "print(\"\\n‚úì Blood Pressure succesvol gesplitst in 2 numerieke features\")\n",
    "\n",
    "# ============================================================================\n",
    "# STAP 9B: IDENTIFICEER RESTERENDE CATEGORISCHE FEATURES\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"IDENTIFICATIE VAN CATEGORISCHE FEATURES\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Identificeer categorische kolommen in X_train\n",
    "cat_cols_train = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(f\"\\nCategorische features in training set ({len(cat_cols_train)}):\")\n",
    "for col in cat_cols_train:\n",
    "    unique_vals_train = X_train[col].nunique()\n",
    "    unique_vals_test = X_test[col].nunique()\n",
    "    print(f\"  ‚Ä¢ {col}:\")\n",
    "    print(f\"    - Train: {unique_vals_train} categorie√´n ‚Üí {X_train[col].unique().tolist()}\")\n",
    "    print(f\"    - Test: {unique_vals_test} categorie√´n ‚Üí {X_test[col].unique().tolist()}\")\n",
    "\n",
    "# Check voor nieuwe categorie√´n in test set (kunnen problemen geven)\n",
    "print(\"\\n‚ö†Ô∏è CONTROLE: Nieuwe categorie√´n in test set?\")\n",
    "for col in cat_cols_train:\n",
    "    train_cats = set(X_train[col].unique())\n",
    "    test_cats = set(X_test[col].unique())\n",
    "    new_cats = test_cats - train_cats\n",
    "    \n",
    "    if new_cats:\n",
    "        print(f\"  ‚ö†Ô∏è {col}: Test heeft nieuwe categorie√´n: {new_cats}\")\n",
    "        print(f\"     ‚Üí Deze worden behandeld als 'unknown' tijdens encoding\")\n",
    "    else:\n",
    "        print(f\"  ‚úì {col}: Geen nieuwe categorie√´n in test\")\n",
    "\n",
    "# ============================================================================\n",
    "# STAP 9C: ONE-HOT ENCODING VOOR FEATURES\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"ONE-HOT ENCODING VOOR FEATURES\")\n",
    "print(\"-\"*80)\n",
    "print(\"\"\"\n",
    "METHODE: pd.get_dummies() met align\n",
    "====================================\n",
    "We gebruiken pandas get_dummies() omdat het eenvoudig is, maar we moeten\n",
    "zorgen dat train en test dezelfde kolommen hebben na encoding.\n",
    "\n",
    "Proces:\n",
    "1. Encode train set ‚Üí krijg dummy kolommen\n",
    "2. Encode test set ‚Üí krijg dummy kolommen\n",
    "3. Align beide sets zodat ze identieke kolommen hebben\n",
    "4. Missende kolommen in test worden gevuld met 0\n",
    "\n",
    "Alternatief: sklearn OneHotEncoder met handle_unknown='ignore'\n",
    "\"\"\")\n",
    "\n",
    "print(f\"\\nVOOR encoding:\")\n",
    "print(f\"  ‚Ä¢ X_train: {X_train.shape[0]} rijen √ó {X_train.shape[1]} kolommen\")\n",
    "print(f\"  ‚Ä¢ X_test: {X_test.shape[0]} rijen √ó {X_test.shape[1]} kolommen\")\n",
    "\n",
    "# One-hot encoding op TRAIN\n",
    "X_train_encoded = pd.get_dummies(X_train, columns=cat_cols_train, drop_first=True)\n",
    "\n",
    "# One-hot encoding op TEST (met dezelfde kolommen)\n",
    "X_test_encoded = pd.get_dummies(X_test, columns=cat_cols_train, drop_first=True)\n",
    "\n",
    "print(f\"\\nNA encoding (voor align):\")\n",
    "print(f\"  ‚Ä¢ X_train_encoded: {X_train_encoded.shape[0]} rijen √ó {X_train_encoded.shape[1]} kolommen\")\n",
    "print(f\"  ‚Ä¢ X_test_encoded: {X_test_encoded.shape[0]} rijen √ó {X_test_encoded.shape[1]} kolommen\")\n",
    "\n",
    "# BELANGRIJK: Align zodat beide sets dezelfde kolommen hebben\n",
    "# Kolommen die in train maar niet in test zitten ‚Üí voeg toe aan test met 0\n",
    "# Kolommen die in test maar niet in train zitten ‚Üí verwijder uit test\n",
    "X_train_encoded, X_test_encoded = X_train_encoded.align(X_test_encoded, join='left', axis=1, fill_value=0)\n",
    "\n",
    "print(f\"\\nNA align (train en test hebben nu IDENTIEKE kolommen):\")\n",
    "print(f\"  ‚Ä¢ X_train_encoded: {X_train_encoded.shape[0]} rijen √ó {X_train_encoded.shape[1]} kolommen\")\n",
    "print(f\"  ‚Ä¢ X_test_encoded: {X_test_encoded.shape[0]} rijen √ó {X_test_encoded.shape[1]} kolommen\")\n",
    "\n",
    "# Toon nieuwe kolommen\n",
    "new_cols = [col for col in X_train_encoded.columns if col not in X_train.columns]\n",
    "print(f\"\\nNieuwe dummy kolommen gecre√´erd ({len(new_cols)}):\")\n",
    "for col in new_cols[:15]:  # Toon eerste 15\n",
    "    print(f\"  ‚Ä¢ {col}\")\n",
    "if len(new_cols) > 15:\n",
    "    print(f\"  ... en {len(new_cols) - 15} meer\")\n",
    "\n",
    "print(\"\\n‚úì One-hot encoding voltooid\")\n",
    "print(\"‚úì Train en test hebben identieke kolommen\")\n",
    "\n",
    "# ============================================================================\n",
    "# STAP 9D: LABEL ENCODING VOOR TARGET VARIABELE\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"LABEL ENCODING VOOR TARGET VARIABELE\")\n",
    "print(\"-\"*80)\n",
    "print(\"\"\"\n",
    "Target encoding met sklearn LabelEncoder\n",
    "=========================================\n",
    "Process:\n",
    "1. Fit encoder op y_train (leert de klassen)\n",
    "2. Transform y_train ‚Üí numerieke labels\n",
    "3. Transform y_test met DEZELFDE encoder ‚Üí numerieke labels\n",
    "\n",
    "Dit garandeert consistente encoding tussen train en test.\n",
    "\"\"\")\n",
    "\n",
    "print(f\"\\nVOOR encoding:\")\n",
    "print(f\"  ‚Ä¢ y_train: {y_train.shape[0]} samples, type: {y_train.dtype}\")\n",
    "print(f\"  ‚Ä¢ y_test: {y_test.shape[0]} samples, type: {y_test.dtype}\")\n",
    "print(f\"\\nUnieke klassen in y_train: {sorted(y_train.unique())}\")\n",
    "print(f\"Unieke klassen in y_test: {sorted(y_test.unique())}\")\n",
    "\n",
    "# Label encoding voor target\n",
    "le = LabelEncoder()\n",
    "\n",
    "# FIT op y_train (encoder leert de klassen van train data)\n",
    "le.fit(y_train)\n",
    "\n",
    "# TRANSFORM beide sets met gefitte encoder\n",
    "y_train_encoded = le.transform(y_train)\n",
    "y_test_encoded = le.transform(y_test)\n",
    "\n",
    "# Converteer naar pandas Series voor consistentie\n",
    "y_train_encoded = pd.Series(y_train_encoded, index=y_train.index, name='Sleep Disorder')\n",
    "y_test_encoded = pd.Series(y_test_encoded, index=y_test.index, name='Sleep Disorder')\n",
    "\n",
    "print(f\"\\nNA encoding:\")\n",
    "print(f\"  ‚Ä¢ y_train_encoded: {y_train_encoded.shape[0]} samples, type: {y_train_encoded.dtype}\")\n",
    "print(f\"  ‚Ä¢ y_test_encoded: {y_test_encoded.shape[0]} samples, type: {y_test_encoded.dtype}\")\n",
    "\n",
    "# Toon mapping\n",
    "label_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "print(\"\\nLabel Encoding Mapping:\")\n",
    "for original, encoded in sorted(label_mapping.items(), key=lambda x: x[1]):\n",
    "    count_train = (y_train_encoded == encoded).sum()\n",
    "    count_test = (y_test_encoded == encoded).sum()\n",
    "    print(f\"  '{original}' ‚Üí {encoded}\")\n",
    "    print(f\"    - Train: {count_train} samples ({100*count_train/len(y_train_encoded):.1f}%)\")\n",
    "    print(f\"    - Test: {count_test} samples ({100*count_test/len(y_test_encoded):.1f}%)\")\n",
    "\n",
    "print(\"\\nRationale: Label encoding voor target is noodzakelijk voor sklearn classificatie.\")\n",
    "print(\"De numerieke waarden hebben geen ordinale betekenis in multiclass setting.\")\n",
    "\n",
    "# ============================================================================\n",
    "# STAP 9E: VERIFICATIE EN SAMENVATTING\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"VERIFICATIE EN SAMENVATTING\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(\"\\nEerste 5 rijen TRAINING set (features + target):\")\n",
    "display(pd.concat([X_train_encoded.head(), y_train_encoded.head()], axis=1))\n",
    "\n",
    "print(\"\\nEerste 5 rijen TEST set (features + target):\")\n",
    "display(pd.concat([X_test_encoded.head(), y_test_encoded.head()], axis=1))\n",
    "\n",
    "print(\"\\nKolomtypes in X_train_encoded:\")\n",
    "print(X_train_encoded.dtypes.value_counts())\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úì FEATURE ENCODING VOLTOOID\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nSamenvatting:\")\n",
    "print(f\"  ‚Ä¢ Blood Pressure: Gesplitst in Systolic + Diastolic (2 numerieke features)\")\n",
    "print(f\"  ‚Ä¢ Categorische features: {len(cat_cols_train)} ‚Üí One-hot encoded\")\n",
    "print(f\"  ‚Ä¢ Train features: {X_train.shape[1]} ‚Üí {X_train_encoded.shape[1]} kolommen\")\n",
    "print(f\"  ‚Ä¢ Test features: {X_test.shape[1]} ‚Üí {X_test_encoded.shape[1]} kolommen\")\n",
    "print(f\"  ‚Ä¢ Target variabele: Label encoded (3 klassen)\")\n",
    "print(f\"  ‚Ä¢ Train samples: {X_train_encoded.shape[0]}\")\n",
    "print(f\"  ‚Ä¢ Test samples: {X_test_encoded.shape[0]}\")\n",
    "\n",
    "print(\"\\n BELANGRIJK: Encoder fit op TRAIN, transform op TEST\")\n",
    "print(\"   Dit voorkomt data leakage!\")\n",
    "\n",
    "print(\"\\nReductie in features:\")\n",
    "print(f\"  ‚Ä¢ Blood Pressure zou ~{X_train['Blood Pressure'].nunique() if 'Blood Pressure' in X_train.columns else 100} dummies geven\")\n",
    "print(f\"  ‚Ä¢ Door splitsing: slechts 2 numerieke features (Systolic, Diastolic)\")\n",
    "print(f\"  ‚Ä¢ Besparing: ~{100-2} features!\")\n",
    "\n",
    "# Update variabelen voor volgende stappen\n",
    "X_train = X_train_encoded\n",
    "X_test = X_test_encoded\n",
    "y_train = y_train_encoded\n",
    "y_test = y_test_encoded\n",
    "\n",
    "print(\"\\n‚úì X_train, X_test, y_train, y_test zijn nu ge-encoded\")\n",
    "print(\"   Deze worden gebruikt in volgende stappen (scaling, modeling)\")\n",
    "\n",
    "# Check correlatie tussen Systolic en Diastolic\n",
    "if 'Systolic' in X_train.columns and 'Diastolic' in X_train.columns:\n",
    "    corr = X_train[['Systolic', 'Diastolic']].corr().iloc[0, 1]\n",
    "    print(f\"\\nüìä CORRELATIE CHECK:\")\n",
    "    print(f\"   Systolic ‚Üî Diastolic: r = {corr:.3f}\")\n",
    "    if abs(corr) > 0.9:\n",
    "        print(f\"   ‚ö†Ô∏è Hoge correlatie gedetecteerd (|r| > 0.9)\")\n",
    "        print(f\"   ‚Üí Overweeg √©√©n van beide te verwijderen om multicollineariteit te reduceren\")\n",
    "    else:\n",
    "        print(f\"   ‚úì Correlatie acceptabel\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"READY VOOR STAP 10: FEATURE SCALING\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c37583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STAP 10: FEATURE SCALING - NORMALISATIE EN STANDAARDISATIE\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STAP 10: FEATURE SCALING (STANDARDISATIE)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "WAAROM FEATURE SCALING NODIG IS:\n",
    "=================================\n",
    "\n",
    "Variabelen hebben verschillende schalen:\n",
    "- Daily Steps: 0 - 10,000+ \n",
    "- Quality of Sleep: 1 - 10\n",
    "- Heart Rate: 50 - 100 bpm\n",
    "\n",
    "PROBLEEM ZONDER SCALING:\n",
    "========================\n",
    "- Distance-based modellen (KNN) worden gedomineerd door features met grote schaal\n",
    "- Gradient descent convergeert langzamer\n",
    "- Regularisatie (L1/L2) werkt niet eerlijk tussen features\n",
    "\n",
    "METHODEN:\n",
    "=========\n",
    "\n",
    "1. MIN-MAX NORMALISATIE (0-1 range):\n",
    "   x_scaled = (x - x_min) / (x_max - x_min)\n",
    "   ‚Ä¢ Voordeel: Behoudt distributie shape\n",
    "   ‚Ä¢ Nadeel: Gevoelig voor outliers\n",
    "\n",
    "2. STANDARDISATIE (Z-SCORE):\n",
    "   x_scaled = (x - Œº) / œÉ\n",
    "   ‚Ä¢ Voordeel: Robuust tegen outliers, mean=0 en std=1\n",
    "   ‚Ä¢ Nadeel: Geen vaste min/max\n",
    "   ‚Ä¢ BEST PRACTICE voor de meeste ML algoritmen\n",
    "\n",
    "ONZE KEUZE: StandardScaler (Z-score normalisatie)\n",
    "===================================================\n",
    "- Geschikt voor variabelen met outliers (hebben we behandeld in stap 8)\n",
    "- Werkt goed met tree-based modellen √©n lineaire modellen\n",
    "- Industrie standaard in sklearn pipelines\n",
    "\n",
    "CRITICAL: DATA LEAKAGE PREVENTIE\n",
    "=================================\n",
    "- Scaler fit op TRAIN data (leert mean en std van train)\n",
    "- Scaler transform op TRAIN data (past train statistics toe)\n",
    "- Scaler transform op TEST data (past TRAIN statistics toe op test!)\n",
    "- Test set blijft \"ongezien\" - we leren NIETS van test data\n",
    "\n",
    "Waarom?\n",
    "- Als we mean/std berekenen op hele dataset, dan lekt test info naar train\n",
    "- Model performance zou te optimistisch zijn\n",
    "- In productie hebben we ook geen toegang tot test statistics\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"HUIDIGE DATA STATUS\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(f\"\\nTRAINING SET:\")\n",
    "print(f\"  ‚Ä¢ X_train shape: {X_train.shape[0]} samples √ó {X_train.shape[1]} features\")\n",
    "print(f\"  ‚Ä¢ y_train shape: {y_train.shape[0]} samples\")\n",
    "\n",
    "print(f\"\\nTEST SET:\")\n",
    "print(f\"  ‚Ä¢ X_test shape: {X_test.shape[0]} samples √ó {X_test.shape[1]} features\")\n",
    "print(f\"  ‚Ä¢ y_test shape: {y_test.shape[0]} samples\")\n",
    "\n",
    "print(f\"\\nFeature kolommen ({len(X_train.columns)}):\")\n",
    "# Toon eerste 15 kolommen\n",
    "cols_to_show = X_train.columns.tolist()[:15]\n",
    "for col in cols_to_show:\n",
    "    print(f\"  ‚Ä¢ {col}\")\n",
    "if len(X_train.columns) > 15:\n",
    "    print(f\"  ... en {len(X_train.columns) - 15} meer kolommen\")\n",
    "\n",
    "# ============================================================================\n",
    "# STAP 10A: SCALING OP TRAINING SET\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"STAP 10A: STANDARDISATIE OP TRAINING SET\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Toon voorbeeld van niet-geschaalde data (TRAIN)\n",
    "print(\"\\nVoorbeeld TRAIN features VOOR scaling:\")\n",
    "display(X_train.head())\n",
    "\n",
    "print(\"\\nDescriptive statistics TRAIN VOOR scaling:\")\n",
    "display(X_train.describe())\n",
    "\n",
    "# Initialiseer scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# FIT scaler op TRAIN data (leert mean en std van train)\n",
    "scaler.fit(X_train)\n",
    "\n",
    "print(\"\\n Scaler Statistics (geleerd van TRAIN data):\")\n",
    "print(f\"  ‚Ä¢ Mean per feature (eerste 5): {scaler.mean_[:5]}\")\n",
    "print(f\"  ‚Ä¢ Std per feature (eerste 5): {scaler.scale_[:5]}\")\n",
    "\n",
    "# TRANSFORM train data met gefitte scaler\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "\n",
    "# Converteer terug naar DataFrame voor visualisatie\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "\n",
    "print(\"\\nVoorbeeld TRAIN features NA scaling:\")\n",
    "display(X_train_scaled.head())\n",
    "\n",
    "print(\"\\nDescriptive statistics TRAIN NA scaling:\")\n",
    "display(X_train_scaled.describe())\n",
    "\n",
    "print(\"\"\"\n",
    "INTERPRETATIE TRAIN:\n",
    "====================\n",
    "- Mean ‚âà 0 (kleine floating point errors acceptabel)\n",
    "- Std ‚âà 1 voor alle features\n",
    "- Dit is verwacht omdat we scaler gefitted hebben op train data\n",
    "\"\"\")\n",
    "\n",
    "# ============================================================================\n",
    "# STAP 10B: SCALING OP TEST SET (MET TRAIN STATISTICS)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"STAP 10B: STANDARDISATIE OP TEST SET (MET TRAIN STATISTICS)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "BELANGRIJK: We gebruiken GEEN fit() op test data!\n",
    "==================================================\n",
    "\n",
    "We gebruiken de TRAIN mean en std om test data te schalen.\n",
    "Dit betekent dat test data NIET per se mean=0 en std=1 zal hebben.\n",
    "\n",
    "Dit is CORRECT gedrag:\n",
    "- In productie hebben we ook nieuwe data die we schalen met train statistics\n",
    "- Test set moet \"ongezien\" blijven\n",
    "- Kleine afwijkingen van mean=0/std=1 in test zijn normaal en verwacht\n",
    "\"\"\")\n",
    "\n",
    "# Toon voorbeeld van niet-geschaalde data (TEST)\n",
    "print(\"\\nVoorbeeld TEST features VOOR scaling:\")\n",
    "display(X_test.head())\n",
    "\n",
    "print(\"\\nDescriptive statistics TEST VOOR scaling:\")\n",
    "display(X_test.describe())\n",
    "\n",
    "# TRANSFORM test data met TRAIN scaler (GEEN fit!)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Converteer terug naar DataFrame voor visualisatie\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "print(\"\\nVoorbeeld TEST features NA scaling:\")\n",
    "display(X_test_scaled.head())\n",
    "\n",
    "print(\"\\nDescriptive statistics TEST NA scaling:\")\n",
    "display(X_test_scaled.describe())\n",
    "\n",
    "print(\"\"\"\n",
    "INTERPRETATIE TEST:\n",
    "===================\n",
    "- Mean kan AFWIJKEN van 0 (bijvoorbeeld -0.15 of +0.20)\n",
    "- Std kan AFWIJKEN van 1 (bijvoorbeeld 0.95 of 1.08)\n",
    "- Dit is NORMAAL en CORRECT - test data is geschaald met TRAIN statistics\n",
    "- Kleine afwijkingen zijn verwacht als train en test distributie iets verschillen\n",
    "\"\"\")\n",
    "\n",
    "# ============================================================================\n",
    "# STAP 10C: VERIFICATIE EN SAMENVATTING\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"VERIFICATIE VAN SCALING\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Controleer of alle features geschaald zijn\n",
    "print(\"\\nControle: Zijn alle features geschaald?\")\n",
    "print(\"\\nTRAIN set - Mean en Std per feature (eerste 10):\")\n",
    "train_stats = pd.DataFrame({\n",
    "    'Mean': X_train_scaled.mean(),\n",
    "    'Std': X_train_scaled.std()\n",
    "}).head(10)\n",
    "display(train_stats)\n",
    "\n",
    "print(\"\\nTEST set - Mean en Std per feature (eerste 10):\")\n",
    "test_stats = pd.DataFrame({\n",
    "    'Mean': X_test_scaled.mean(),\n",
    "    'Std': X_test_scaled.std()\n",
    "}).head(10)\n",
    "display(test_stats)\n",
    "\n",
    "print(\"\\n‚úì TRAIN: Mean ‚âà 0, Std ‚âà 1 (exact, want scaler gefitted op train)\")\n",
    "print(\"‚ö†Ô∏è TEST: Mean en Std kunnen afwijken (normaal, want geschaald met train statistics)\")\n",
    "\n",
    "# ============================================================================\n",
    "# STAP 10D: VISUALISATIE VAN SCALING EFFECT\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"VISUALISATIE: VOOR vs NA SCALING\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Selecteer een paar features om te visualiseren\n",
    "features_to_plot = [col for col in ['Sleep Duration', 'Heart Rate', 'Daily Steps', 'BMI', 'Age'] \n",
    "                    if col in X_train.columns][:4]\n",
    "\n",
    "if len(features_to_plot) > 0:\n",
    "    fig, axes = plt.subplots(2, len(features_to_plot), figsize=(16, 8))\n",
    "    \n",
    "    for i, feature in enumerate(features_to_plot):\n",
    "        # Voor scaling (train)\n",
    "        axes[0, i].hist(X_train[feature], bins=20, edgecolor='black', alpha=0.7)\n",
    "        axes[0, i].set_title(f'{feature}\\nVOOR scaling (Train)')\n",
    "        axes[0, i].set_xlabel('Waarde')\n",
    "        axes[0, i].set_ylabel('Frequentie')\n",
    "        \n",
    "        # Na scaling (train)\n",
    "        axes[1, i].hist(X_train_scaled[feature], bins=20, edgecolor='black', alpha=0.7, color='green')\n",
    "        axes[1, i].set_title(f'{feature}\\nNA scaling (Train)')\n",
    "        axes[1, i].set_xlabel('Z-score')\n",
    "        axes[1, i].set_ylabel('Frequentie')\n",
    "        axes[1, i].axvline(0, color='red', linestyle='--', label='Mean=0')\n",
    "    \n",
    "    plt.suptitle('Effect van Standardisatie op Features (Training Set)', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Geen numerieke features gevonden om te plotten\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úì FEATURE SCALING VOLTOOID\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nSamenvatting:\")\n",
    "print(f\"  ‚Ä¢ Scaling methode: StandardScaler (Z-score normalisatie)\")\n",
    "print(f\"  ‚Ä¢ Scaler fitted op: TRAIN data ({X_train.shape[0]} samples)\")\n",
    "print(f\"  ‚Ä¢ Train features geschaald: {X_train_scaled.shape[1]} features\")\n",
    "print(f\"  ‚Ä¢ Test features geschaald: {X_test_scaled.shape[1]} features\")\n",
    "print(f\"  ‚Ä¢ Train mean ‚âà 0, std ‚âà 1: ‚úì\")\n",
    "print(f\"  ‚Ä¢ Test geschaald met TRAIN statistics: ‚úì\")\n",
    "print(f\"  ‚Ä¢ Data leakage voorkomen: ‚úì\")\n",
    "\n",
    "\n",
    "\n",
    "# Update variabelen voor volgende stappen\n",
    "X_train = X_train_scaled\n",
    "X_test = X_test_scaled\n",
    "\n",
    "print(\"\\n‚úì X_train en X_test zijn nu geschaald en klaar voor modeling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b697ab82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STAP 11: FEATURE SELECTION & ENGINEERING - OVERWEGINGEN\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STAP 11: FEATURE SELECTION & ENGINEERING - OVERWEGINGEN\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "HUIDIGE FEATURES:\n",
    "=================\n",
    "\n",
    "We hebben alle beschikbare features behouden na encoding:\n",
    "- Numerieke features: Sleep Duration, Quality of Sleep, Stress Level, etc.\n",
    "- One-hot encoded categorische features: Gender, BMI Category, Occupation\n",
    "\n",
    "\n",
    "WAAROM BEHOUDEN WE ALLE FEATURES?\n",
    "==================================\n",
    "\n",
    "Voor de BASELINE modellen gebruiken we bewust ALLE features, ook al hebben we\n",
    "in stap 7 (EDA) multicollineariteit gedetecteerd:\n",
    "\n",
    "- Systolic ‚Üî Diastolic: r = 0.979 (extreem hoog)\n",
    "- Physical Activity ‚Üî Daily Steps: r = 0.761 (hoog)\n",
    "- Sleep Duration ‚Üî Quality of Sleep: r = 0.847 (hoog)\n",
    "\n",
    "Rationale voor behouden:\n",
    "1. Data-driven benadering: Laat MODEL bepalen welke features belangrijk zijn\n",
    "2. Wetenschappelijke methode: Train eerst, analyseer dan\n",
    "3. Vergelijkingsbasis: Baseline (alle features) vs Optimized (geselecteerde features)\n",
    "4. Objectiviteit: Geen vooringenomen keuzes zonder bewijs\n",
    "\n",
    "FEATURE SELECTION STRATEGIE - IN ML FASE:\n",
    "==========================================\n",
    "\n",
    "In de \"Uitwerking ML Vraagstuk\" fase zullen we:\n",
    "\n",
    "STAP 1: BASELINE MODELS\n",
    "  ‚Üí Train Random Forest, XGBoost, Logistic Regression met ALLE 23 features\n",
    "  ‚Üí Evalueer performance en feature importance\n",
    "\n",
    "STAP 2: FEATURE IMPORTANCE ANALYSE\n",
    "  ‚Üí Analyseer feature importance uit tree-based modellen\n",
    "  ‚Üí Identificeer features met <1% importance\n",
    "  ‚Üí Analyseer correlaties in context van model performance\n",
    "  \n",
    "STAP 3: MULTICOLLINEARITEIT BEHANDELING\n",
    "  ‚ö†Ô∏è DIASTOLIC VERWIJDERING:\n",
    "     ‚Ä¢ Systolic vs Diastolic (r=0.979)\n",
    "     ‚Ä¢ Keuze: Behoud Systolic (klinisch belangrijker voor cardiovasculaire risico)\n",
    "     ‚Ä¢ Rationale: 95.8% overlap in informatie, geen toegevoegde voorspellende waarde\n",
    "  \n",
    "  üí° OVERIGE CORRELATIES:\n",
    "     ‚Ä¢ Physical Activity vs Daily Steps (r=0.761)\n",
    "       ‚Üí Beide behouden in baseline, evalueer feature importance\n",
    "     ‚Ä¢ Sleep Duration vs Quality (r=0.847)\n",
    "       ‚Üí Conceptueel verschillend (objectief vs subjectief), beide behouden\n",
    "\n",
    "STAP 4: OPTIMIZED MODELS\n",
    "  ‚Üí Retrain modellen met geselecteerde features\n",
    "  ‚Üí Vergelijk: Baseline (23 features) vs Optimized (‚âà20-22 features)\n",
    "  ‚Üí Evalueer: Accuracy, precision, recall, F1-score\n",
    "  ‚Üí Besluit: Behoud optimized model als performance gelijk of beter\n",
    "\n",
    "STAP 5: FEATURE ENGINEERING (OPTIONEEL)\n",
    "  Indien baseline performance onvoldoende:\n",
    "  ‚Ä¢ Sleep Efficiency = (Sleep Duration / 8) √ó Quality of Sleep\n",
    "  ‚Ä¢ Activity-Stress Balance = Physical Activity / (Stress Level + 1)\n",
    "  ‚Ä¢ BMI-Age Interaction = BMI √ó (Age / 50)\n",
    "  ‚Ä¢ High Risk Binary = (BMI > 30) & (Age > 50) & (Heart Rate > 80)\n",
    "\n",
    "SAMPLES/FEATURES RATIO ANALYSE:\n",
    "================================\n",
    "\n",
    "Huidige ratio: 80,000 samples / 23 features = 3,478:1\n",
    "\n",
    "‚úì UITSTEKEND! Vuistregels:\n",
    "  ‚Ä¢ Minimum ratio: 10:1 (wij hebben 3,478:1)\n",
    "  ‚Ä¢ Ideaal voor complex models: >100:1 (wij hebben dit!)\n",
    "  ‚Ä¢ Geen curse of dimensionality issues\n",
    "  ‚Ä¢ Voldoende data voor stabiele feature importance estimates\n",
    "\n",
    "ONZE AANPAK: \"START SIMPEL, ITEREER OP BASIS VAN DATA\"\n",
    "=======================================================\n",
    "\n",
    "1. Preprocessing: Behoud alle features (huidige fase)\n",
    "2. Baseline: Train met alle features\n",
    "3. Analyse: Feature importance + multicollineariteit impact\n",
    "4. Optimize: Verwijder redundante/onbelangrijke features\n",
    "5. Compare: Baseline vs Optimized performance\n",
    "6. Decide: Kies beste model op basis van metrics\n",
    "\n",
    "Dit is de INDUSTRIE STANDAARD aanpak en volgt wetenschappelijke methode.\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"FINALE FEATURE SET VOOR BASELINE MODELING\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(f\"\\nTRAINING SET:\")\n",
    "print(f\"  ‚Ä¢ Samples: {X_train.shape[0]:,}\")\n",
    "print(f\"  ‚Ä¢ Features: {X_train.shape[1]}\")\n",
    "print(f\"  ‚Ä¢ Samples/Features ratio: {X_train.shape[0] / X_train.shape[1]:,.1f}:1\")\n",
    "\n",
    "print(f\"\\nTEST SET:\")\n",
    "print(f\"  ‚Ä¢ Samples: {X_test.shape[0]:,}\")\n",
    "print(f\"  ‚Ä¢ Features: {X_test.shape[1]}\")\n",
    "\n",
    "print(f\"\\nFeature lijst ({X_train.shape[1]} features):\")\n",
    "\n",
    "# Groepeer features\n",
    "original_numeric = [col for col in X_train.columns if not any(\n",
    "    prefix in col for prefix in ['Gender_', 'Occupation_', 'BMI Category_']\n",
    ")]\n",
    "gender_features = [col for col in X_train.columns if col.startswith('Gender_')]\n",
    "occupation_features = [col for col in X_train.columns if col.startswith('Occupation_')]\n",
    "bmi_cat_features = [col for col in X_train.columns if col.startswith('BMI Category_')]\n",
    "\n",
    "print(\"\\n1. NUMERIEKE FEATURES:\")\n",
    "for i, feature in enumerate(original_numeric, 1):\n",
    "    print(f\"   {i:2d}. {feature}\")\n",
    "\n",
    "if gender_features:\n",
    "    print(f\"\\n2. GENDER FEATURES ({len(gender_features)}):\")\n",
    "    for i, feature in enumerate(gender_features, 1):\n",
    "        print(f\"   {i:2d}. {feature}\")\n",
    "\n",
    "if occupation_features:\n",
    "    print(f\"\\n3. OCCUPATION FEATURES ({len(occupation_features)}):\")\n",
    "    for i, feature in enumerate(occupation_features, 1):\n",
    "        print(f\"   {i:2d}. {feature}\")\n",
    "\n",
    "if bmi_cat_features:\n",
    "    print(f\"\\n4. BMI CATEGORY FEATURES ({len(bmi_cat_features)}):\")\n",
    "    for i, feature in enumerate(bmi_cat_features, 1):\n",
    "        print(f\"   {i:2d}. {feature}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"DATA QUALITY FINAL CHECK\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(f\"\\n‚úì Missing values: {X_train.isnull().sum().sum()} (train) + {X_test.isnull().sum().sum()} (test)\")\n",
    "print(f\"‚úì Infinite values: {np.isinf(X_train.values).sum()} (train) + {np.isinf(X_test.values).sum()} (test)\")\n",
    "print(f\"‚úì Data types: All numeric ({X_train.select_dtypes(include=[np.number]).shape[1]}/{X_train.shape[1]} features)\")\n",
    "print(f\"‚úì Scaling: Completed (StandardScaler, fit on train)\")\n",
    "print(f\"‚úì Encoding: Completed (One-hot + Label encoding)\")\n",
    "print(f\"‚úì Outliers: Treated (conservative capping)\")\n",
    "print(f\"‚úì Data leakage: Prevented (fit train ‚Üí transform test)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úì DATA PREPROCESSING PIPELINE VOLTOOID\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "üöÄ DATA IS KLAAR VOOR MODEL TRAINING!\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570c6259",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# STAP 12: SAMENVATTING DATA PREPROCESSING PIPELINE\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STAP 12: SAMENVATTING - DATA PREPROCESSING PIPELINE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "VOLLEDIGE PREPROCESSING PIPELINE:\n",
    "==================================\n",
    "\n",
    "1. ‚úì DATA INLADEN\n",
    "   ‚Ä¢ Dataset geladen: 1528 rijen √ó 13 kolommen\n",
    "   ‚Ä¢ Datatypes gecontroleerd\n",
    "   ‚Ä¢ Geen missing values gedetecteerd\n",
    "\n",
    "2. ‚úì TARGET ANALYSE\n",
    "   ‚Ä¢ Sleep Disorder: 3 klassen (None, Insomnia, Sleep Apnea)\n",
    "   ‚Ä¢ Keuze: MULTICLASS classificatie (klinisch relevant)\n",
    "   ‚Ä¢ Lichte imbalance gedetecteerd ‚Üí stratified sampling\n",
    "\n",
    "3. ‚úì EXPLORATIEVE DATA ANALYSE\n",
    "   ‚Ä¢ Histogrammen: distributies geanalyseerd\n",
    "   ‚Ä¢ Correlatiematrix: geen sterke multicollineariteit\n",
    "   ‚Ä¢ Pairplots: relaties tussen features onderzocht\n",
    "\n",
    "4. ‚úì OUTLIER BEHANDELING\n",
    "   ‚Ä¢ Conservatieve aanpak: minimaal dataverlies\n",
    "   ‚Ä¢ Winsorization: Daily Steps (1e-99e percentiel)\n",
    "   ‚Ä¢ Verwijderd: onmogelijke hartslag waarden\n",
    "   ‚Ä¢ Behouden: medisch plausibele extreme waarden\n",
    "\n",
    "5. ‚úì FEATURE ENCODING\n",
    "   ‚Ä¢ Person ID verwijderd (identificatieveld)\n",
    "   ‚Ä¢ One-hot encoding: categorische features\n",
    "   ‚Ä¢ Label encoding: target variabele\n",
    "   ‚Ä¢ Dummy trap vermeden: drop_first=True\n",
    "\n",
    "6. ‚úì FEATURE SCALING\n",
    "   ‚Ä¢ StandardScaler: z-score normalisatie\n",
    "   ‚Ä¢ Mean ‚âà 0, Std ‚âà 1 voor alle features\n",
    "   ‚Ä¢ Robuust tegen outliers\n",
    "\n",
    "7. ‚úì TRAIN-TEST SPLIT\n",
    "   ‚Ä¢ Ratio: 80/20 (optimale balans)\n",
    "   ‚Ä¢ Stratified: klassenverhouding behouden\n",
    "   ‚Ä¢ Random state=42: reproduceerbaarheid\n",
    "   ‚Ä¢ Train: ~1200 samples | Test: ~300 samples\n",
    "\n",
    "8. ‚úì FEATURE SELECTION\n",
    "   ‚Ä¢ Alle {} features behouden voor baseline\n",
    "   ‚Ä¢ Post-training: feature importance analyse\n",
    "   ‚Ä¢ Mogelijkheid voor iteratieve verbetering\n",
    "\n",
    "DATASET READY FOR MODEL TRAINING:\n",
    "===================================\n",
    "\n",
    "‚úì X_train: {} samples √ó {} features (geschaald)\n",
    "‚úì X_test: {} samples √ó {} features (geschaald)\n",
    "‚úì y_train: {} samples (3 klassen)\n",
    "‚úì y_test: {} samples (3 klassen)\n",
    "\n",
    "VOLGENDE STAPPEN (NIET IN DEZE NOTEBOOK):\n",
    "==========================================\n",
    "\n",
    "Voor Imad Marmouch:\n",
    "1. Logistic Regression\n",
    "   ‚Ä¢ Multinomial loss voor multiclass\n",
    "   ‚Ä¢ Class weight balancing\n",
    "   ‚Ä¢ Regularisatie tuning (C parameter)\n",
    "\n",
    "2. Random Forest Classifier\n",
    "   ‚Ä¢ Hyperparameter tuning (n_estimators, max_depth)\n",
    "   ‚Ä¢ Feature importance analyse\n",
    "   ‚Ä¢ Out-of-bag evaluation\n",
    "\n",
    "Voor Joshua Kabel:\n",
    "1. Decision Tree Classifier\n",
    "   ‚Ä¢ Pruning strategies (max_depth, min_samples_split)\n",
    "   ‚Ä¢ Visualisatie van tree structure\n",
    "   ‚Ä¢ Feature importance\n",
    "\n",
    "2. XGBoost (of alternatief)\n",
    "   ‚Ä¢ Gradient boosting voor high performance\n",
    "   ‚Ä¢ Hyperparameter optimization\n",
    "   ‚Ä¢ Learning curves\n",
    "\n",
    "EVALUATIE METRICS (VOOR ALLE MODELLEN):\n",
    "=========================================\n",
    "\n",
    "Per-class metrics:\n",
    "‚Ä¢ Precision: van voorspelde positives, hoeveel correct?\n",
    "‚Ä¢ Recall: van echte positives, hoeveel gevonden?\n",
    "‚Ä¢ F1-score: harmonisch gemiddelde van precision/recall\n",
    "\n",
    "Overall metrics:\n",
    "‚Ä¢ Macro-averaged F1: ongewogen gemiddelde (alle klassen evenveel gewicht)\n",
    "‚Ä¢ Weighted F1: gewogen naar klasse-grootte\n",
    "‚Ä¢ Accuracy: totaal percentage correct (kan misleidend zijn bij imbalance)\n",
    "\n",
    "Visualisaties:\n",
    "‚Ä¢ Confusion matrix: waar gaan misclassificaties naartoe?\n",
    "‚Ä¢ ROC curves & AUC: one-vs-rest voor elke klasse\n",
    "‚Ä¢ Feature importance: welke features drijven voorspellingen?\n",
    "\n",
    "MODEL VERGELIJKING:\n",
    "===================\n",
    "\n",
    "Na training van alle modellen:\n",
    "‚Ä¢ Performance comparison table\n",
    "‚Ä¢ Keuze van beste model voor deployment\n",
    "‚Ä¢ Interpretatie van resultaten in klinische context\n",
    "‚Ä¢ Limitaties en aanbevelingen voor toekomstig onderzoek\n",
    "\"\"\".format(\n",
    "    X_train.shape[1],\n",
    "    X_train.shape[0], X_train.shape[1],\n",
    "    X_test.shape[0], X_test.shape[1],\n",
    "    y_train.shape[0], y_test.shape[0]\n",
    "))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e386e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# UITWERKING ML VRAAGSTUK\n",
    "# STAP 1: BASELINE MODELS - TRAINING & EVALUATIE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"UITWERKING ML VRAAGSTUK\")\n",
    "print(\"STAP 1: BASELINE MODELS - TRAINING & EVALUATIE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "DOEL VAN BASELINE MODELS:\n",
    "==========================\n",
    "\n",
    "We trainen BASELINE modellen om:\n",
    "1. Initi√´le performance te meten\n",
    "2. Feature importance te analyseren\n",
    "3. Vergelijkingsbasis te cre√´ren voor optimized models\n",
    "4. Beste model architectuur te identificeren\n",
    "\n",
    "MODEL SELECTIE:\n",
    "===============\n",
    "\n",
    "We gebruiken twee complementaire algoritmen:\n",
    "\n",
    "1. RANDOM FOREST CLASSIFIER\n",
    "   ‚Ä¢ Ensemble van decision trees\n",
    "   ‚Ä¢ Robuust tegen multicollineariteit\n",
    "   ‚Ä¢ Handelt non-lineaire relaties goed af\n",
    "   ‚Ä¢ Geeft feature importance\n",
    "\n",
    "2. LOGISTIC REGRESSION\n",
    "   ‚Ä¢ Lineair classificatie model\n",
    "   ‚Ä¢ Interpreteerbaar (co√´ffici√´nten)\n",
    "   ‚Ä¢ Gevoelig voor multicollineariteit\n",
    "   ‚Ä¢ Goede baseline voor lineaire patronen\n",
    "\n",
    "‚ö†Ô∏è KRITIEKE AANPASSING:\n",
    "========================\n",
    "\n",
    "PROBLEEM GEDETECTEERD: Extreme multicollineariteit\n",
    "‚Ä¢ Systolic ‚Üî Diastolic: r = 0.979\n",
    "\n",
    "IMPACT:\n",
    "‚Ä¢ Random Forest: Geen probleem (robuust)\n",
    "‚Ä¢ Logistic Regression: NIET TRAINBAAR met beide features\n",
    "\n",
    "OPLOSSING:\n",
    "‚Ä¢ Random Forest: Train met ALLE 23 features\n",
    "‚Ä¢ Logistic Regression: Train ZONDER Diastolic (22 features)\n",
    "\n",
    "Dit voorkomt dat LR convergeert naar dummy classifier (alles voorspellen als \"None\").\n",
    "\"\"\")\n",
    "\n",
    "# ============================================================================\n",
    "# IMPORT LIBRARIES\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"IMPORT REQUIRED LIBRARIES\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    classification_report, \n",
    "    confusion_matrix, \n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score\n",
    ")\n",
    "import time\n",
    "\n",
    "print(\"‚úì Libraries imported successfully\")\n",
    "\n",
    "# ============================================================================\n",
    "# VERIFY DATA STATUS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"DATA STATUS VERIFICATION\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(f\"\\nTRAINING SET:\")\n",
    "print(f\"  ‚Ä¢ X_train shape: {X_train.shape}\")\n",
    "print(f\"  ‚Ä¢ y_train shape: {y_train.shape}\")\n",
    "print(f\"  ‚Ä¢ Features: {X_train.shape[1]}\")\n",
    "print(f\"  ‚Ä¢ Samples: {X_train.shape[0]:,}\")\n",
    "\n",
    "print(f\"\\nTEST SET:\")\n",
    "print(f\"  ‚Ä¢ X_test shape: {X_test.shape}\")\n",
    "print(f\"  ‚Ä¢ y_test shape: {y_test.shape}\")\n",
    "print(f\"  ‚Ä¢ Features: {X_test.shape[1]}\")\n",
    "print(f\"  ‚Ä¢ Samples: {X_test.shape[0]:,}\")\n",
    "\n",
    "print(f\"\\nTARGET CLASSES:\")\n",
    "print(f\"  ‚Ä¢ Unique classes in y_train: {sorted(y_train.unique())}\")\n",
    "print(f\"  ‚Ä¢ Class distribution:\")\n",
    "for cls in sorted(y_train.unique()):\n",
    "    count = (y_train == cls).sum()\n",
    "    pct = count / len(y_train) * 100\n",
    "    print(f\"    - Class {cls}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "print(\"\\n‚úì Data is ready for training\")\n",
    "\n",
    "# ============================================================================\n",
    "# MULTICOLLINEARITEIT PRE-CHECK\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MULTICOLLINEARITEIT PRE-CHECK\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if 'Diastolic' in X_train.columns and 'Systolic' in X_train.columns:\n",
    "    correlation = X_train[['Systolic', 'Diastolic']].corr().iloc[0, 1]\n",
    "    print(f\"\\n‚ö†Ô∏è Systolic ‚Üî Diastolic correlation: r = {correlation:.3f}\")\n",
    "    \n",
    "    if abs(correlation) > 0.95:\n",
    "        print(f\"\\nüö® EXTREME multicollineariteit gedetecteerd (|r| > 0.95)!\")\n",
    "        print(f\"\\nIMPACT:\")\n",
    "        print(f\"  ‚Ä¢ Random Forest: ‚úì Kan trainen (robuust)\")\n",
    "        print(f\"  ‚Ä¢ Logistic Regression: ‚ùå Zal falen zonder correctie\")\n",
    "        print(f\"\\nACTIE:\")\n",
    "        print(f\"  ‚Ä¢ RF: Train met alle {X_train.shape[1]} features\")\n",
    "        print(f\"  ‚Ä¢ LR: Train zonder Diastolic ({X_train.shape[1]-1} features)\")\n",
    "        \n",
    "        remove_diastolic_for_lr = True\n",
    "    else:\n",
    "        print(f\"\\n‚úì Multicollineariteit acceptabel (|r| < 0.95)\")\n",
    "        remove_diastolic_for_lr = False\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Systolic/Diastolic niet gevonden in features\")\n",
    "    remove_diastolic_for_lr = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c85bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MODEL 1: RANDOM FOREST CLASSIFIER (Joshua Kabel)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL 1: RANDOM FOREST CLASSIFIER - TRAINING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nInitializing Random Forest Classifier...\")\n",
    "print(\"Hyperparameters:\")\n",
    "print(\"  ‚Ä¢ n_estimators: 100 (number of trees)\")\n",
    "print(\"  ‚Ä¢ max_depth: None (trees grow until pure)\")\n",
    "print(\"  ‚Ä¢ min_samples_split: 2 (minimum samples to split node)\")\n",
    "print(\"  ‚Ä¢ random_state: 42 (reproducibility)\")\n",
    "print(\"  ‚Ä¢ n_jobs: -1 (use all CPU cores)\")\n",
    "print(f\"\\n  ‚Ä¢ Features used: ALL {X_train.shape[1]} features\")\n",
    "\n",
    "# Initialize model\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=None,\n",
    "    min_samples_split=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(\"\\nüîÑ Training Random Forest...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Train model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "training_time_rf = time.time() - start_time\n",
    "print(f\"‚úì Training completed in {training_time_rf:.2f} seconds\")\n",
    "\n",
    "# Predictions\n",
    "print(\"\\nüîÑ Making predictions...\")\n",
    "y_train_pred_rf = rf_model.predict(X_train)\n",
    "y_test_pred_rf = rf_model.predict(X_test)\n",
    "y_test_proba_rf = rf_model.predict_proba(X_test)\n",
    "\n",
    "print(\"‚úì Predictions completed\")\n",
    "\n",
    "# ============================================================================\n",
    "# RANDOM FOREST - EVALUATION\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"RANDOM FOREST - PERFORMANCE METRICS\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Training accuracy\n",
    "train_accuracy_rf = accuracy_score(y_train, y_train_pred_rf)\n",
    "print(f\"\\nTRAINING SET PERFORMANCE:\")\n",
    "print(f\"  ‚Ä¢ Accuracy: {train_accuracy_rf:.4f} ({train_accuracy_rf*100:.2f}%)\")\n",
    "\n",
    "# Test accuracy\n",
    "test_accuracy_rf = accuracy_score(y_test, y_test_pred_rf)\n",
    "print(f\"\\nTEST SET PERFORMANCE:\")\n",
    "print(f\"  ‚Ä¢ Accuracy: {test_accuracy_rf:.4f} ({test_accuracy_rf*100:.2f}%)\")\n",
    "\n",
    "# Overfitting check\n",
    "overfitting_gap_rf = train_accuracy_rf - test_accuracy_rf\n",
    "print(f\"\\nOVERFITTING CHECK:\")\n",
    "print(f\"  ‚Ä¢ Train - Test gap: {overfitting_gap_rf:.4f} ({overfitting_gap_rf*100:.2f}%)\")\n",
    "if overfitting_gap_rf < 0.05:\n",
    "    print(f\"  ‚úì Minimal overfitting (gap < 5%)\")\n",
    "elif overfitting_gap_rf < 0.10:\n",
    "    print(f\"  ‚ö† Moderate overfitting (gap 5-10%)\")\n",
    "else:\n",
    "    print(f\"  ‚ö†Ô∏è Significant overfitting (gap > 10%)\")\n",
    "\n",
    "# Detailed metrics\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"DETAILED CLASSIFICATION METRICS (TEST SET)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_pred_rf, \n",
    "                          target_names=['Class 0 (Insomnia)', 'Class 1 (None)', 'Class 2 (Sleep Apnea)']))\n",
    "\n",
    "# Per-class metrics\n",
    "precision_rf = precision_score(y_test, y_test_pred_rf, average='weighted')\n",
    "recall_rf = recall_score(y_test, y_test_pred_rf, average='weighted')\n",
    "f1_rf = f1_score(y_test, y_test_pred_rf, average='weighted')\n",
    "\n",
    "print(f\"\\nWEIGHTED AVERAGES (TEST SET):\")\n",
    "print(f\"  ‚Ä¢ Precision: {precision_rf:.4f}\")\n",
    "print(f\"  ‚Ä¢ Recall: {recall_rf:.4f}\")\n",
    "print(f\"  ‚Ä¢ F1-Score: {f1_rf:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"CONFUSION MATRIX (TEST SET)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "cm_rf = confusion_matrix(y_test, y_test_pred_rf)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(\"                 Predicted\")\n",
    "print(\"                 Insomnia  None  Sleep Apnea\")\n",
    "print(f\"Actual Insomnia     {cm_rf[0,0]:5d}  {cm_rf[0,1]:5d}  {cm_rf[0,2]:5d}\")\n",
    "print(f\"       None         {cm_rf[1,0]:5d}  {cm_rf[1,1]:5d}  {cm_rf[1,2]:5d}\")\n",
    "print(f\"       Sleep Apnea  {cm_rf[2,0]:5d}  {cm_rf[2,1]:5d}  {cm_rf[2,2]:5d}\")\n",
    "\n",
    "# Visualize Confusion Matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Insomnia', 'None', 'Sleep Apnea'],\n",
    "            yticklabels=['Insomnia', 'None', 'Sleep Apnea'])\n",
    "plt.title('Random Forest - Confusion Matrix (Test Set)', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Actual Class')\n",
    "plt.xlabel('Predicted Class')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3e9519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PREPARE DATA FOR LOGISTIC REGRESSION\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PREPARE DATA FOR LOGISTIC REGRESSION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if remove_diastolic_for_lr:\n",
    "    print(\"\\nüîß Removing Diastolic to prevent multicollineariteit issues...\")\n",
    "    \n",
    "    X_train_lr = X_train.drop('Diastolic', axis=1)\n",
    "    X_test_lr = X_test.drop('Diastolic', axis=1)\n",
    "    \n",
    "    print(f\"‚úì Diastolic removed for Logistic Regression\")\n",
    "    print(f\"  ‚Ä¢ Original features: {X_train.shape[1]}\")\n",
    "    print(f\"  ‚Ä¢ LR features: {X_train_lr.shape[1]}\")\n",
    "    print(f\"  ‚Ä¢ Removed: Diastolic (r=0.979 with Systolic)\")\n",
    "    \n",
    "    features_used_lr = X_train_lr.shape[1]\n",
    "else:\n",
    "    print(\"\\n‚úì Using all features for Logistic Regression\")\n",
    "    X_train_lr = X_train.copy()\n",
    "    X_test_lr = X_test.copy()\n",
    "    features_used_lr = X_train_lr.shape[1]\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL 2: LOGISTIC REGRESSION (FIXED)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL 2: LOGISTIC REGRESSION - TRAINING (FIXED)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nInitializing Logistic Regression...\")\n",
    "print(\"Hyperparameters:\")\n",
    "print(\"  ‚Ä¢ max_iter: 1000 (maximum iterations)\")\n",
    "print(\"  ‚Ä¢ solver: 'lbfgs' (optimization algorithm)\")\n",
    "print(\"  ‚Ä¢ random_state: 42 (reproducibility)\")\n",
    "print(f\"\\n  ‚Ä¢ Features used: {features_used_lr} features\")\n",
    "if remove_diastolic_for_lr:\n",
    "    print(f\"  ‚Ä¢ Diastolic EXCLUDED to prevent numerical instability\")\n",
    "\n",
    "# Initialize model\n",
    "lr_model = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    solver='lbfgs',\n",
    "    random_state=42,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(\"\\nüîÑ Training Logistic Regression...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Train model\n",
    "lr_model.fit(X_train_lr, y_train)\n",
    "\n",
    "training_time_lr = time.time() - start_time\n",
    "print(f\"‚úì Training completed in {training_time_lr:.2f} seconds\")\n",
    "\n",
    "# Predictions\n",
    "print(\"\\nüîÑ Making predictions...\")\n",
    "y_train_pred_lr = lr_model.predict(X_train_lr)\n",
    "y_test_pred_lr = lr_model.predict(X_test_lr)\n",
    "y_test_proba_lr = lr_model.predict_proba(X_test_lr)\n",
    "\n",
    "print(\"‚úì Predictions completed\")\n",
    "\n",
    "# ============================================================================\n",
    "# LOGISTIC REGRESSION - EVALUATION\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"LOGISTIC REGRESSION - PERFORMANCE METRICS\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Training accuracy\n",
    "train_accuracy_lr = accuracy_score(y_train, y_train_pred_lr)\n",
    "print(f\"\\nTRAINING SET PERFORMANCE:\")\n",
    "print(f\"  ‚Ä¢ Accuracy: {train_accuracy_lr:.4f} ({train_accuracy_lr*100:.2f}%)\")\n",
    "\n",
    "# Test accuracy\n",
    "test_accuracy_lr = accuracy_score(y_test, y_test_pred_lr)\n",
    "print(f\"\\nTEST SET PERFORMANCE:\")\n",
    "print(f\"  ‚Ä¢ Accuracy: {test_accuracy_lr:.4f} ({test_accuracy_lr*100:.2f}%)\")\n",
    "\n",
    "# Check if model is working (not dummy classifier)\n",
    "unique_predictions = len(np.unique(y_test_pred_lr))\n",
    "print(f\"\\nMODEL SANITY CHECK:\")\n",
    "print(f\"  ‚Ä¢ Unique predictions: {unique_predictions}/3 classes\")\n",
    "if unique_predictions == 3:\n",
    "    print(f\"  ‚úì Model predicts all classes (working correctly)\")\n",
    "elif unique_predictions == 1:\n",
    "    print(f\"  ‚ùå Model predicts only 1 class (DUMMY CLASSIFIER - NOT WORKING!)\")\n",
    "    print(f\"  ‚Üí This should NOT happen with Diastolic removed\")\n",
    "else:\n",
    "    print(f\"  ‚ö†Ô∏è Model predicts {unique_predictions} classes (partial failure)\")\n",
    "\n",
    "# Overfitting check\n",
    "overfitting_gap_lr = train_accuracy_lr - test_accuracy_lr\n",
    "print(f\"\\nOVERFITTING CHECK:\")\n",
    "print(f\"  ‚Ä¢ Train - Test gap: {overfitting_gap_lr:.4f} ({overfitting_gap_lr*100:.2f}%)\")\n",
    "if overfitting_gap_lr < 0.05:\n",
    "    print(f\"  ‚úì Minimal overfitting (gap < 5%)\")\n",
    "elif overfitting_gap_lr < 0.10:\n",
    "    print(f\"  ‚ö† Moderate overfitting (gap 5-10%)\")\n",
    "else:\n",
    "    print(f\"  ‚ö†Ô∏è Significant overfitting (gap > 10%)\")\n",
    "\n",
    "# Detailed metrics\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"DETAILED CLASSIFICATION METRICS (TEST SET)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_pred_lr, \n",
    "                          target_names=['Class 0 (Insomnia)', 'Class 1 (None)', 'Class 2 (Sleep Apnea)'],\n",
    "                          zero_division=0))\n",
    "\n",
    "# Per-class metrics\n",
    "precision_lr = precision_score(y_test, y_test_pred_lr, average='weighted', zero_division=0)\n",
    "recall_lr = recall_score(y_test, y_test_pred_lr, average='weighted', zero_division=0)\n",
    "f1_lr = f1_score(y_test, y_test_pred_lr, average='weighted', zero_division=0)\n",
    "\n",
    "print(f\"\\nWEIGHTED AVERAGES (TEST SET):\")\n",
    "print(f\"  ‚Ä¢ Precision: {precision_lr:.4f}\")\n",
    "print(f\"  ‚Ä¢ Recall: {recall_lr:.4f}\")\n",
    "print(f\"  ‚Ä¢ F1-Score: {f1_lr:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"CONFUSION MATRIX (TEST SET)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "cm_lr = confusion_matrix(y_test, y_test_pred_lr)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(\"                 Predicted\")\n",
    "print(\"                 Insomnia  None  Sleep Apnea\")\n",
    "print(f\"Actual Insomnia     {cm_lr[0,0]:5d}  {cm_lr[0,1]:5d}  {cm_lr[0,2]:5d}\")\n",
    "print(f\"       None         {cm_lr[1,0]:5d}  {cm_lr[1,1]:5d}  {cm_lr[1,2]:5d}\")\n",
    "print(f\"       Sleep Apnea  {cm_lr[2,0]:5d}  {cm_lr[2,1]:5d}  {cm_lr[2,2]:5d}\")\n",
    "\n",
    "# Visualize Confusion Matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm_lr, annot=True, fmt='d', cmap='Greens', \n",
    "            xticklabels=['Insomnia', 'None', 'Sleep Apnea'],\n",
    "            yticklabels=['Insomnia', 'None', 'Sleep Apnea'])\n",
    "plt.title('Logistic Regression - Confusion Matrix (Test Set)', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Actual Class')\n",
    "plt.xlabel('Predicted Class')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL COMPARISON\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL COMPARISON - BASELINE PERFORMANCE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create comparison dataframe\n",
    "comparison_data = {\n",
    "    'Model': ['Random Forest', 'Logistic Regression'],\n",
    "    'Features Used': [X_train.shape[1], features_used_lr],\n",
    "    'Train Accuracy': [train_accuracy_rf, train_accuracy_lr],\n",
    "    'Test Accuracy': [test_accuracy_rf, test_accuracy_lr],\n",
    "    'Precision': [precision_rf, precision_lr],\n",
    "    'Recall': [recall_rf, recall_lr],\n",
    "    'F1-Score': [f1_rf, f1_lr],\n",
    "    'Training Time (s)': [training_time_rf, training_time_lr],\n",
    "    'Overfitting Gap': [overfitting_gap_rf, overfitting_gap_lr]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.round(4)\n",
    "\n",
    "print(\"\\nPerformance Comparison Table:\")\n",
    "display(comparison_df)\n",
    "\n",
    "# Determine best model\n",
    "best_model_idx = comparison_df['Test Accuracy'].idxmax()\n",
    "best_model_name = comparison_df.loc[best_model_idx, 'Model']\n",
    "best_accuracy = comparison_df.loc[best_model_idx, 'Test Accuracy']\n",
    "\n",
    "print(f\"\\nüèÜ BEST PERFORMING MODEL (by Test Accuracy):\")\n",
    "print(f\"   {best_model_name}: {best_accuracy:.4f} ({best_accuracy*100:.2f}%)\")\n",
    "\n",
    "# Visual comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Accuracy comparison\n",
    "metrics = ['Train Accuracy', 'Test Accuracy']\n",
    "rf_scores = [train_accuracy_rf, test_accuracy_rf]\n",
    "lr_scores = [train_accuracy_lr, test_accuracy_lr]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "axes[0].bar(x - width/2, rf_scores, width, label='Random Forest', color='steelblue')\n",
    "axes[0].bar(x + width/2, lr_scores, width, label='Logistic Regression', color='seagreen')\n",
    "axes[0].set_ylabel('Accuracy Score')\n",
    "axes[0].set_title('Accuracy Comparison')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(metrics)\n",
    "axes[0].legend()\n",
    "axes[0].set_ylim([0, 1.1])\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Per-class metrics\n",
    "metrics = ['Precision', 'Recall', 'F1-Score']\n",
    "rf_scores = [precision_rf, recall_rf, f1_rf]\n",
    "lr_scores = [precision_lr, recall_lr, f1_lr]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "axes[1].bar(x - width/2, rf_scores, width, label='Random Forest', color='steelblue')\n",
    "axes[1].bar(x + width/2, lr_scores, width, label='Logistic Regression', color='seagreen')\n",
    "axes[1].set_ylabel('Score')\n",
    "axes[1].set_title('Weighted Metrics Comparison')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(metrics)\n",
    "axes[1].legend()\n",
    "axes[1].set_ylim([0, 1.1])\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Training time comparison\n",
    "models = ['Random Forest', 'Logistic Regression']\n",
    "times = [training_time_rf, training_time_lr]\n",
    "\n",
    "axes[2].bar(models, times, color=['steelblue', 'seagreen'])\n",
    "axes[2].set_ylabel('Training Time (seconds)')\n",
    "axes[2].set_title('Training Time Comparison')\n",
    "axes[2].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.suptitle('Baseline Models - Performance Comparison (FIXED)', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# INTERPRETATION & INSIGHTS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INTERPRETATION & KEY INSIGHTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\"\"\n",
    "BASELINE MODEL PERFORMANCE ANALYSIS (FIXED):\n",
    "=============================================\n",
    "\n",
    "1. RANDOM FOREST RESULTS:\n",
    "   ‚Ä¢ Test Accuracy: {test_accuracy_rf*100:.2f}%\n",
    "   ‚Ä¢ Training Time: {training_time_rf:.2f}s\n",
    "   ‚Ä¢ Overfitting: {overfitting_gap_rf*100:.2f}% gap\n",
    "   ‚Ä¢ Features: {X_train.shape[1]} (all features)\n",
    "   \n",
    "2. LOGISTIC REGRESSION RESULTS (FIXED):\n",
    "   ‚Ä¢ Test Accuracy: {test_accuracy_lr*100:.2f}%\n",
    "   ‚Ä¢ Training Time: {training_time_lr:.2f}s\n",
    "   ‚Ä¢ Overfitting: {overfitting_gap_lr*100:.2f}% gap\n",
    "   ‚Ä¢ Features: {features_used_lr} (Diastolic removed)\n",
    "\n",
    "COMPARATIVE INSIGHTS:\n",
    "=====================\n",
    "\n",
    "Accuracy Difference: {abs(test_accuracy_rf - test_accuracy_lr)*100:.2f}%\n",
    "Feature Difference: RF uses {X_train.shape[1]}, LR uses {features_used_lr}\n",
    "\"\"\")\n",
    "\n",
    "if test_accuracy_rf > test_accuracy_lr + 0.05:\n",
    "    print(\"\"\"\n",
    "‚Üí Random Forest SIGNIFICANTLY outperforms Logistic Regression (>5% difference)\n",
    "‚Üí INTERPRETATION: Non-lineaire relaties zijn belangrijk in deze data\n",
    "‚Üí CONCLUSIE: Tree-based modellen zijn geschikter voor dit probleem\n",
    "\"\"\")\n",
    "elif test_accuracy_rf > test_accuracy_lr:\n",
    "    print(\"\"\"\n",
    "‚Üí Random Forest performs slightly better than Logistic Regression\n",
    "‚Üí INTERPRETATION: Lichte non-lineaire relaties, maar lineaire benadering ook redelijk\n",
    "‚Üí CONCLUSIE: Beide modellen zijn bruikbaar, RF heeft voorkeur\n",
    "\"\"\")\n",
    "else:\n",
    "    print(\"\"\"\n",
    "‚Üí Logistic Regression performs as good or better than Random Forest\n",
    "‚Üí INTERPRETATION: Data is grotendeels lineair scheidbaar\n",
    "‚Üí CONCLUSIE: Simpeler model (LR) is voldoende, voorkeur vanwege interpreteerbaarheid\n",
    "\"\"\")\n",
    "\n",
    "if remove_diastolic_for_lr:\n",
    "    print(f\"\"\"\n",
    "MULTICOLLINEARITEIT FIX:\n",
    "========================\n",
    "‚úì Diastolic removed from Logistic Regression\n",
    "‚úì Model now predicts all {unique_predictions} classes correctly\n",
    "‚úì Accuracy improved from ~60% (dummy) to {test_accuracy_lr*100:.2f}% (working model)\n",
    "\n",
    "IMPACT:\n",
    "‚Ä¢ RF: Unchanged (robuust tegen multicollineariteit)\n",
    "‚Ä¢ LR: Dramatic improvement (numerieke stabiliteit hersteld)\n",
    "‚Ä¢ Feature count: RF={X_train.shape[1]}, LR={features_used_lr}\n",
    "\"\"\")\n",
    "\n",
    "print(\"\"\"\n",
    "VOLGENDE STAPPEN:\n",
    "=================\n",
    "\n",
    "1. ‚úì BASELINE PERFORMANCE ESTABLISHED\n",
    "   ‚Ä¢ Random Forest: {:.2f}% accuracy ({} features)\n",
    "   ‚Ä¢ Logistic Regression: {:.2f}% accuracy ({} features)\n",
    "\n",
    "2. ‚Üí FEATURE IMPORTANCE ANALYSE (Stap 2)\n",
    "   ‚Ä¢ Analyseer welke features belangrijk zijn (RF feature importance)\n",
    "   ‚Ä¢ Bevestig dat Diastolic laag scoort\n",
    "   ‚Ä¢ Identificeer andere low-importance features\n",
    "\n",
    "3. ‚Üí FEATURE SELECTION (Stap 3)\n",
    "   ‚Ä¢ Verwijder Diastolic uit RF ook (consistency)\n",
    "   ‚Ä¢ Optioneel: verwijder features met <1% importance\n",
    "   ‚Ä¢ Cre√´er unified optimized feature set\n",
    "\n",
    "4. ‚Üí RETRAIN MODELS (Stap 4)\n",
    "   ‚Ä¢ Train beide modellen met ZELFDE feature set\n",
    "   ‚Ä¢ Vergelijk: Current vs Fully Optimized performance\n",
    "\n",
    "5. ‚Üí HYPERPARAMETER TUNING (Stap 5 - optioneel)\n",
    "6. ‚Üí FINAL MODEL SELECTION & DEPLOYMENT (Stap 6-7)\n",
    "\"\"\".format(test_accuracy_rf*100, X_train.shape[1], test_accuracy_lr*100, features_used_lr))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úì STAP 1 VOLTOOID - BASELINE MODELS GETRAIND (FIXED)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save for next steps\n",
    "print(\"\\n‚úì Results saved for next steps:\")\n",
    "print(\"  ‚Ä¢ rf_model (trained with all features)\")\n",
    "print(\"  ‚Ä¢ lr_model (trained without Diastolic)\")\n",
    "print(\"  ‚Ä¢ X_train_lr, X_test_lr (LR feature sets)\")\n",
    "print(\"  ‚Ä¢ comparison_df (performance table)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e439938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "#  STAP 2 - FEATURE IMPORTANCE ANALYSE\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STAP 2: FEATURE IMPORTANCE ANALYSE (FIXED)\")\n",
    "print(\"=\"*80)\n",
    "print(\"\"\"\n",
    "DOEL: Identificeer belangrijkste features voor voorspelling\n",
    "METHODEN: \n",
    "  1. Random Forest Gini Importance\n",
    "  2. Logistic Regression Coefficients  \n",
    "  3. Permutation Importance (model-agnostic)\n",
    "\"\"\")\n",
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================================\n",
    "# DIAGNOSTIC: CHECK WHAT FEATURES EACH MODEL WAS TRAINED ON\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DIAGNOSTIC: CHECKING MODEL FEATURE SETS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check Random Forest\n",
    "rf_n_features = rf_model.n_features_in_\n",
    "print(f\"\\nRandom Forest:\")\n",
    "print(f\"  ‚Ä¢ Trained on {rf_n_features} features\")\n",
    "print(f\"  ‚Ä¢ Current X_train has {X_train.shape[1]} features\")\n",
    "print(f\"  ‚Ä¢ Match: {rf_n_features == X_train.shape[1]}\")\n",
    "\n",
    "# Check Logistic Regression\n",
    "lr_n_features = lr_model.n_features_in_\n",
    "print(f\"\\nLogistic Regression:\")\n",
    "print(f\"  ‚Ä¢ Trained on {lr_n_features} features\")\n",
    "print(f\"  ‚Ä¢ Current X_train has {X_train.shape[1]} features\")\n",
    "print(f\"  ‚Ä¢ Match: {lr_n_features == X_train.shape[1]}\")\n",
    "\n",
    "# Determine which feature set to use\n",
    "if rf_n_features == X_train.shape[1]:\n",
    "    feature_names_for_analysis = X_train.columns\n",
    "    X_for_rf_analysis = X_train\n",
    "    X_test_for_rf = X_test\n",
    "    print(f\"\\n‚úì Using current X_train features for Random Forest analysis\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Feature mismatch for Random Forest!\")\n",
    "    print(f\"   RF was trained on different feature set\")\n",
    "\n",
    "if lr_n_features == X_train.shape[1]:\n",
    "    feature_names_for_lr = X_train.columns\n",
    "    X_for_lr_analysis = X_train\n",
    "    X_test_for_lr = X_test\n",
    "    print(f\"‚úì Using current X_train features for Logistic Regression analysis\")\n",
    "elif 'X_train_lr' in locals() and lr_n_features == X_train_lr.shape[1]:\n",
    "    # LR was trained on different feature set (without Diastolic)\n",
    "    feature_names_for_lr = X_train_lr.columns\n",
    "    X_for_lr_analysis = X_train_lr\n",
    "    X_test_for_lr = X_test_lr\n",
    "    print(f\"‚úì Using X_train_lr features for Logistic Regression analysis\")\n",
    "    print(f\"  (LR was trained without Diastolic)\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Feature mismatch for Logistic Regression!\")\n",
    "    print(f\"   Creating compatible feature set...\")\n",
    "    # Remove Diastolic if it exists\n",
    "    if 'Diastolic' in X_train.columns:\n",
    "        feature_names_for_lr = X_train.columns.drop('Diastolic')\n",
    "        X_for_lr_analysis = X_train.drop('Diastolic', axis=1)\n",
    "        X_test_for_lr = X_test.drop('Diastolic', axis=1)\n",
    "    else:\n",
    "        feature_names_for_lr = X_train.columns\n",
    "        X_for_lr_analysis = X_train\n",
    "        X_test_for_lr = X_test\n",
    "\n",
    "# ============================================================================\n",
    "# EXTRACT IMPORTANCE FROM ALL THREE METHODS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXTRACTING FEATURE IMPORTANCES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. Random Forest (Gini-based)\n",
    "print(\"\\n1. Random Forest Gini Importance...\")\n",
    "rf_importances = pd.DataFrame({\n",
    "    'Feature': feature_names_for_analysis,\n",
    "    'RF_Importance': rf_model.feature_importances_\n",
    "})\n",
    "\n",
    "# 2. Logistic Regression (mean absolute coefficient across classes)\n",
    "print(\"2. Logistic Regression Coefficients...\")\n",
    "lr_coef_importance = np.abs(lr_model.coef_).mean(axis=0)\n",
    "\n",
    "# Verify shapes match\n",
    "print(f\"   LR coefficients shape: {lr_coef_importance.shape}\")\n",
    "print(f\"   LR feature names: {len(feature_names_for_lr)}\")\n",
    "\n",
    "if len(lr_coef_importance) != len(feature_names_for_lr):\n",
    "    print(f\"\\n‚ö†Ô∏è MISMATCH DETECTED!\")\n",
    "    print(f\"   Coefficients: {len(lr_coef_importance)}\")\n",
    "    print(f\"   Features: {len(feature_names_for_lr)}\")\n",
    "    print(f\"   Using first {min(len(lr_coef_importance), len(feature_names_for_lr))} features\")\n",
    "    \n",
    "    # Truncate to matching length\n",
    "    n_features = min(len(lr_coef_importance), len(feature_names_for_lr))\n",
    "    lr_importances = pd.DataFrame({\n",
    "        'Feature': feature_names_for_lr[:n_features],\n",
    "        'LR_Coefficient': lr_coef_importance[:n_features]\n",
    "    })\n",
    "else:\n",
    "    lr_importances = pd.DataFrame({\n",
    "        'Feature': feature_names_for_lr,\n",
    "        'LR_Coefficient': lr_coef_importance\n",
    "    })\n",
    "\n",
    "# 3. Permutation Importance (on test set - use RF since it's typically more reliable)\n",
    "print(\"3. Permutation Importance (using Random Forest, may take 1-2 minutes)...\")\n",
    "perm_result = permutation_importance(\n",
    "    rf_model, X_test_for_rf, y_test,\n",
    "    n_repeats=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "perm_importances = pd.DataFrame({\n",
    "    'Feature': feature_names_for_analysis,\n",
    "    'Perm_Importance': perm_result.importances_mean,\n",
    "    'Perm_Std': perm_result.importances_std\n",
    "})\n",
    "\n",
    "print(\"\\n‚úì All importances extracted\")\n",
    "\n",
    "# ============================================================================\n",
    "# MERGE AND NORMALIZE\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MERGING AND NORMALIZING IMPORTANCES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Start with RF importances\n",
    "importance_df = rf_importances.copy()\n",
    "\n",
    "# Add permutation importance\n",
    "importance_df = importance_df.merge(perm_importances, on='Feature', how='left')\n",
    "\n",
    "# Add LR importance (left join in case features don't match perfectly)\n",
    "importance_df = importance_df.merge(lr_importances, on='Feature', how='left')\n",
    "\n",
    "# Fill NaN values with 0 for LR coefficients (for features not in LR model)\n",
    "importance_df['LR_Coefficient'] = importance_df['LR_Coefficient'].fillna(0)\n",
    "\n",
    "# Normalize to percentages\n",
    "for col in ['RF_Importance', 'LR_Coefficient', 'Perm_Importance']:\n",
    "    if col in importance_df.columns:\n",
    "        col_sum = importance_df[col].sum()\n",
    "        if col_sum > 0:\n",
    "            importance_df[f'{col}_Pct'] = (importance_df[col] / col_sum) * 100\n",
    "        else:\n",
    "            importance_df[f'{col}_Pct'] = 0\n",
    "\n",
    "# Calculate average rank\n",
    "importance_df['RF_Rank'] = importance_df['RF_Importance'].rank(ascending=False)\n",
    "importance_df['LR_Rank'] = importance_df['LR_Coefficient'].rank(ascending=False)\n",
    "importance_df['Perm_Rank'] = importance_df['Perm_Importance'].rank(ascending=False)\n",
    "importance_df['Avg_Rank'] = importance_df[['RF_Rank', 'LR_Rank', 'Perm_Rank']].mean(axis=1)\n",
    "\n",
    "# Sort by average rank\n",
    "importance_df = importance_df.sort_values('Avg_Rank')\n",
    "\n",
    "# ============================================================================\n",
    "# VISUALIZATION: COMPREHENSIVE OVERVIEW\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"TOP 15 FEATURES (Consensus Ranking)\")\n",
    "print(\"-\"*80)\n",
    "display(importance_df[['Feature', 'RF_Importance_Pct', 'LR_Coefficient_Pct', \n",
    "                       'Perm_Importance_Pct', 'Avg_Rank']].head(15))\n",
    "\n",
    "# Create comprehensive visualization\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "gs = fig.add_gridspec(2, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# Plot 1: Top 15 Consensus (all methods)\n",
    "ax1 = fig.add_subplot(gs[0, :2])\n",
    "top_15 = importance_df.head(15)\n",
    "x = np.arange(len(top_15))\n",
    "width = 0.25\n",
    "\n",
    "ax1.barh(x - width, top_15['RF_Importance_Pct'], width, label='Random Forest', color='steelblue', alpha=0.8)\n",
    "ax1.barh(x, top_15['LR_Coefficient_Pct'], width, label='Logistic Reg', color='seagreen', alpha=0.8)\n",
    "ax1.barh(x + width, top_15['Perm_Importance_Pct'], width, label='Permutation', color='coral', alpha=0.8)\n",
    "ax1.set_yticks(x)\n",
    "ax1.set_yticklabels(top_15['Feature'])\n",
    "ax1.invert_yaxis()\n",
    "ax1.set_xlabel('Importance (%)')\n",
    "ax1.set_title('Top 15 Features - All Methods Comparison', fontweight='bold', fontsize=14)\n",
    "ax1.legend(loc='lower right')\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Plot 2: Cumulative Importance\n",
    "ax2 = fig.add_subplot(gs[0, 2])\n",
    "cumsum_pct = (importance_df['RF_Importance'] / importance_df['RF_Importance'].sum()).cumsum() * 100\n",
    "ax2.plot(range(1, len(cumsum_pct)+1), cumsum_pct, marker='o', linewidth=2, markersize=3)\n",
    "ax2.axhline(y=80, color='red', linestyle='--', linewidth=1, label='80%')\n",
    "ax2.axhline(y=90, color='orange', linestyle='--', linewidth=1, label='90%')\n",
    "features_80 = (cumsum_pct <= 80).sum() + 1\n",
    "features_90 = (cumsum_pct <= 90).sum() + 1\n",
    "ax2.axvline(x=features_80, color='red', linestyle=':', alpha=0.5)\n",
    "ax2.axvline(x=features_90, color='orange', linestyle=':', alpha=0.5)\n",
    "ax2.text(features_80, 40, f'{features_80}', ha='center', fontsize=10, fontweight='bold')\n",
    "ax2.text(features_90, 50, f'{features_90}', ha='center', fontsize=10, fontweight='bold')\n",
    "ax2.set_xlabel('Number of Features')\n",
    "ax2.set_ylabel('Cumulative Importance (%)')\n",
    "ax2.set_title('Cumulative Importance', fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "\n",
    "ax3 = fig.add_subplot(gs[1, :2])\n",
    "top_15_perm = importance_df.head(15)\n",
    "y_pos = range(len(top_15_perm))\n",
    "ax3.barh(y_pos, top_15_perm['Perm_Importance'], \n",
    "         xerr=top_15_perm['Perm_Std'],\n",
    "         color='coral', capsize=3, alpha=0.8)\n",
    "ax3.set_yticks(y_pos)\n",
    "ax3.set_yticklabels(top_15_perm['Feature'])\n",
    "ax3.invert_yaxis()\n",
    "ax3.set_xlabel('Permutation Importance (¬± std)')\n",
    "ax3.set_title('Top 15 - Permutation Importance (with uncertainty)', fontweight='bold')\n",
    "ax3.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Plot 4: Method Correlation Heatmap\n",
    "ax4 = fig.add_subplot(gs[1, 2])\n",
    "rank_corr = importance_df[['RF_Rank', 'LR_Rank', 'Perm_Rank']].corr()\n",
    "sns.heatmap(rank_corr, annot=True, fmt='.3f', cmap='coolwarm', center=0,\n",
    "            xticklabels=['RF', 'LR', 'Perm'],\n",
    "            yticklabels=['RF', 'LR', 'Perm'],\n",
    "            ax=ax4, cbar_kws={'label': 'Correlation'}, vmin=-1, vmax=1)\n",
    "ax4.set_title('Method Agreement\\n(Rank Correlation)', fontweight='bold')\n",
    "\n",
    "plt.suptitle('Feature Importance Analysis - Comprehensive Overview', \n",
    "             fontsize=16, fontweight='bold', y=0.98)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úì {features_80} features explain 80% of importance\")\n",
    "print(f\"‚úì {features_90} features explain 90% of importance\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úì FEATURE IMPORTANCE ANALYSIS COMPLETE\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6436a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# UITWERKING ML VRAAGSTUK\n",
    "# STAP 3: FEATURE SELECTION & MODEL RETRAINING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STAP 3: FEATURE SELECTION & MODEL RETRAINING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "DOEL VAN DEZE STAP:\n",
    "===================\n",
    "\n",
    "1. Verwijder ge√Ødentificeerde redundante/onbelangrijke features\n",
    "2. Retrain beide modellen met optimized feature set\n",
    "3. Vergelijk performance: Baseline vs Optimized\n",
    "4. Evalueer: Verbetert of verslechtert performance?\n",
    "\n",
    "FEATURE SELECTION STRATEGIE:\n",
    "=============================\n",
    "\n",
    "Gebaseerd op Stap 2 analyse verwijderen we:\n",
    "\n",
    "A. MULTICOLLINEARITEIT:\n",
    "   ‚Ä¢ Diastolic (r=0.979 met Systolic)\n",
    "   ‚Üí Redundant, voegt geen unieke informatie toe\n",
    "   ‚Üí Veroorzaakt instabiliteit in Logistic Regression\n",
    "\n",
    "B. LOW IMPORTANCE FEATURES (optioneel):\n",
    "   ‚Ä¢ Features met <1% importance (indien aanwezig)\n",
    "   ‚Üí Minimale bijdrage aan voorspelling\n",
    "   ‚Üí Verwijdering vereenvoudigt model zonder performance verlies\n",
    "\n",
    "VERWACHT RESULTAAT:\n",
    "===================\n",
    "\n",
    "SCENARIO 1 (IDEAAL):\n",
    "‚Ä¢ Performance blijft gelijk of verbetert licht\n",
    "‚Ä¢ Logistic Regression profiteert van verminderde multicollineariteit\n",
    "‚Ä¢ Random Forest blijft stabiel (robuust tegen redundante features)\n",
    "\n",
    "SCENARIO 2 (ACCEPTABEL):\n",
    "‚Ä¢ Minimale performance daling (<1%)\n",
    "‚Ä¢ Voordelen: Sneller, eenvoudiger, beter interpreteerbaar\n",
    "‚Ä¢ Trade-off: Kleine accuracy drop voor cleaner model\n",
    "\n",
    "SCENARIO 3 (ONGEWENST):\n",
    "‚Ä¢ Significante performance daling (>2%)\n",
    "‚Ä¢ Conclusie: Verwijderde features waren toch belangrijker dan gedacht\n",
    "‚Ä¢ Actie: Re-evalueer feature selection criteria\n",
    "\"\"\")\n",
    "# ============================================================================\n",
    "# STEP 3A: IDENTIFY FEATURES TO REMOVE\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 3A: FEATURE SELECTION - IDENTIFY FEATURES TO REMOVE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check if recommended_features_to_remove exists from Step 2\n",
    "if 'recommended_features_to_remove' not in locals():\n",
    "    print(\"\\n‚ö†Ô∏è Creating feature removal list (recommended_features_to_remove not found)\")\n",
    "    recommended_features_to_remove = []\n",
    "    \n",
    "    # Always remove Diastolic IF IT EXISTS\n",
    "    if 'Diastolic' in X_train.columns:\n",
    "        recommended_features_to_remove.append('Diastolic')\n",
    "        print(\"  ‚Üí Diastolic found and marked for removal\")\n",
    "    else:\n",
    "        print(\"  ‚ÑπÔ∏è Diastolic not found in dataset (possibly already processed)\")\n",
    "    \n",
    "    # Optionally add low-importance features from RF\n",
    "    if 'rf_importance_df' in locals():\n",
    "        low_imp_features = rf_importance_df[rf_importance_df['Importance_Pct'] < 1.0]['Feature'].tolist()\n",
    "        for feat in low_imp_features:\n",
    "            if feat not in recommended_features_to_remove and feat in X_train.columns:\n",
    "                recommended_features_to_remove.append(feat)\n",
    "else:\n",
    "    # Filter recommended features to only include those that actually exist\n",
    "    print(\"\\n‚úì Using feature removal list from Step 2\")\n",
    "    recommended_features_to_remove = [f for f in recommended_features_to_remove if f in X_train.columns]\n",
    "    \n",
    "    # Check which recommended features don't exist\n",
    "    missing_features = [f for f in locals().get('recommended_features_to_remove', []) \n",
    "                       if f not in X_train.columns]\n",
    "    if missing_features:\n",
    "        print(f\"\\n  ‚ÑπÔ∏è The following recommended features don't exist in dataset: {missing_features}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"FEATURES IDENTIFIED FOR REMOVAL:\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(f\"\\nTotal features to remove: {len(recommended_features_to_remove)}\")\n",
    "\n",
    "if len(recommended_features_to_remove) > 0:\n",
    "    for i, feat in enumerate(recommended_features_to_remove, 1):\n",
    "        # Get importance info if available\n",
    "        if 'feature_importance_comparison' in locals():\n",
    "            feat_data = feature_importance_comparison[feature_importance_comparison['Feature'] == feat]\n",
    "            if len(feat_data) > 0:\n",
    "                rf_imp = feat_data['RF_Importance_%'].values[0]\n",
    "                avg_rank = feat_data['Average_Rank'].values[0]\n",
    "                print(f\"  {i}. {feat}\")\n",
    "                print(f\"     ‚Üí RF Importance: {rf_imp:.3f}%\")\n",
    "                print(f\"     ‚Üí Average Rank: {avg_rank:.1f}\")\n",
    "                \n",
    "                # Rationale\n",
    "                if feat == 'Diastolic':\n",
    "                    print(f\"     ‚Üí Rationale: Extreem hoge correlatie met Systolic (r=0.979)\")\n",
    "                elif rf_imp < 1.0:\n",
    "                    print(f\"     ‚Üí Rationale: Low importance (<1%)\")\n",
    "            else:\n",
    "                print(f\"  {i}. {feat}\")\n",
    "        else:\n",
    "            print(f\"  {i}. {feat}\")\n",
    "else:\n",
    "    print(\"  ‚ÑπÔ∏è No features identified for removal\")\n",
    "    print(\"  ‚Üí Proceeding with all features\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3B: CREATE OPTIMIZED FEATURE SETS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 3B: CREATE OPTIMIZED FEATURE SETS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nCreating optimized feature sets by removing selected features...\")\n",
    "\n",
    "# Store original feature sets for comparison\n",
    "X_train_baseline = X_train.copy()\n",
    "X_test_baseline = X_test.copy()\n",
    "\n",
    "print(f\"\\nBASELINE (Original):\")\n",
    "print(f\"  ‚Ä¢ Train: {X_train_baseline.shape[0]:,} samples √ó {X_train_baseline.shape[1]} features\")\n",
    "print(f\"  ‚Ä¢ Test: {X_test_baseline.shape[0]:,} samples √ó {X_test_baseline.shape[1]} features\")\n",
    "\n",
    "# Remove features\n",
    "if len(recommended_features_to_remove) > 0:\n",
    "    # Create optimized sets\n",
    "    X_train_optimized = X_train_baseline.drop(columns=recommended_features_to_remove)\n",
    "    X_test_optimized = X_test_baseline.drop(columns=recommended_features_to_remove)\n",
    "    \n",
    "    print(f\"\\nOPTIMIZED (After removal):\")\n",
    "    print(f\"  ‚Ä¢ Train: {X_train_optimized.shape[0]:,} samples √ó {X_train_optimized.shape[1]} features\")\n",
    "    print(f\"  ‚Ä¢ Test: {X_test_optimized.shape[0]:,} samples √ó {X_test_optimized.shape[1]} features\")\n",
    "    \n",
    "    features_removed = X_train_baseline.shape[1] - X_train_optimized.shape[1]\n",
    "    print(f\"\\n‚úì Removed {features_removed} features\")\n",
    "    print(f\"‚úì Feature reduction: {X_train_baseline.shape[1]} ‚Üí {X_train_optimized.shape[1]} ({features_removed/X_train_baseline.shape[1]*100:.1f}% reduction)\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No features to remove - using baseline feature set\")\n",
    "    X_train_optimized = X_train_baseline.copy()\n",
    "    X_test_optimized = X_test_baseline.copy()\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3C: RETRAIN MODELS WITH OPTIMIZED FEATURES\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 3C: RETRAIN MODELS WITH OPTIMIZED FEATURE SET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL 1: RANDOM FOREST - OPTIMIZED\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"RETRAINING RANDOM FOREST WITH OPTIMIZED FEATURES\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(\"\\nüîÑ Training Random Forest (Optimized)...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Initialize and train\n",
    "rf_model_optimized = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=None,\n",
    "    min_samples_split=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "rf_model_optimized.fit(X_train_optimized, y_train)\n",
    "\n",
    "training_time_rf_opt = time.time() - start_time\n",
    "print(f\"‚úì Training completed in {training_time_rf_opt:.2f} seconds\")\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_rf_opt = rf_model_optimized.predict(X_train_optimized)\n",
    "y_test_pred_rf_opt = rf_model_optimized.predict(X_test_optimized)\n",
    "\n",
    "# Metrics\n",
    "train_accuracy_rf_opt = accuracy_score(y_train, y_train_pred_rf_opt)\n",
    "test_accuracy_rf_opt = accuracy_score(y_test, y_test_pred_rf_opt)\n",
    "precision_rf_opt = precision_score(y_test, y_test_pred_rf_opt, average='weighted')\n",
    "recall_rf_opt = recall_score(y_test, y_test_pred_rf_opt, average='weighted')\n",
    "f1_rf_opt = f1_score(y_test, y_test_pred_rf_opt, average='weighted')\n",
    "\n",
    "print(f\"\\nPerformance (Optimized):\")\n",
    "print(f\"  ‚Ä¢ Train Accuracy: {train_accuracy_rf_opt:.4f} ({train_accuracy_rf_opt*100:.2f}%)\")\n",
    "print(f\"  ‚Ä¢ Test Accuracy: {test_accuracy_rf_opt:.4f} ({test_accuracy_rf_opt*100:.2f}%)\")\n",
    "print(f\"  ‚Ä¢ Precision: {precision_rf_opt:.4f}\")\n",
    "print(f\"  ‚Ä¢ Recall: {recall_rf_opt:.4f}\")\n",
    "print(f\"  ‚Ä¢ F1-Score: {f1_rf_opt:.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL 2: LOGISTIC REGRESSION - OPTIMIZED\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"RETRAINING LOGISTIC REGRESSION WITH OPTIMIZED FEATURES\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(\"\\nüîÑ Training Logistic Regression (Optimized)...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Initialize and train\n",
    "lr_model_optimized = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    multi_class='multinomial',\n",
    "    solver='lbfgs',\n",
    "    random_state=42,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "lr_model_optimized.fit(X_train_optimized, y_train)\n",
    "\n",
    "training_time_lr_opt = time.time() - start_time\n",
    "print(f\"‚úì Training completed in {training_time_lr_opt:.2f} seconds\")\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_lr_opt = lr_model_optimized.predict(X_train_optimized)\n",
    "y_test_pred_lr_opt = lr_model_optimized.predict(X_test_optimized)\n",
    "\n",
    "# Metrics\n",
    "train_accuracy_lr_opt = accuracy_score(y_train, y_train_pred_lr_opt)\n",
    "test_accuracy_lr_opt = accuracy_score(y_test, y_test_pred_lr_opt)\n",
    "precision_lr_opt = precision_score(y_test, y_test_pred_lr_opt, average='weighted')\n",
    "recall_lr_opt = recall_score(y_test, y_test_pred_lr_opt, average='weighted')\n",
    "f1_lr_opt = f1_score(y_test, y_test_pred_lr_opt, average='weighted')\n",
    "\n",
    "print(f\"\\nPerformance (Optimized):\")\n",
    "print(f\"  ‚Ä¢ Train Accuracy: {train_accuracy_lr_opt:.4f} ({train_accuracy_lr_opt*100:.2f}%)\")\n",
    "print(f\"  ‚Ä¢ Test Accuracy: {test_accuracy_lr_opt:.4f} ({test_accuracy_lr_opt*100:.2f}%)\")\n",
    "print(f\"  ‚Ä¢ Precision: {precision_lr_opt:.4f}\")\n",
    "print(f\"  ‚Ä¢ Recall: {recall_lr_opt:.4f}\")\n",
    "print(f\"  ‚Ä¢ F1-Score: {f1_lr_opt:.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3D: BASELINE VS OPTIMIZED COMPARISON\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 3D: BASELINE VS OPTIMIZED COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create comprehensive comparison\n",
    "comparison_data = {\n",
    "    'Model': [\n",
    "        'Random Forest (Baseline)',\n",
    "        'Random Forest (Optimized)',\n",
    "        'Logistic Regression (Baseline)',\n",
    "        'Logistic Regression (Optimized)'\n",
    "    ],\n",
    "    'Features': [\n",
    "        X_train_baseline.shape[1],\n",
    "        X_train_optimized.shape[1],\n",
    "        X_train_baseline.shape[1],\n",
    "        X_train_optimized.shape[1]\n",
    "    ],\n",
    "    'Train Accuracy': [\n",
    "        train_accuracy_rf,\n",
    "        train_accuracy_rf_opt,\n",
    "        train_accuracy_lr,\n",
    "        train_accuracy_lr_opt\n",
    "    ],\n",
    "    'Test Accuracy': [\n",
    "        test_accuracy_rf,\n",
    "        test_accuracy_rf_opt,\n",
    "        test_accuracy_lr,\n",
    "        test_accuracy_lr_opt\n",
    "    ],\n",
    "    'Precision': [\n",
    "        precision_rf,\n",
    "        precision_rf_opt,\n",
    "        precision_lr,\n",
    "        precision_lr_opt\n",
    "    ],\n",
    "    'Recall': [\n",
    "        recall_rf,\n",
    "        recall_rf_opt,\n",
    "        recall_lr,\n",
    "        recall_lr_opt\n",
    "    ],\n",
    "    'F1-Score': [\n",
    "        f1_rf,\n",
    "        f1_rf_opt,\n",
    "        f1_lr,\n",
    "        f1_lr_opt\n",
    "    ],\n",
    "    'Training Time (s)': [\n",
    "        training_time_rf,\n",
    "        training_time_rf_opt,\n",
    "        training_time_lr,\n",
    "        training_time_lr_opt\n",
    "    ]\n",
    "}\n",
    "\n",
    "comparison_results = pd.DataFrame(comparison_data)\n",
    "comparison_results = comparison_results.round(4)\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"COMPREHENSIVE PERFORMANCE COMPARISON\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "display(comparison_results)\n",
    "\n",
    "# Calculate improvements\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"PERFORMANCE CHANGES (Baseline ‚Üí Optimized)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Random Forest\n",
    "rf_acc_change = (test_accuracy_rf_opt - test_accuracy_rf) * 100\n",
    "rf_f1_change = (f1_rf_opt - f1_rf) * 100\n",
    "print(f\"\\nRANDOM FOREST:\")\n",
    "print(f\"  ‚Ä¢ Test Accuracy: {test_accuracy_rf*100:.2f}% ‚Üí {test_accuracy_rf_opt*100:.2f}% ({rf_acc_change:+.2f}%)\")\n",
    "print(f\"  ‚Ä¢ F1-Score: {f1_rf:.4f} ‚Üí {f1_rf_opt:.4f} ({rf_f1_change:+.2f}%)\")\n",
    "print(f\"  ‚Ä¢ Training Time: {training_time_rf:.2f}s ‚Üí {training_time_rf_opt:.2f}s\")\n",
    "\n",
    "if abs(rf_acc_change) < 0.5:\n",
    "    print(f\"  ‚úì Performance MAINTAINED (change < 0.5%)\")\n",
    "elif rf_acc_change > 0:\n",
    "    print(f\"  ‚úì Performance IMPROVED (+{rf_acc_change:.2f}%)\")\n",
    "else:\n",
    "    print(f\"  ‚ö†Ô∏è Performance DECREASED ({rf_acc_change:.2f}%)\")\n",
    "\n",
    "# Logistic Regression\n",
    "lr_acc_change = (test_accuracy_lr_opt - test_accuracy_lr) * 100\n",
    "lr_f1_change = (f1_lr_opt - f1_lr) * 100\n",
    "print(f\"\\nLOGISTIC REGRESSION:\")\n",
    "print(f\"  ‚Ä¢ Test Accuracy: {test_accuracy_lr*100:.2f}% ‚Üí {test_accuracy_lr_opt*100:.2f}% ({lr_acc_change:+.2f}%)\")\n",
    "print(f\"  ‚Ä¢ F1-Score: {f1_lr:.4f} ‚Üí {f1_lr_opt:.4f} ({lr_f1_change:+.2f}%)\")\n",
    "print(f\"  ‚Ä¢ Training Time: {training_time_lr:.2f}s ‚Üí {training_time_lr_opt:.2f}s\")\n",
    "\n",
    "if abs(lr_acc_change) < 0.5:\n",
    "    print(f\"  ‚úì Performance MAINTAINED (change < 0.5%)\")\n",
    "elif lr_acc_change > 0:\n",
    "    print(f\"  ‚úì Performance IMPROVED (+{lr_acc_change:.2f}%)\")\n",
    "else:\n",
    "    print(f\"  ‚ö†Ô∏è Performance DECREASED ({lr_acc_change:.2f}%)\")\n",
    "\n",
    "# ============================================================================\n",
    "# VISUALIZATION: BASELINE VS OPTIMIZED\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"VISUALIZATION: BASELINE VS OPTIMIZED COMPARISON\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Plot 1: Accuracy Comparison\n",
    "models = ['RF\\nBaseline', 'RF\\nOptimized', 'LR\\nBaseline', 'LR\\nOptimized']\n",
    "train_accs = [train_accuracy_rf, train_accuracy_rf_opt, train_accuracy_lr, train_accuracy_lr_opt]\n",
    "test_accs = [test_accuracy_rf, test_accuracy_rf_opt, test_accuracy_lr, test_accuracy_lr_opt]\n",
    "\n",
    "x = np.arange(len(models))\n",
    "width = 0.35\n",
    "\n",
    "axes[0, 0].bar(x - width/2, train_accs, width, label='Train Accuracy', color='lightblue')\n",
    "axes[0, 0].bar(x + width/2, test_accs, width, label='Test Accuracy', color='steelblue')\n",
    "axes[0, 0].set_ylabel('Accuracy')\n",
    "axes[0, 0].set_title('Accuracy Comparison: Baseline vs Optimized', fontweight='bold')\n",
    "axes[0, 0].set_xticks(x)\n",
    "axes[0, 0].set_xticklabels(models)\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].set_ylim([0.7, 1.0])\n",
    "axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, (train, test) in enumerate(zip(train_accs, test_accs)):\n",
    "    axes[0, 0].text(i - width/2, train + 0.01, f'{train:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "    axes[0, 0].text(i + width/2, test + 0.01, f'{test:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "# Plot 2: F1-Score Comparison\n",
    "f1_scores = [f1_rf, f1_rf_opt, f1_lr, f1_lr_opt]\n",
    "colors = ['steelblue', 'lightblue', 'seagreen', 'lightgreen']\n",
    "\n",
    "axes[0, 1].bar(models, f1_scores, color=colors)\n",
    "axes[0, 1].set_ylabel('F1-Score (weighted)')\n",
    "axes[0, 1].set_title('F1-Score Comparison: Baseline vs Optimized', fontweight='bold')\n",
    "axes[0, 1].set_ylim([0.7, 1.0])\n",
    "axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, score in enumerate(f1_scores):\n",
    "    axes[0, 1].text(i, score + 0.01, f'{score:.4f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Plot 3: Accuracy Change\n",
    "acc_changes = [rf_acc_change, lr_acc_change]\n",
    "model_names = ['Random Forest', 'Logistic Regression']\n",
    "colors_change = ['green' if x >= 0 else 'red' for x in acc_changes]\n",
    "\n",
    "axes[1, 0].bar(model_names, acc_changes, color=colors_change, alpha=0.7)\n",
    "axes[1, 0].axhline(y=0, color='black', linestyle='-', linewidth=0.8)\n",
    "axes[1, 0].set_ylabel('Accuracy Change (%)')\n",
    "axes[1, 0].set_title('Performance Change: Baseline ‚Üí Optimized', fontweight='bold')\n",
    "axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, change in enumerate(acc_changes):\n",
    "    axes[1, 0].text(i, change + 0.1 if change >= 0 else change - 0.1, \n",
    "                    f'{change:+.2f}%', ha='center', va='bottom' if change >= 0 else 'top', \n",
    "                    fontsize=10, fontweight='bold')\n",
    "\n",
    "# Plot 4: Feature Count & Training Time\n",
    "x = np.arange(len(model_names))\n",
    "width = 0.35\n",
    "\n",
    "# Feature count (normalized)\n",
    "feature_reduction = [(X_train_baseline.shape[1] - X_train_optimized.shape[1]) / X_train_baseline.shape[1] * 100] * 2\n",
    "\n",
    "ax4_twin = axes[1, 1].twinx()\n",
    "\n",
    "bars1 = axes[1, 1].bar(x - width/2, [training_time_rf, training_time_lr], width, \n",
    "                       label='Baseline Time', color='lightcoral')\n",
    "bars2 = axes[1, 1].bar(x + width/2, [training_time_rf_opt, training_time_lr_opt], width,\n",
    "                       label='Optimized Time', color='coral')\n",
    "\n",
    "axes[1, 1].set_ylabel('Training Time (seconds)', color='coral')\n",
    "axes[1, 1].set_xlabel('Model')\n",
    "axes[1, 1].set_title('Training Time & Feature Reduction', fontweight='bold')\n",
    "axes[1, 1].set_xticks(x)\n",
    "axes[1, 1].set_xticklabels(model_names)\n",
    "axes[1, 1].tick_params(axis='y', labelcolor='coral')\n",
    "axes[1, 1].legend(loc='upper left')\n",
    "\n",
    "# Feature reduction on secondary axis\n",
    "ax4_twin.plot(x, feature_reduction, color='steelblue', marker='o', linewidth=2, markersize=10, label='Feature Reduction %')\n",
    "ax4_twin.set_ylabel('Feature Reduction (%)', color='steelblue')\n",
    "ax4_twin.tick_params(axis='y', labelcolor='steelblue')\n",
    "ax4_twin.legend(loc='upper right')\n",
    "ax4_twin.set_ylim([0, max(feature_reduction) * 1.5])\n",
    "\n",
    "plt.suptitle('Baseline vs Optimized Models - Comprehensive Comparison', \n",
    "             fontsize=16, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# CONFUSION MATRICES: OPTIMIZED MODELS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"CONFUSION MATRICES: OPTIMIZED MODELS\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Random Forest - Optimized\n",
    "cm_rf_opt = confusion_matrix(y_test, y_test_pred_rf_opt)\n",
    "sns.heatmap(cm_rf_opt, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Insomnia', 'None', 'Sleep Apnea'],\n",
    "            yticklabels=['Insomnia', 'None', 'Sleep Apnea'],\n",
    "            ax=axes[0])\n",
    "axes[0].set_title(f'Random Forest (Optimized)\\nTest Accuracy: {test_accuracy_rf_opt*100:.2f}%', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Actual Class')\n",
    "axes[0].set_xlabel('Predicted Class')\n",
    "\n",
    "# Logistic Regression - Optimized\n",
    "cm_lr_opt = confusion_matrix(y_test, y_test_pred_lr_opt)\n",
    "sns.heatmap(cm_lr_opt, annot=True, fmt='d', cmap='Greens', \n",
    "            xticklabels=['Insomnia', 'None', 'Sleep Apnea'],\n",
    "            yticklabels=['Insomnia', 'None', 'Sleep Apnea'],\n",
    "            ax=axes[1])\n",
    "axes[1].set_title(f'Logistic Regression (Optimized)\\nTest Accuracy: {test_accuracy_lr_opt*100:.2f}%', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Actual Class')\n",
    "axes[1].set_xlabel('Predicted Class')\n",
    "\n",
    "plt.suptitle('Confusion Matrices - Optimized Models (Test Set)', \n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# FINAL EVALUATION & RECOMMENDATIONS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL EVALUATION & RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\"\"\n",
    "FEATURE SELECTION IMPACT SUMMARY:\n",
    "==================================\n",
    "\n",
    "FEATURE REDUCTION:\n",
    "‚Ä¢ Features removed: {X_train_baseline.shape[1] - X_train_optimized.shape[1]}\n",
    "‚Ä¢ Original: {X_train_baseline.shape[1]} features\n",
    "‚Ä¢ Optimized: {X_train_optimized.shape[1]} features\n",
    "‚Ä¢ Reduction: {(X_train_baseline.shape[1] - X_train_optimized.shape[1])/X_train_baseline.shape[1]*100:.1f}%\n",
    "\n",
    "RANDOM FOREST:\n",
    "‚Ä¢ Accuracy change: {rf_acc_change:+.2f}%\n",
    "‚Ä¢ F1-Score change: {rf_f1_change:+.2f}%\n",
    "‚Ä¢ Conclusion: {\"‚úì Feature selection successful - performance maintained/improved\" if abs(rf_acc_change) < 1.0 else \"‚ö†Ô∏è Performance impacted - re-evaluate feature selection\"}\n",
    "\n",
    "LOGISTIC REGRESSION:\n",
    "‚Ä¢ Accuracy change: {lr_acc_change:+.2f}%\n",
    "‚Ä¢ F1-Score change: {lr_f1_change:+.2f}%\n",
    "‚Ä¢ Conclusion: {\"‚úì Feature selection successful - multicollineariteit reduced\" if lr_acc_change >= -0.5 else \"‚ö†Ô∏è Performance impacted significantly\"}\n",
    "\"\"\")\n",
    "\n",
    "# Determine best overall model\n",
    "best_model_baseline = 'Random Forest' if test_accuracy_rf > test_accuracy_lr else 'Logistic Regression'\n",
    "best_acc_baseline = max(test_accuracy_rf, test_accuracy_lr)\n",
    "\n",
    "best_model_optimized = 'Random Forest' if test_accuracy_rf_opt > test_accuracy_lr_opt else 'Logistic Regression'\n",
    "best_acc_optimized = max(test_accuracy_rf_opt, test_accuracy_lr_opt)\n",
    "\n",
    "print(f\"\\nBEST MODEL IDENTIFICATION:\")\n",
    "print(f\"  ‚Ä¢ Baseline: {best_model_baseline} ({best_acc_baseline*100:.2f}%)\")\n",
    "print(f\"  ‚Ä¢ Optimized: {best_model_optimized} ({best_acc_optimized*100:.2f}%)\")\n",
    "\n",
    "if best_acc_optimized >= best_acc_baseline:\n",
    "    print(f\"\\nüèÜ RECOMMENDATION: Use OPTIMIZED {best_model_optimized}\")\n",
    "    print(f\"   ‚Ä¢ Accuracy: {best_acc_optimized*100:.2f}%\")\n",
    "    print(f\"   ‚Ä¢ Features: {X_train_optimized.shape[1]} (simpler model)\")\n",
    "    print(f\"   ‚Ä¢ Benefits: Cleaner, faster, more interpretable\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è RECOMMENDATION: Consider using BASELINE {best_model_baseline}\")\n",
    "    print(f\"   ‚Ä¢ Accuracy: {best_acc_baseline*100:.2f}%\")\n",
    "    print(f\"   ‚Ä¢ Trade-off: More features but better performance\")\n",
    "\n",
    "print(f\"\"\"\n",
    "NEXT STEPS:\n",
    "===========\n",
    "\n",
    "1. ‚úì FEATURE SELECTION COMPLETED\n",
    "   ‚Ä¢ Removed {X_train_baseline.shape[1] - X_train_optimized.shape[1]} features\n",
    "   ‚Ä¢ Performance impact evaluated\n",
    "\n",
    "2. ‚Üí HYPERPARAMETER TUNING (Stap 4 - Optioneel)\n",
    "   ‚Ä¢ Grid Search / Random Search\n",
    "   ‚Ä¢ Cross-validation\n",
    "   ‚Ä¢ Optimize best-performing model\n",
    "\n",
    "3. ‚Üí FINAL MODEL EVALUATION (Stap 5)\n",
    "   ‚Ä¢ Detailed performance analysis\n",
    "   ‚Ä¢ Per-class metrics\n",
    "   ‚Ä¢ Error analysis\n",
    "   ‚Ä¢ Feature importance in final model\n",
    "\n",
    "4. ‚Üí MODEL INTERPRETATION & DEPLOYMENT (Stap 6)\n",
    "   ‚Ä¢ Business insights\n",
    "   ‚Ä¢ Deployment considerations\n",
    "   ‚Ä¢ Monitoring strategy\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úì STAP 3 VOLTOOID - FEATURE SELECTION & RETRAINING COMPLEET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save optimized models and data for next steps\n",
    "print(\"\\n‚úì Results saved for next steps:\")\n",
    "print(\"  ‚Ä¢ rf_model_optimized (trained model)\")\n",
    "print(\"  ‚Ä¢ lr_model_optimized (trained model)\")\n",
    "print(\"  ‚Ä¢ X_train_optimized, X_test_optimized (feature sets)\")\n",
    "print(\"  ‚Ä¢ comparison_results (performance dataframe)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d5bcfcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STAP 4: MODEL RETRAINING MET OPTIMIZED FEATURES\n",
      "================================================================================\n",
      "\n",
      "DOEL VAN DEZE STAP:\n",
      "===================\n",
      "\n",
      "Nu we in Stap 3 features hebben verwijderd (vooral Diastolic vanwege extreme\n",
      "multicollineariteit met Systolic), gaan we beide modellen OPNIEUW trainen met\n",
      "een UNIFIED feature set.\n",
      "\n",
      "BELANGRIJKSTE WIJZIGING T.O.V. BASELINE:\n",
      "=========================================\n",
      "\n",
      "BASELINE (Stap 1):\n",
      "‚Ä¢ Random Forest: 23 features (inclusief Diastolic)\n",
      "‚Ä¢ Logistic Regression: 22 features (Diastolic al verwijderd)\n",
      "‚Üí Inconsistent: modellen gebruikten verschillende feature sets!\n",
      "\n",
      "OPTIMIZED (Stap 4):\n",
      "‚Ä¢ Random Forest: 22 features (Diastolic verwijderd)\n",
      "‚Ä¢ Logistic Regression: 22 features (Diastolic verwijderd)\n",
      "‚Üí Consistent: BEIDE modellen gebruiken DEZELFDE features!\n",
      "\n",
      "WAAROM DIT BELANGRIJK IS:\n",
      "==========================\n",
      "\n",
      "1. EERLIJKE VERGELIJKING:\n",
      "   ‚Ä¢ Nu kunnen we RF en LR direct vergelijken\n",
      "   ‚Ä¢ Beide modellen hebben toegang tot dezelfde informatie\n",
      "\n",
      "2. MULTICOLLINEARITEIT OPGELOST:\n",
      "   ‚Ä¢ Systolic ‚Üî Diastolic (r=0.979) probleem verholpen\n",
      "   ‚Ä¢ LR kan nu stabiel trainen zonder numerieke instabiliteit\n",
      "\n",
      "3. CLEANER MODEL:\n",
      "   ‚Ä¢ Minder redundante informatie\n",
      "   ‚Ä¢ Sneller training en inference\n",
      "   ‚Ä¢ Beter interpreteerbaar\n",
      "\n",
      "VERWACHTING:\n",
      "============\n",
      "\n",
      "‚Ä¢ Random Forest: Minimale performance wijziging (robuust tegen redundantie)\n",
      "‚Ä¢ Logistic Regression: Mogelijk BETERE performance (minder multicollineariteit)\n",
      "‚Ä¢ Beide: Snellere training door minder features\n",
      "\n",
      "\n",
      "================================================================================\n",
      "STAP 4A: VERIFY DATA STATUS & CREATE UNIFIED FEATURE SET\n",
      "================================================================================\n",
      "\n",
      "CURRENT DATA STATUS:\n",
      "  ‚Ä¢ X_train_baseline: (4000, 23)\n",
      "  ‚Ä¢ X_test_baseline: (1000, 23)\n",
      "\n",
      "‚úì Diastolic found in baseline data - will be removed\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "FEATURES TO REMOVE:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Total features to remove: 1\n",
      "  1. Diastolic\n",
      "     ‚Üí Rationale: Extreme multicollineariteit met Systolic (r=0.979)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "CREATING UNIFIED OPTIMIZED FEATURE SET:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "‚úì Features removed: 1\n",
      "‚úì Baseline features: 23\n",
      "‚úì Unified features: 22\n",
      "\n",
      "FINAL UNIFIED FEATURE SET:\n",
      "  ‚Ä¢ Train: 4,000 samples √ó 22 features\n",
      "  ‚Ä¢ Test: 1,000 samples √ó 22 features\n",
      "\n",
      "üìã Feature List (22 features):\n",
      "   1. Age\n",
      "   2. Sleep Duration\n",
      "   3. Quality of Sleep\n",
      "   4. Physical Activity Level\n",
      "   5. Stress Level\n",
      "   6. Heart Rate\n",
      "   7. Daily Steps\n",
      "   8. Systolic\n",
      "   9. Gender_Male\n",
      "  10. Occupation_Doctor\n",
      "  11. Occupation_Engineer\n",
      "  12. Occupation_Lawyer\n",
      "  13. Occupation_Manager\n",
      "  14. Occupation_Nurse\n",
      "  15. Occupation_Sales Representative\n",
      "  ... and 7 more features\n",
      "\n",
      "================================================================================\n",
      "STAP 4B: TRAIN RANDOM FOREST WITH UNIFIED FEATURES\n",
      "================================================================================\n",
      "\n",
      "Model Configuration:\n",
      "  ‚Ä¢ Algorithm: Random Forest Classifier\n",
      "  ‚Ä¢ Features: 22 (unified set)\n",
      "  ‚Ä¢ n_estimators: 100\n",
      "  ‚Ä¢ max_depth: None (full trees)\n",
      "  ‚Ä¢ random_state: 42\n",
      "\n",
      "üîÑ Training Random Forest (Unified)...\n",
      "‚úì Training completed in 0.08 seconds\n",
      "\n",
      "üîÑ Making predictions...\n",
      "\n",
      "üìä RANDOM FOREST (UNIFIED) - PERFORMANCE:\n",
      "  ‚Ä¢ Train Accuracy: 1.0000 (100.00%)\n",
      "  ‚Ä¢ Test Accuracy: 0.7390 (73.90%)\n",
      "  ‚Ä¢ Precision: 0.7383\n",
      "  ‚Ä¢ Recall: 0.7390\n",
      "  ‚Ä¢ F1-Score: 0.7380\n",
      "\n",
      "  ‚Ä¢ Overfitting gap: 0.2610 (26.10%)\n",
      "  ‚ö†Ô∏è Significant overfitting\n",
      "\n",
      "================================================================================\n",
      "STAP 4C: TRAIN LOGISTIC REGRESSION WITH UNIFIED FEATURES\n",
      "================================================================================\n",
      "\n",
      "Model Configuration:\n",
      "  ‚Ä¢ Algorithm: Logistic Regression\n",
      "  ‚Ä¢ Features: 22 (unified set - SAME as RF!)\n",
      "  ‚Ä¢ multi_class: multinomial\n",
      "  ‚Ä¢ solver: lbfgs\n",
      "  ‚Ä¢ max_iter: 1000\n",
      "  ‚Ä¢ random_state: 42\n",
      "\n",
      "üîÑ Training Logistic Regression (Unified)...\n",
      "‚úì Training completed in 0.01 seconds\n",
      "\n",
      "üîÑ Making predictions...\n",
      "\n",
      "üìä LOGISTIC REGRESSION (UNIFIED) - PERFORMANCE:\n",
      "  ‚Ä¢ Train Accuracy: 0.7390 (73.90%)\n",
      "  ‚Ä¢ Test Accuracy: 0.7390 (73.90%)\n",
      "  ‚Ä¢ Precision: 0.7391\n",
      "  ‚Ä¢ Recall: 0.7390\n",
      "  ‚Ä¢ F1-Score: 0.7390\n",
      "\n",
      "  ‚Ä¢ Overfitting gap: 0.0000 (0.00%)\n",
      "  ‚úì Minimal overfitting\n",
      "\n",
      "  ‚Ä¢ Unique predictions: 3/3 classes\n",
      "  ‚úì Model predicts all classes (working correctly)\n",
      "\n",
      "================================================================================\n",
      "STAP 4D: COMPREHENSIVE COMPARISON - BASELINE vs UNIFIED\n",
      "================================================================================\n",
      "\n",
      "üìä PERFORMANCE COMPARISON TABLE:\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Features</th>\n",
       "      <th>Train Acc</th>\n",
       "      <th>Test Acc</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>Training Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RF - Baseline (Step 1)</td>\n",
       "      <td>23</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.741</td>\n",
       "      <td>0.7410</td>\n",
       "      <td>0.741</td>\n",
       "      <td>0.7406</td>\n",
       "      <td>0.1071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RF - Unified (Step 4)</td>\n",
       "      <td>22</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.739</td>\n",
       "      <td>0.7383</td>\n",
       "      <td>0.739</td>\n",
       "      <td>0.7380</td>\n",
       "      <td>0.0787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LR - Baseline (Step 1)</td>\n",
       "      <td>22</td>\n",
       "      <td>0.739</td>\n",
       "      <td>0.739</td>\n",
       "      <td>0.7391</td>\n",
       "      <td>0.739</td>\n",
       "      <td>0.7390</td>\n",
       "      <td>0.0093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LR - Unified (Step 4)</td>\n",
       "      <td>22</td>\n",
       "      <td>0.739</td>\n",
       "      <td>0.739</td>\n",
       "      <td>0.7391</td>\n",
       "      <td>0.739</td>\n",
       "      <td>0.7390</td>\n",
       "      <td>0.0069</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Model  Features  Train Acc  Test Acc  Precision  Recall  \\\n",
       "0  RF - Baseline (Step 1)        23      1.000     0.741     0.7410   0.741   \n",
       "1   RF - Unified (Step 4)        22      1.000     0.739     0.7383   0.739   \n",
       "2  LR - Baseline (Step 1)        22      0.739     0.739     0.7391   0.739   \n",
       "3   LR - Unified (Step 4)        22      0.739     0.739     0.7391   0.739   \n",
       "\n",
       "   F1-Score  Training Time  \n",
       "0    0.7406         0.1071  \n",
       "1    0.7380         0.0787  \n",
       "2    0.7390         0.0093  \n",
       "3    0.7390         0.0069  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "PERFORMANCE CHANGES (Baseline ‚Üí Unified)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "RANDOM FOREST:\n",
      "  ‚Ä¢ Accuracy: 74.10% ‚Üí 73.90% (-0.20%)\n",
      "  ‚Ä¢ Features: 23 ‚Üí 22\n",
      "  ‚úì Performance STABLE (minimal change)\n",
      "\n",
      "LOGISTIC REGRESSION:\n",
      "  ‚Ä¢ Accuracy: 73.90% ‚Üí 73.90% (+0.00%)\n",
      "  ‚Ä¢ Features: Both using 22 (NOW CONSISTENT!)\n",
      "  ‚úì Performance STABLE\n",
      "\n",
      "================================================================================\n",
      "STAP 4E: DETAILED CLASSIFICATION REPORTS (UNIFIED MODELS)\n",
      "================================================================================\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "RANDOM FOREST (UNIFIED) - CLASSIFICATION REPORT\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Classification Report (Test Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Insomnia     0.6533    0.5882    0.6190       221\n",
      "        None     0.8018    0.8074    0.8046       566\n",
      " Sleep Apnea     0.6580    0.7136    0.6847       213\n",
      "\n",
      "    accuracy                         0.7390      1000\n",
      "   macro avg     0.7043    0.7031    0.7028      1000\n",
      "weighted avg     0.7383    0.7390    0.7380      1000\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "LOGISTIC REGRESSION (UNIFIED) - CLASSIFICATION REPORT\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Classification Report (Test Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Insomnia     0.6330    0.6244    0.6287       221\n",
      "        None     0.8050    0.8021    0.8035       566\n",
      " Sleep Apnea     0.6743    0.6901    0.6821       213\n",
      "\n",
      "    accuracy                         0.7390      1000\n",
      "   macro avg     0.7041    0.7056    0.7048      1000\n",
      "weighted avg     0.7391    0.7390    0.7390      1000\n",
      "\n",
      "\n",
      "================================================================================\n",
      "STAP 4F: CONFUSION MATRICES (UNIFIED MODELS)\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x600 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STAP 4G: PERFORMANCE COMPARISON VISUALIZATION\n",
      "================================================================================\n",
      "\n",
      "Extracting per-class metrics for Random Forest...\n",
      "Extracting per-class metrics for Logistic Regression...\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x1200 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STAP 4H: FINAL SUMMARY & RECOMMENDATIONS\n",
      "================================================================================\n",
      "\n",
      "UNIFIED FEATURE SET RESULTS:\n",
      "============================\n",
      "\n",
      "FEATURE REDUCTION:\n",
      "‚Ä¢ Baseline: 23 features\n",
      "‚Ä¢ Unified: 22 features\n",
      "‚Ä¢ Removed: 1 features (4.3% reduction)\n",
      "‚Ä¢ Removed features: Diastolic\n",
      "\n",
      "RANDOM FOREST (UNIFIED):\n",
      "‚Ä¢ Test Accuracy: 73.90%\n",
      "‚Ä¢ F1-Score: 0.7380\n",
      "‚Ä¢ Training Time: 0.08s\n",
      "‚Ä¢ Status: ‚úì Stable performance with fewer features\n",
      "\n",
      "LOGISTIC REGRESSION (UNIFIED):\n",
      "‚Ä¢ Test Accuracy: 73.90%\n",
      "‚Ä¢ F1-Score: 0.7390\n",
      "‚Ä¢ Training Time: 0.01s\n",
      "‚Ä¢ Status: ‚úì Improved stability (multicollineariteit reduced)\n",
      "\n",
      "KEY IMPROVEMENTS:\n",
      "=================\n",
      "\n",
      "1. ‚úì CONSISTENCY: Both models now use SAME 22 features\n",
      "2. ‚úì MULTICOLLINEARITEIT: Diastolic removed (r=0.979 with Systolic)\n",
      "3. ‚úì STABILITY: Logistic Regression trains without numerical issues\n",
      "4. ‚úì EFFICIENCY: Faster training with fewer features\n",
      "5. ‚úì INTERPRETABILITY: Cleaner model without redundant features\n",
      "\n",
      "\n",
      "üèÜ BEST PERFORMING MODEL:\n",
      "  ‚Ä¢ Model: Logistic Regression\n",
      "  ‚Ä¢ Test Accuracy: 73.90%\n",
      "  ‚Ä¢ F1-Score: 0.7390\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# UITWERKING ML VRAAGSTUK\n",
    "# STAP 4: MODEL RETRAINING MET OPTIMIZED FEATURES (22 FEATURES)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STAP 4: MODEL RETRAINING MET OPTIMIZED FEATURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "DOEL VAN DEZE STAP:\n",
    "===================\n",
    "\n",
    "Nu we in Stap 3 features hebben verwijderd (vooral Diastolic vanwege extreme\n",
    "multicollineariteit met Systolic), gaan we beide modellen OPNIEUW trainen met\n",
    "een UNIFIED feature set.\n",
    "\n",
    "BELANGRIJKSTE WIJZIGING T.O.V. BASELINE:\n",
    "=========================================\n",
    "\n",
    "BASELINE (Stap 1):\n",
    "‚Ä¢ Random Forest: 23 features (inclusief Diastolic)\n",
    "‚Ä¢ Logistic Regression: 22 features (Diastolic al verwijderd)\n",
    "‚Üí Inconsistent: modellen gebruikten verschillende feature sets!\n",
    "\n",
    "OPTIMIZED (Stap 4):\n",
    "‚Ä¢ Random Forest: 22 features (Diastolic verwijderd)\n",
    "‚Ä¢ Logistic Regression: 22 features (Diastolic verwijderd)\n",
    "‚Üí Consistent: BEIDE modellen gebruiken DEZELFDE features!\n",
    "\n",
    "WAAROM DIT BELANGRIJK IS:\n",
    "==========================\n",
    "\n",
    "1. EERLIJKE VERGELIJKING:\n",
    "   ‚Ä¢ Nu kunnen we RF en LR direct vergelijken\n",
    "   ‚Ä¢ Beide modellen hebben toegang tot dezelfde informatie\n",
    "   \n",
    "2. MULTICOLLINEARITEIT OPGELOST:\n",
    "   ‚Ä¢ Systolic ‚Üî Diastolic (r=0.979) probleem verholpen\n",
    "   ‚Ä¢ LR kan nu stabiel trainen zonder numerieke instabiliteit\n",
    "   \n",
    "3. CLEANER MODEL:\n",
    "   ‚Ä¢ Minder redundante informatie\n",
    "   ‚Ä¢ Sneller training en inference\n",
    "   ‚Ä¢ Beter interpreteerbaar\n",
    "\n",
    "VERWACHTING:\n",
    "============\n",
    "\n",
    "‚Ä¢ Random Forest: Minimale performance wijziging (robuust tegen redundantie)\n",
    "‚Ä¢ Logistic Regression: Mogelijk BETERE performance (minder multicollineariteit)\n",
    "‚Ä¢ Beide: Snellere training door minder features\n",
    "\"\"\")\n",
    "\n",
    "# ============================================================================\n",
    "# STAP 4A: VERIFY DATA STATUS & CREATE UNIFIED FEATURE SET\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STAP 4A: VERIFY DATA STATUS & CREATE UNIFIED FEATURE SET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check if we have the baseline data\n",
    "if 'X_train_baseline' not in locals() or 'X_test_baseline' not in locals():\n",
    "    print(\"\\n‚ö†Ô∏è Baseline data niet gevonden - we gebruiken de huidige X_train en X_test\")\n",
    "    X_train_baseline = X_train.copy()\n",
    "    X_test_baseline = X_test.copy()\n",
    "\n",
    "print(f\"\\nCURRENT DATA STATUS:\")\n",
    "print(f\"  ‚Ä¢ X_train_baseline: {X_train_baseline.shape}\")\n",
    "print(f\"  ‚Ä¢ X_test_baseline: {X_test_baseline.shape}\")\n",
    "\n",
    "# Identify features to remove\n",
    "features_to_remove_unified = []\n",
    "\n",
    "# Always remove Diastolic if it exists\n",
    "if 'Diastolic' in X_train_baseline.columns:\n",
    "    features_to_remove_unified.append('Diastolic')\n",
    "    print(f\"\\n‚úì Diastolic found in baseline data - will be removed\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Diastolic not found - possibly already removed in previous steps\")\n",
    "\n",
    "# Check for other low-importance features if we have the analysis\n",
    "if 'rf_importance_df' in locals():\n",
    "    # Get features with <1% importance (excluding Diastolic which we already added)\n",
    "    low_imp_features = rf_importance_df[\n",
    "        (rf_importance_df['Importance_Pct'] < 1.0) & \n",
    "        (rf_importance_df['Feature'] != 'Diastolic')\n",
    "    ]['Feature'].tolist()\n",
    "    \n",
    "    if len(low_imp_features) > 0:\n",
    "        print(f\"\\nüìä Additional low-importance features found (<1%):\")\n",
    "        for feat in low_imp_features:\n",
    "            if feat in X_train_baseline.columns:\n",
    "                imp_pct = rf_importance_df[rf_importance_df['Feature'] == feat]['Importance_Pct'].values[0]\n",
    "                print(f\"  ‚Ä¢ {feat}: {imp_pct:.3f}%\")\n",
    "                \n",
    "                # Ask if we should remove (for now, we keep them to be conservative)\n",
    "                # features_to_remove_unified.append(feat)\n",
    "\n",
    "print(f\"\\n\" + \"-\"*80)\n",
    "print(\"FEATURES TO REMOVE:\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(f\"\\nTotal features to remove: {len(features_to_remove_unified)}\")\n",
    "for i, feat in enumerate(features_to_remove_unified, 1):\n",
    "    print(f\"  {i}. {feat}\")\n",
    "    if feat == 'Diastolic':\n",
    "        print(f\"     ‚Üí Rationale: Extreme multicollineariteit met Systolic (r=0.979)\")\n",
    "\n",
    "# Create optimized feature sets\n",
    "print(f\"\\n\" + \"-\"*80)\n",
    "print(\"CREATING UNIFIED OPTIMIZED FEATURE SET:\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "if len(features_to_remove_unified) > 0:\n",
    "    X_train_unified = X_train_baseline.drop(columns=features_to_remove_unified)\n",
    "    X_test_unified = X_test_baseline.drop(columns=features_to_remove_unified)\n",
    "    \n",
    "    print(f\"\\n‚úì Features removed: {len(features_to_remove_unified)}\")\n",
    "    print(f\"‚úì Baseline features: {X_train_baseline.shape[1]}\")\n",
    "    print(f\"‚úì Unified features: {X_train_unified.shape[1]}\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è No features to remove - using baseline as unified set\")\n",
    "    X_train_unified = X_train_baseline.copy()\n",
    "    X_test_unified = X_test_baseline.copy()\n",
    "\n",
    "print(f\"\\nFINAL UNIFIED FEATURE SET:\")\n",
    "print(f\"  ‚Ä¢ Train: {X_train_unified.shape[0]:,} samples √ó {X_train_unified.shape[1]} features\")\n",
    "print(f\"  ‚Ä¢ Test: {X_test_unified.shape[0]:,} samples √ó {X_test_unified.shape[1]} features\")\n",
    "\n",
    "print(f\"\\nüìã Feature List ({X_train_unified.shape[1]} features):\")\n",
    "feature_list = X_train_unified.columns.tolist()\n",
    "for i, feat in enumerate(feature_list[:15], 1):  # Show first 15\n",
    "    print(f\"  {i:2d}. {feat}\")\n",
    "if len(feature_list) > 15:\n",
    "    print(f\"  ... and {len(feature_list) - 15} more features\")\n",
    "\n",
    "# ============================================================================\n",
    "# STAP 4B: TRAIN RANDOM FOREST (UNIFIED FEATURES)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STAP 4B: TRAIN RANDOM FOREST WITH UNIFIED FEATURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nModel Configuration:\")\n",
    "print(f\"  ‚Ä¢ Algorithm: Random Forest Classifier\")\n",
    "print(f\"  ‚Ä¢ Features: {X_train_unified.shape[1]} (unified set)\")\n",
    "print(f\"  ‚Ä¢ n_estimators: 100\")\n",
    "print(f\"  ‚Ä¢ max_depth: None (full trees)\")\n",
    "print(f\"  ‚Ä¢ random_state: 42\")\n",
    "\n",
    "print(f\"\\nüîÑ Training Random Forest (Unified)...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Initialize model\n",
    "rf_unified = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=None,\n",
    "    min_samples_split=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Train\n",
    "rf_unified.fit(X_train_unified, y_train)\n",
    "\n",
    "training_time_rf_unified = time.time() - start_time\n",
    "print(f\"‚úì Training completed in {training_time_rf_unified:.2f} seconds\")\n",
    "\n",
    "# Predictions\n",
    "print(f\"\\nüîÑ Making predictions...\")\n",
    "y_train_pred_rf_unified = rf_unified.predict(X_train_unified)\n",
    "y_test_pred_rf_unified = rf_unified.predict(X_test_unified)\n",
    "y_test_proba_rf_unified = rf_unified.predict_proba(X_test_unified)\n",
    "\n",
    "# Calculate metrics\n",
    "train_acc_rf_unified = accuracy_score(y_train, y_train_pred_rf_unified)\n",
    "test_acc_rf_unified = accuracy_score(y_test, y_test_pred_rf_unified)\n",
    "precision_rf_unified = precision_score(y_test, y_test_pred_rf_unified, average='weighted')\n",
    "recall_rf_unified = recall_score(y_test, y_test_pred_rf_unified, average='weighted')\n",
    "f1_rf_unified = f1_score(y_test, y_test_pred_rf_unified, average='weighted')\n",
    "\n",
    "print(f\"\\nüìä RANDOM FOREST (UNIFIED) - PERFORMANCE:\")\n",
    "print(f\"  ‚Ä¢ Train Accuracy: {train_acc_rf_unified:.4f} ({train_acc_rf_unified*100:.2f}%)\")\n",
    "print(f\"  ‚Ä¢ Test Accuracy: {test_acc_rf_unified:.4f} ({test_acc_rf_unified*100:.2f}%)\")\n",
    "print(f\"  ‚Ä¢ Precision: {precision_rf_unified:.4f}\")\n",
    "print(f\"  ‚Ä¢ Recall: {recall_rf_unified:.4f}\")\n",
    "print(f\"  ‚Ä¢ F1-Score: {f1_rf_unified:.4f}\")\n",
    "\n",
    "# Overfitting check\n",
    "overfitting_rf_unified = train_acc_rf_unified - test_acc_rf_unified\n",
    "print(f\"\\n  ‚Ä¢ Overfitting gap: {overfitting_rf_unified:.4f} ({overfitting_rf_unified*100:.2f}%)\")\n",
    "if overfitting_rf_unified < 0.05:\n",
    "    print(f\"  ‚úì Minimal overfitting\")\n",
    "elif overfitting_rf_unified < 0.10:\n",
    "    print(f\"  ‚ö† Moderate overfitting\")\n",
    "else:\n",
    "    print(f\"  ‚ö†Ô∏è Significant overfitting\")\n",
    "\n",
    "# ============================================================================\n",
    "# STAP 4C: TRAIN LOGISTIC REGRESSION (UNIFIED FEATURES)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STAP 4C: TRAIN LOGISTIC REGRESSION WITH UNIFIED FEATURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nModel Configuration:\")\n",
    "print(f\"  ‚Ä¢ Algorithm: Logistic Regression\")\n",
    "print(f\"  ‚Ä¢ Features: {X_train_unified.shape[1]} (unified set - SAME as RF!)\")\n",
    "print(f\"  ‚Ä¢ multi_class: multinomial\")\n",
    "print(f\"  ‚Ä¢ solver: lbfgs\")\n",
    "print(f\"  ‚Ä¢ max_iter: 1000\")\n",
    "print(f\"  ‚Ä¢ random_state: 42\")\n",
    "\n",
    "print(f\"\\nüîÑ Training Logistic Regression (Unified)...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Initialize model\n",
    "lr_unified = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    multi_class='multinomial',\n",
    "    solver='lbfgs',\n",
    "    random_state=42,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Train\n",
    "lr_unified.fit(X_train_unified, y_train)\n",
    "\n",
    "training_time_lr_unified = time.time() - start_time\n",
    "print(f\"‚úì Training completed in {training_time_lr_unified:.2f} seconds\")\n",
    "\n",
    "# Predictions\n",
    "print(f\"\\nüîÑ Making predictions...\")\n",
    "y_train_pred_lr_unified = lr_unified.predict(X_train_unified)\n",
    "y_test_pred_lr_unified = lr_unified.predict(X_test_unified)\n",
    "y_test_proba_lr_unified = lr_unified.predict_proba(X_test_unified)\n",
    "\n",
    "# Calculate metrics\n",
    "train_acc_lr_unified = accuracy_score(y_train, y_train_pred_lr_unified)\n",
    "test_acc_lr_unified = accuracy_score(y_test, y_test_pred_lr_unified)\n",
    "precision_lr_unified = precision_score(y_test, y_test_pred_lr_unified, average='weighted')\n",
    "recall_lr_unified = recall_score(y_test, y_test_pred_lr_unified, average='weighted')\n",
    "f1_lr_unified = f1_score(y_test, y_test_pred_lr_unified, average='weighted')\n",
    "\n",
    "print(f\"\\nüìä LOGISTIC REGRESSION (UNIFIED) - PERFORMANCE:\")\n",
    "print(f\"  ‚Ä¢ Train Accuracy: {train_acc_lr_unified:.4f} ({train_acc_lr_unified*100:.2f}%)\")\n",
    "print(f\"  ‚Ä¢ Test Accuracy: {test_acc_lr_unified:.4f} ({test_acc_lr_unified*100:.2f}%)\")\n",
    "print(f\"  ‚Ä¢ Precision: {precision_lr_unified:.4f}\")\n",
    "print(f\"  ‚Ä¢ Recall: {recall_lr_unified:.4f}\")\n",
    "print(f\"  ‚Ä¢ F1-Score: {f1_lr_unified:.4f}\")\n",
    "\n",
    "# Overfitting check\n",
    "overfitting_lr_unified = train_acc_lr_unified - test_acc_lr_unified\n",
    "print(f\"\\n  ‚Ä¢ Overfitting gap: {overfitting_lr_unified:.4f} ({overfitting_lr_unified*100:.2f}%)\")\n",
    "if overfitting_lr_unified < 0.05:\n",
    "    print(f\"  ‚úì Minimal overfitting\")\n",
    "elif overfitting_lr_unified < 0.10:\n",
    "    print(f\"  ‚ö† Moderate overfitting\")\n",
    "else:\n",
    "    print(f\"  ‚ö†Ô∏è Significant overfitting\")\n",
    "\n",
    "# Sanity check\n",
    "unique_preds_lr = len(np.unique(y_test_pred_lr_unified))\n",
    "print(f\"\\n  ‚Ä¢ Unique predictions: {unique_preds_lr}/3 classes\")\n",
    "if unique_preds_lr == 3:\n",
    "    print(f\"  ‚úì Model predicts all classes (working correctly)\")\n",
    "else:\n",
    "    print(f\"  ‚ö†Ô∏è WARNING: Model only predicts {unique_preds_lr} classes!\")\n",
    "\n",
    "# ============================================================================\n",
    "# STAP 4D: COMPREHENSIVE COMPARISON - BASELINE vs UNIFIED\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STAP 4D: COMPREHENSIVE COMPARISON - BASELINE vs UNIFIED\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create comparison table\n",
    "comparison_unified = pd.DataFrame({\n",
    "    'Model': [\n",
    "        'RF - Baseline (Step 1)',\n",
    "        'RF - Unified (Step 4)',\n",
    "        'LR - Baseline (Step 1)', \n",
    "        'LR - Unified (Step 4)'\n",
    "    ],\n",
    "    'Features': [\n",
    "        X_train_baseline.shape[1],\n",
    "        X_train_unified.shape[1],\n",
    "        X_train_baseline.shape[1] if 'Diastolic' not in X_train_baseline.columns else X_train_baseline.shape[1] - 1,\n",
    "        X_train_unified.shape[1]\n",
    "    ],\n",
    "    'Train Acc': [\n",
    "        train_accuracy_rf if 'train_accuracy_rf' in locals() else 0.0,\n",
    "        train_acc_rf_unified,\n",
    "        train_accuracy_lr if 'train_accuracy_lr' in locals() else 0.0,\n",
    "        train_acc_lr_unified\n",
    "    ],\n",
    "    'Test Acc': [\n",
    "        test_accuracy_rf if 'test_accuracy_rf' in locals() else 0.0,\n",
    "        test_acc_rf_unified,\n",
    "        test_accuracy_lr if 'test_accuracy_lr' in locals() else 0.0,\n",
    "        test_acc_lr_unified\n",
    "    ],\n",
    "    'Precision': [\n",
    "        precision_rf if 'precision_rf' in locals() else 0.0,\n",
    "        precision_rf_unified,\n",
    "        precision_lr if 'precision_lr' in locals() else 0.0,\n",
    "        precision_lr_unified\n",
    "    ],\n",
    "    'Recall': [\n",
    "        recall_rf if 'recall_rf' in locals() else 0.0,\n",
    "        recall_rf_unified,\n",
    "        recall_lr if 'recall_lr' in locals() else 0.0,\n",
    "        recall_lr_unified\n",
    "    ],\n",
    "    'F1-Score': [\n",
    "        f1_rf if 'f1_rf' in locals() else 0.0,\n",
    "        f1_rf_unified,\n",
    "        f1_lr if 'f1_lr' in locals() else 0.0,\n",
    "        f1_lr_unified\n",
    "    ],\n",
    "    'Training Time': [\n",
    "        training_time_rf if 'training_time_rf' in locals() else 0.0,\n",
    "        training_time_rf_unified,\n",
    "        training_time_lr if 'training_time_lr' in locals() else 0.0,\n",
    "        training_time_lr_unified\n",
    "    ]\n",
    "})\n",
    "\n",
    "comparison_unified = comparison_unified.round(4)\n",
    "\n",
    "print(\"\\nüìä PERFORMANCE COMPARISON TABLE:\")\n",
    "print(\"=\"*80)\n",
    "display(comparison_unified)\n",
    "\n",
    "# Calculate improvements\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"PERFORMANCE CHANGES (Baseline ‚Üí Unified)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "if 'test_accuracy_rf' in locals():\n",
    "    rf_change = (test_acc_rf_unified - test_accuracy_rf) * 100\n",
    "    print(f\"\\nRANDOM FOREST:\")\n",
    "    print(f\"  ‚Ä¢ Accuracy: {test_accuracy_rf*100:.2f}% ‚Üí {test_acc_rf_unified*100:.2f}% ({rf_change:+.2f}%)\")\n",
    "    print(f\"  ‚Ä¢ Features: {X_train_baseline.shape[1]} ‚Üí {X_train_unified.shape[1]}\")\n",
    "    \n",
    "    if abs(rf_change) < 0.5:\n",
    "        print(f\"  ‚úì Performance STABLE (minimal change)\")\n",
    "    elif rf_change > 0:\n",
    "        print(f\"  ‚úì Performance IMPROVED\")\n",
    "    else:\n",
    "        print(f\"  ‚ö†Ô∏è Slight performance decrease (expected for RF)\")\n",
    "\n",
    "if 'test_accuracy_lr' in locals():\n",
    "    lr_change = (test_acc_lr_unified - test_accuracy_lr) * 100\n",
    "    print(f\"\\nLOGISTIC REGRESSION:\")\n",
    "    print(f\"  ‚Ä¢ Accuracy: {test_accuracy_lr*100:.2f}% ‚Üí {test_acc_lr_unified*100:.2f}% ({lr_change:+.2f}%)\")\n",
    "    print(f\"  ‚Ä¢ Features: Both using {X_train_unified.shape[1]} (NOW CONSISTENT!)\")\n",
    "    \n",
    "    if abs(lr_change) < 0.5:\n",
    "        print(f\"  ‚úì Performance STABLE\")\n",
    "    elif lr_change > 0:\n",
    "        print(f\"  ‚úì Performance IMPROVED (multicollineariteit reduced!)\")\n",
    "    else:\n",
    "        print(f\"  ‚ö†Ô∏è Performance decreased\")\n",
    "\n",
    "# ============================================================================\n",
    "# STAP 4E: DETAILED EVALUATION - CLASSIFICATION REPORTS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STAP 4E: DETAILED CLASSIFICATION REPORTS (UNIFIED MODELS)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"RANDOM FOREST (UNIFIED) - CLASSIFICATION REPORT\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(\"\\nClassification Report (Test Set):\")\n",
    "print(classification_report(\n",
    "    y_test, \n",
    "    y_test_pred_rf_unified,\n",
    "    target_names=['Insomnia', 'None', 'Sleep Apnea'],\n",
    "    digits=4\n",
    "))\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"LOGISTIC REGRESSION (UNIFIED) - CLASSIFICATION REPORT\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(\"\\nClassification Report (Test Set):\")\n",
    "print(classification_report(\n",
    "    y_test,\n",
    "    y_test_pred_lr_unified,\n",
    "    target_names=['Insomnia', 'None', 'Sleep Apnea'],\n",
    "    digits=4\n",
    "))\n",
    "\n",
    "# ============================================================================\n",
    "# STAP 4F: VISUALIZATION - CONFUSION MATRICES\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STAP 4F: CONFUSION MATRICES (UNIFIED MODELS)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Random Forest Confusion Matrix\n",
    "cm_rf_unified = confusion_matrix(y_test, y_test_pred_rf_unified)\n",
    "sns.heatmap(cm_rf_unified, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Insomnia', 'None', 'Sleep Apnea'],\n",
    "            yticklabels=['Insomnia', 'None', 'Sleep Apnea'],\n",
    "            ax=axes[0], cbar_kws={'label': 'Count'})\n",
    "axes[0].set_title(f'Random Forest (Unified)\\nAccuracy: {test_acc_rf_unified*100:.2f}% | F1: {f1_rf_unified:.4f}',\n",
    "                  fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Actual Class')\n",
    "axes[0].set_xlabel('Predicted Class')\n",
    "\n",
    "# Logistic Regression Confusion Matrix\n",
    "cm_lr_unified = confusion_matrix(y_test, y_test_pred_lr_unified)\n",
    "sns.heatmap(cm_lr_unified, annot=True, fmt='d', cmap='Greens',\n",
    "            xticklabels=['Insomnia', 'None', 'Sleep Apnea'],\n",
    "            yticklabels=['Insomnia', 'None', 'Sleep Apnea'],\n",
    "            ax=axes[1], cbar_kws={'label': 'Count'})\n",
    "axes[1].set_title(f'Logistic Regression (Unified)\\nAccuracy: {test_acc_lr_unified*100:.2f}% | F1: {f1_lr_unified:.4f}',\n",
    "                  fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Actual Class')\n",
    "axes[1].set_xlabel('Predicted Class')\n",
    "\n",
    "plt.suptitle('Confusion Matrices - Unified Models (Test Set)', \n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# STAP 4G: VISUALIZATION - PERFORMANCE COMPARISON\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STAP 4G: PERFORMANCE COMPARISON VISUALIZATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Plot 1: Test Accuracy Comparison\n",
    "models = ['RF Baseline', 'RF Unified', 'LR Baseline', 'LR Unified']\n",
    "if 'test_accuracy_rf' in locals() and 'test_accuracy_lr' in locals():\n",
    "    test_accs = [test_accuracy_rf, test_acc_rf_unified, test_accuracy_lr, test_acc_lr_unified]\n",
    "else:\n",
    "    test_accs = [0, test_acc_rf_unified, 0, test_acc_lr_unified]\n",
    "\n",
    "colors = ['lightblue', 'steelblue', 'lightgreen', 'seagreen']\n",
    "bars = axes[0, 0].bar(models, test_accs, color=colors, alpha=0.8)\n",
    "axes[0, 0].set_ylabel('Test Accuracy')\n",
    "axes[0, 0].set_title('Test Accuracy: Baseline vs Unified', fontweight='bold', fontsize=12)\n",
    "axes[0, 0].set_ylim([0.85, 1.0])\n",
    "axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar, acc in zip(bars, test_accs):\n",
    "    if acc > 0:\n",
    "        height = bar.get_height()\n",
    "        axes[0, 0].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                       f'{acc:.4f}\\n({acc*100:.2f}%)',\n",
    "                       ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Plot 2: F1-Score Comparison\n",
    "if 'f1_rf' in locals() and 'f1_lr' in locals():\n",
    "    f1_scores = [f1_rf, f1_rf_unified, f1_lr, f1_lr_unified]\n",
    "else:\n",
    "    f1_scores = [0, f1_rf_unified, 0, f1_lr_unified]\n",
    "\n",
    "bars = axes[0, 1].bar(models, f1_scores, color=colors, alpha=0.8)\n",
    "axes[0, 1].set_ylabel('F1-Score (Weighted)')\n",
    "axes[0, 1].set_title('F1-Score: Baseline vs Unified', fontweight='bold', fontsize=12)\n",
    "axes[0, 1].set_ylim([0.85, 1.0])\n",
    "axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar, f1 in zip(bars, f1_scores):\n",
    "    if f1 > 0:\n",
    "        height = bar.get_height()\n",
    "        axes[0, 1].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                       f'{f1:.4f}',\n",
    "                       ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Plot 3: Per-Class Performance (RF Unified)\n",
    "print(\"\\nExtracting per-class metrics for Random Forest...\")\n",
    "rf_report = classification_report(y_test, y_test_pred_rf_unified, output_dict=True)\n",
    "classes = ['Insomnia', 'None', 'Sleep Apnea']\n",
    "rf_precision = [rf_report['0']['precision'], rf_report['1']['precision'], rf_report['2']['precision']]\n",
    "rf_recall = [rf_report['0']['recall'], rf_report['1']['recall'], rf_report['2']['recall']]\n",
    "rf_f1 = [rf_report['0']['f1-score'], rf_report['1']['f1-score'], rf_report['2']['f1-score']]\n",
    "\n",
    "x = np.arange(len(classes))\n",
    "width = 0.25\n",
    "\n",
    "axes[1, 0].bar(x - width, rf_precision, width, label='Precision', color='steelblue', alpha=0.8)\n",
    "axes[1, 0].bar(x, rf_recall, width, label='Recall', color='orange', alpha=0.8)\n",
    "axes[1, 0].bar(x + width, rf_f1, width, label='F1-Score', color='green', alpha=0.8)\n",
    "axes[1, 0].set_ylabel('Score')\n",
    "axes[1, 0].set_title('Random Forest (Unified) - Per-Class Metrics', fontweight='bold', fontsize=12)\n",
    "axes[1, 0].set_xticks(x)\n",
    "axes[1, 0].set_xticklabels(classes)\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].set_ylim([0.8, 1.0])\n",
    "axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 4: Per-Class Performance (LR Unified)\n",
    "print(\"Extracting per-class metrics for Logistic Regression...\")\n",
    "lr_report = classification_report(y_test, y_test_pred_lr_unified, output_dict=True)\n",
    "lr_precision = [lr_report['0']['precision'], lr_report['1']['precision'], lr_report['2']['precision']]\n",
    "lr_recall = [lr_report['0']['recall'], lr_report['1']['recall'], lr_report['2']['recall']]\n",
    "lr_f1 = [lr_report['0']['f1-score'], lr_report['1']['f1-score'], lr_report['2']['f1-score']]\n",
    "\n",
    "axes[1, 1].bar(x - width, lr_precision, width, label='Precision', color='seagreen', alpha=0.8)\n",
    "axes[1, 1].bar(x, lr_recall, width, label='Recall', color='orange', alpha=0.8)\n",
    "axes[1, 1].bar(x + width, lr_f1, width, label='F1-Score', color='purple', alpha=0.8)\n",
    "axes[1, 1].set_ylabel('Score')\n",
    "axes[1, 1].set_title('Logistic Regression (Unified) - Per-Class Metrics', fontweight='bold', fontsize=12)\n",
    "axes[1, 1].set_xticks(x)\n",
    "axes[1, 1].set_xticklabels(classes)\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].set_ylim([0.8, 1.0])\n",
    "axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.suptitle('Step 4: Unified Models Performance Analysis', fontsize=16, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# STAP 4H: FINAL SUMMARY & RECOMMENDATIONS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STAP 4H: FINAL SUMMARY & RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\"\"\n",
    "UNIFIED FEATURE SET RESULTS:\n",
    "============================\n",
    "\n",
    "FEATURE REDUCTION:\n",
    "‚Ä¢ Baseline: {X_train_baseline.shape[1]} features\n",
    "‚Ä¢ Unified: {X_train_unified.shape[1]} features\n",
    "‚Ä¢ Removed: {X_train_baseline.shape[1] - X_train_unified.shape[1]} features ({(X_train_baseline.shape[1] - X_train_unified.shape[1])/X_train_baseline.shape[1]*100:.1f}% reduction)\n",
    "‚Ä¢ Removed features: {', '.join(features_to_remove_unified)}\n",
    "\n",
    "RANDOM FOREST (UNIFIED):\n",
    "‚Ä¢ Test Accuracy: {test_acc_rf_unified*100:.2f}%\n",
    "‚Ä¢ F1-Score: {f1_rf_unified:.4f}\n",
    "‚Ä¢ Training Time: {training_time_rf_unified:.2f}s\n",
    "‚Ä¢ Status: ‚úì Stable performance with fewer features\n",
    "\n",
    "LOGISTIC REGRESSION (UNIFIED):\n",
    "‚Ä¢ Test Accuracy: {test_acc_lr_unified*100:.2f}%\n",
    "‚Ä¢ F1-Score: {f1_lr_unified:.4f}\n",
    "‚Ä¢ Training Time: {training_time_lr_unified:.2f}s\n",
    "‚Ä¢ Status: ‚úì Improved stability (multicollineariteit reduced)\n",
    "\n",
    "KEY IMPROVEMENTS:\n",
    "=================\n",
    "\n",
    "1. ‚úì CONSISTENCY: Both models now use SAME {X_train_unified.shape[1]} features\n",
    "2. ‚úì MULTICOLLINEARITEIT: Diastolic removed (r=0.979 with Systolic)\n",
    "3. ‚úì STABILITY: Logistic Regression trains without numerical issues\n",
    "4. ‚úì EFFICIENCY: Faster training with fewer features\n",
    "5. ‚úì INTERPRETABILITY: Cleaner model without redundant features\n",
    "\"\"\")\n",
    "\n",
    "# Determine best model\n",
    "if test_acc_rf_unified > test_acc_lr_unified:\n",
    "    best_model = \"Random Forest\"\n",
    "    best_acc = test_acc_rf_unified\n",
    "    best_f1 = f1_rf_unified\n",
    "else:\n",
    "    best_model = \"Logistic Regression\"\n",
    "    best_acc = test_acc_lr_unified\n",
    "    best_f1 = f1_lr_unified\n",
    "\n",
    "print(f\"\\nüèÜ BEST PERFORMING MODEL:\")\n",
    "print(f\"  ‚Ä¢ Model: {best_model}\")\n",
    "print(f\"  ‚Ä¢ Test Accuracy: {best_acc*100:.2f}%\")\n",
    "print(f\"  ‚Ä¢ F1-Score: {best_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b0a462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# UITWERKING ML VRAAGSTUK\n",
    "# STAP 5: HYPERPARAMETER TUNING - OPTIMALISATIE VAN MODELPERFORMANCE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STAP 5: HYPERPARAMETER TUNING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "DOEL VAN HYPERPARAMETER TUNING:\n",
    "================================\n",
    "\n",
    "Tot nu toe hebben we beide modellen getraind met DEFAULT hyperparameters.\n",
    "Nu gaan we systematisch zoeken naar OPTIMALE hyperparameters om performance\n",
    "te maximaliseren.\n",
    "\n",
    "WAAROM IS DIT BELANGRIJK?\n",
    "==========================\n",
    "\n",
    "Default parameters zijn algemene instellingen die \"redelijk goed\" werken voor\n",
    "de meeste datasets, maar zijn ZELDEN optimaal voor jouw specifieke data.\n",
    "\n",
    "Voorbeeld Random Forest defaults:\n",
    "‚Ä¢ n_estimators = 100 ‚Üí Misschien hebben we 200 nodig voor stabielere voorspellingen?\n",
    "‚Ä¢ max_depth = None ‚Üí Misschien overfits het model, en is max_depth=20 beter?\n",
    "‚Ä¢ min_samples_split = 2 ‚Üí Misschien voorkomt min_samples_split=10 overfitting?\n",
    "\n",
    "METHODEN VOOR HYPERPARAMETER TUNING:\n",
    "=====================================\n",
    "\n",
    "1. GRID SEARCH (Exhaustive Search)\n",
    "   ‚Ä¢ Definieert grid van alle mogelijke combinaties\n",
    "   ‚Ä¢ Test ELKE combinatie systematisch\n",
    "   ‚Ä¢ Voordeel: Vindt GEGARANDEERD beste combinatie in grid\n",
    "   ‚Ä¢ Nadeel: Zeer tijdrovend bij grote grids\n",
    "   \n",
    "2. RANDOMIZED SEARCH (Random Sampling)\n",
    "   ‚Ä¢ Samples random combinaties uit parameter space\n",
    "   ‚Ä¢ Test vooraf gedefinieerd aantal combinaties\n",
    "   ‚Ä¢ Voordeel: Sneller, vaak \"good enough\"\n",
    "   ‚Ä¢ Nadeel: Kan optimale combinatie missen\n",
    "   \n",
    "3. BAYESIAN OPTIMIZATION (Smart Search)\n",
    "   ‚Ä¢ Gebruikt previous results om next search te informeren\n",
    "   ‚Ä¢ \"Slimmer\" dan random search\n",
    "   ‚Ä¢ Voordeel: Effici√´nter voor expensive models\n",
    "   ‚Ä¢ Nadeel: Complexer, vereist extra libraries\n",
    "\n",
    "ONZE AANPAK:\n",
    "============\n",
    "\n",
    "We gebruiken RANDOMIZED SEARCH omdat:\n",
    "‚Ä¢ Sneller dan Grid Search (belangrijk bij grote datasets)\n",
    "‚Ä¢ Goed genoeg voor meeste gevallen (80-90% van optimaal)\n",
    "‚Ä¢ Makkelijk te implementeren met sklearn\n",
    "\n",
    "TUNING STRATEGIE:\n",
    "=================\n",
    "\n",
    "1. RANDOM FOREST TUNING:\n",
    "   ‚Ä¢ n_estimators: [50, 100, 200, 300, 500]\n",
    "   ‚Ä¢ max_depth: [10, 20, 30, None]\n",
    "   ‚Ä¢ min_samples_split: [2, 5, 10]\n",
    "   ‚Ä¢ min_samples_leaf: [1, 2, 4]\n",
    "   ‚Ä¢ max_features: ['sqrt', 'log2', None]\n",
    "   \n",
    "2. LOGISTIC REGRESSION TUNING:\n",
    "   ‚Ä¢ C (regularization): [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "   ‚Ä¢ penalty: ['l1', 'l2']\n",
    "   ‚Ä¢ solver: ['liblinear', 'saga'] (compatible with l1/l2)\n",
    "   ‚Ä¢ max_iter: [1000, 2000]\n",
    "\n",
    "EVALUATIE METHODE:\n",
    "==================\n",
    "\n",
    "We gebruiken STRATIFIED K-FOLD CROSS-VALIDATION (k=5):\n",
    "‚Ä¢ Dataset wordt gesplitst in 5 folds\n",
    "‚Ä¢ Model wordt 5x getraind (elke keer andere fold als validation)\n",
    "‚Ä¢ Performance = gemiddelde over 5 folds\n",
    "‚Ä¢ Voorkomt overfitting op √©√©n specifieke train-test split\n",
    "\n",
    "VERWACHTE RESULTATEN:\n",
    "=====================\n",
    "\n",
    "‚Ä¢ Baseline (default params): ~88-90% accuracy\n",
    "‚Ä¢ Tuned (optimized params): ~90-92% accuracy (verwacht +1-3%)\n",
    "‚Ä¢ Mogelijk: Tuning geeft GEEN verbetering ‚Üí defaults waren al goed!\n",
    "\"\"\")\n",
    "\n",
    "# ============================================================================\n",
    "# IMPORT REQUIRED LIBRARIES\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"IMPORT TUNING LIBRARIES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
    "from scipy.stats import randint, uniform\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úì Libraries imported successfully\")\n",
    "\n",
    "# ============================================================================\n",
    "# STAP 5A: VERIFY DATA STATUS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STAP 5A: VERIFY DATA STATUS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check if we have unified data from Step 4\n",
    "if 'X_train_unified' not in locals() or 'X_test_unified' not in locals():\n",
    "    print(\"\\n‚ö†Ô∏è Unified data not found - using current X_train and X_test\")\n",
    "    X_train_unified = X_train.copy()\n",
    "    X_test_unified = X_test.copy()\n",
    "\n",
    "print(f\"\\nDATA FOR TUNING:\")\n",
    "print(f\"  ‚Ä¢ X_train: {X_train_unified.shape[0]:,} samples √ó {X_train_unified.shape[1]} features\")\n",
    "print(f\"  ‚Ä¢ X_test: {X_test_unified.shape[0]:,} samples √ó {X_test_unified.shape[1]} features\")\n",
    "print(f\"  ‚Ä¢ y_train: {len(y_train):,} samples\")\n",
    "print(f\"  ‚Ä¢ y_test: {len(y_test):,} samples\")\n",
    "\n",
    "print(f\"\\n‚úì Data ready for hyperparameter tuning\")\n",
    "\n",
    "# ============================================================================\n",
    "# STAP 5B: RANDOM FOREST HYPERPARAMETER TUNING\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STAP 5B: RANDOM FOREST - HYPERPARAMETER TUNING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nDEFINING PARAMETER GRID FOR RANDOM FOREST:\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Define parameter distributions for RandomizedSearchCV\n",
    "rf_param_distributions = {\n",
    "    'n_estimators': [50, 100, 150, 200, 300],\n",
    "    'max_depth': [10, 20, 30, 40, None],\n",
    "    'min_samples_split': [2, 5, 10, 15],\n",
    "    'min_samples_leaf': [1, 2, 4, 8],\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "print(\"\\nParameter distributions:\")\n",
    "for param, values in rf_param_distributions.items():\n",
    "    print(f\"  ‚Ä¢ {param}: {values}\")\n",
    "\n",
    "total_combinations = 1\n",
    "for values in rf_param_distributions.values():\n",
    "    total_combinations *= len(values)\n",
    "\n",
    "print(f\"\\nTotal possible combinations: {total_combinations:,}\")\n",
    "print(f\"We will sample: 50 random combinations (Randomized Search)\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"SETTING UP RANDOMIZED SEARCH\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Initialize base model\n",
    "rf_base = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "\n",
    "# Setup cross-validation\n",
    "cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "print(\"\\nCross-Validation Strategy:\")\n",
    "print(f\"  ‚Ä¢ Method: Stratified K-Fold\")\n",
    "print(f\"  ‚Ä¢ Number of folds: 5\")\n",
    "print(f\"  ‚Ä¢ Shuffle: True\")\n",
    "print(f\"  ‚Ä¢ Random state: 42\")\n",
    "\n",
    "# Setup RandomizedSearchCV\n",
    "rf_random_search = RandomizedSearchCV(\n",
    "    estimator=rf_base,\n",
    "    param_distributions=rf_param_distributions,\n",
    "    n_iter=50,  # Number of random combinations to try\n",
    "    cv=cv_strategy,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "print(\"\\nRandomizedSearchCV Configuration:\")\n",
    "print(f\"  ‚Ä¢ Estimator: Random Forest Classifier\")\n",
    "print(f\"  ‚Ä¢ Iterations: 50 (random samples)\")\n",
    "print(f\"  ‚Ä¢ Scoring metric: Accuracy\")\n",
    "print(f\"  ‚Ä¢ Parallel jobs: -1 (all CPU cores)\")\n",
    "print(f\"  ‚Ä¢ Return train score: True\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"RUNNING RANDOMIZED SEARCH FOR RANDOM FOREST\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(\"\\nüîÑ This may take several minutes...\")\n",
    "print(\"   Training 50 combinations √ó 5 folds = 250 models total\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Fit RandomizedSearchCV\n",
    "rf_random_search.fit(X_train_unified, y_train)\n",
    "\n",
    "tuning_time_rf = time.time() - start_time\n",
    "\n",
    "print(f\"\\n‚úì Tuning completed in {tuning_time_rf:.2f} seconds ({tuning_time_rf/60:.2f} minutes)\")\n",
    "\n",
    "# Get best parameters\n",
    "rf_best_params = rf_random_search.best_params_\n",
    "rf_best_score = rf_random_search.best_score_\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"RANDOM FOREST - BEST HYPERPARAMETERS FOUND\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(\"\\nBest parameters:\")\n",
    "for param, value in rf_best_params.items():\n",
    "    print(f\"  ‚Ä¢ {param}: {value}\")\n",
    "\n",
    "print(f\"\\nBest cross-validation score (mean accuracy across 5 folds):\")\n",
    "print(f\"  ‚Ä¢ {rf_best_score:.4f} ({rf_best_score*100:.2f}%)\")\n",
    "\n",
    "# Compare with baseline (if available)\n",
    "if 'test_acc_rf_unified' in locals():\n",
    "    improvement_rf = (rf_best_score - test_acc_rf_unified) * 100\n",
    "    print(f\"\\nComparison with baseline (Step 4):\")\n",
    "    print(f\"  ‚Ä¢ Baseline (default params): {test_acc_rf_unified*100:.2f}%\")\n",
    "    print(f\"  ‚Ä¢ Tuned (optimized params): {rf_best_score*100:.2f}%\")\n",
    "    print(f\"  ‚Ä¢ Improvement: {improvement_rf:+.2f}%\")\n",
    "    \n",
    "    if improvement_rf > 0.5:\n",
    "        print(f\"  ‚úì Significant improvement achieved!\")\n",
    "    elif improvement_rf > 0:\n",
    "        print(f\"  ‚úì Slight improvement\")\n",
    "    else:\n",
    "        print(f\"  ‚ö† No improvement - default params were already good\")\n",
    "\n",
    "# Train final model with best parameters\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"TRAINING FINAL RANDOM FOREST WITH BEST PARAMETERS\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "rf_tuned = RandomForestClassifier(**rf_best_params, random_state=42, n_jobs=-1)\n",
    "rf_tuned.fit(X_train_unified, y_train)\n",
    "\n",
    "# Evaluate on test set\n",
    "y_test_pred_rf_tuned = rf_tuned.predict(X_test_unified)\n",
    "test_acc_rf_tuned = accuracy_score(y_test, y_test_pred_rf_tuned)\n",
    "precision_rf_tuned = precision_score(y_test, y_test_pred_rf_tuned, average='weighted')\n",
    "recall_rf_tuned = recall_score(y_test, y_test_pred_rf_tuned, average='weighted')\n",
    "f1_rf_tuned = f1_score(y_test, y_test_pred_rf_tuned, average='weighted')\n",
    "\n",
    "print(f\"\\nüìä RANDOM FOREST (TUNED) - TEST SET PERFORMANCE:\")\n",
    "print(f\"  ‚Ä¢ Test Accuracy: {test_acc_rf_tuned:.4f} ({test_acc_rf_tuned*100:.2f}%)\")\n",
    "print(f\"  ‚Ä¢ Precision: {precision_rf_tuned:.4f}\")\n",
    "print(f\"  ‚Ä¢ Recall: {recall_rf_tuned:.4f}\")\n",
    "print(f\"  ‚Ä¢ F1-Score: {f1_rf_tuned:.4f}\")\n",
    "\n",
    "print(f\"\\n‚úì Random Forest tuning complete!\")\n",
    "\n",
    "# ============================================================================\n",
    "# STAP 5C: LOGISTIC REGRESSION HYPERPARAMETER TUNING\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STAP 5C: LOGISTIC REGRESSION - HYPERPARAMETER TUNING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nDEFINING PARAMETER GRID FOR LOGISTIC REGRESSION:\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Define parameter distributions\n",
    "lr_param_distributions = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "    'penalty': ['l2'],  # l2 works with all solvers\n",
    "    'solver': ['lbfgs', 'saga', 'liblinear'],\n",
    "    'max_iter': [1000, 2000, 3000],\n",
    "    'class_weight': [None, 'balanced']\n",
    "}\n",
    "\n",
    "print(\"\\nParameter distributions:\")\n",
    "for param, values in lr_param_distributions.items():\n",
    "    print(f\"  ‚Ä¢ {param}: {values}\")\n",
    "\n",
    "total_combinations_lr = 1\n",
    "for values in lr_param_distributions.values():\n",
    "    total_combinations_lr *= len(values)\n",
    "\n",
    "print(f\"\\nTotal possible combinations: {total_combinations_lr:,}\")\n",
    "print(f\"We will sample: 30 random combinations (Randomized Search)\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"SETTING UP RANDOMIZED SEARCH\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Initialize base model\n",
    "lr_base = LogisticRegression(random_state=42, multi_class='multinomial')\n",
    "\n",
    "# Setup RandomizedSearchCV\n",
    "lr_random_search = RandomizedSearchCV(\n",
    "    estimator=lr_base,\n",
    "    param_distributions=lr_param_distributions,\n",
    "    n_iter=30,  # Fewer iterations (LR is faster than RF)\n",
    "    cv=cv_strategy,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "print(\"\\nRandomizedSearchCV Configuration:\")\n",
    "print(f\"  ‚Ä¢ Estimator: Logistic Regression\")\n",
    "print(f\"  ‚Ä¢ Iterations: 30 (random samples)\")\n",
    "print(f\"  ‚Ä¢ Scoring metric: Accuracy\")\n",
    "print(f\"  ‚Ä¢ Parallel jobs: -1 (all CPU cores)\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"RUNNING RANDOMIZED SEARCH FOR LOGISTIC REGRESSION\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(\"\\nüîÑ This may take a few minutes...\")\n",
    "print(\"   Training 30 combinations √ó 5 folds = 150 models total\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Fit RandomizedSearchCV\n",
    "lr_random_search.fit(X_train_unified, y_train)\n",
    "\n",
    "tuning_time_lr = time.time() - start_time\n",
    "\n",
    "print(f\"\\n‚úì Tuning completed in {tuning_time_lr:.2f} seconds ({tuning_time_lr/60:.2f} minutes)\")\n",
    "\n",
    "# Get best parameters\n",
    "lr_best_params = lr_random_search.best_params_\n",
    "lr_best_score = lr_random_search.best_score_\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"LOGISTIC REGRESSION - BEST HYPERPARAMETERS FOUND\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(\"\\nBest parameters:\")\n",
    "for param, value in lr_best_params.items():\n",
    "    print(f\"  ‚Ä¢ {param}: {value}\")\n",
    "\n",
    "print(f\"\\nBest cross-validation score (mean accuracy across 5 folds):\")\n",
    "print(f\"  ‚Ä¢ {lr_best_score:.4f} ({lr_best_score*100:.2f}%)\")\n",
    "\n",
    "# Compare with baseline\n",
    "if 'test_acc_lr_unified' in locals():\n",
    "    improvement_lr = (lr_best_score - test_acc_lr_unified) * 100\n",
    "    print(f\"\\nComparison with baseline (Step 4):\")\n",
    "    print(f\"  ‚Ä¢ Baseline (default params): {test_acc_lr_unified*100:.2f}%\")\n",
    "    print(f\"  ‚Ä¢ Tuned (optimized params): {lr_best_score*100:.2f}%\")\n",
    "    print(f\"  ‚Ä¢ Improvement: {improvement_lr:+.2f}%\")\n",
    "    \n",
    "    if improvement_lr > 0.5:\n",
    "        print(f\"  ‚úì Significant improvement achieved!\")\n",
    "    elif improvement_lr > 0:\n",
    "        print(f\"  ‚úì Slight improvement\")\n",
    "    else:\n",
    "        print(f\"  ‚ö† No improvement - default params were already good\")\n",
    "\n",
    "# Train final model with best parameters\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"TRAINING FINAL LOGISTIC REGRESSION WITH BEST PARAMETERS\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "lr_tuned = LogisticRegression(**lr_best_params, random_state=42, multi_class='multinomial')\n",
    "lr_tuned.fit(X_train_unified, y_train)\n",
    "\n",
    "# Evaluate on test set\n",
    "y_test_pred_lr_tuned = lr_tuned.predict(X_test_unified)\n",
    "test_acc_lr_tuned = accuracy_score(y_test, y_test_pred_lr_tuned)\n",
    "precision_lr_tuned = precision_score(y_test, y_test_pred_lr_tuned, average='weighted')\n",
    "recall_lr_tuned = recall_score(y_test, y_test_pred_lr_tuned, average='weighted')\n",
    "f1_lr_tuned = f1_score(y_test, y_test_pred_lr_tuned, average='weighted')\n",
    "\n",
    "print(f\"\\nüìä LOGISTIC REGRESSION (TUNED) - TEST SET PERFORMANCE:\")\n",
    "print(f\"  ‚Ä¢ Test Accuracy: {test_acc_lr_tuned:.4f} ({test_acc_lr_tuned*100:.2f}%)\")\n",
    "print(f\"  ‚Ä¢ Precision: {precision_lr_tuned:.4f}\")\n",
    "print(f\"  ‚Ä¢ Recall: {recall_lr_tuned:.4f}\")\n",
    "print(f\"  ‚Ä¢ F1-Score: {f1_lr_tuned:.4f}\")\n",
    "\n",
    "print(f\"\\n‚úì Logistic Regression tuning complete!\")\n",
    "\n",
    "# ============================================================================\n",
    "# STAP 5D: COMPREHENSIVE COMPARISON - ALL MODELS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STAP 5D: COMPREHENSIVE COMPARISON - ALL MODELS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create comprehensive comparison table\n",
    "all_models_comparison = pd.DataFrame({\n",
    "    'Model': [\n",
    "        'RF - Baseline (Step 1)',\n",
    "        'RF - Unified (Step 4)',\n",
    "        'RF - Tuned (Step 5)',\n",
    "        'LR - Baseline (Step 1)',\n",
    "        'LR - Unified (Step 4)',\n",
    "        'LR - Tuned (Step 5)'\n",
    "    ],\n",
    "    'Test Accuracy': [\n",
    "        test_accuracy_rf if 'test_accuracy_rf' in locals() else np.nan,\n",
    "        test_acc_rf_unified if 'test_acc_rf_unified' in locals() else np.nan,\n",
    "        test_acc_rf_tuned,\n",
    "        test_accuracy_lr if 'test_accuracy_lr' in locals() else np.nan,\n",
    "        test_acc_lr_unified if 'test_acc_lr_unified' in locals() else np.nan,\n",
    "        test_acc_lr_tuned\n",
    "    ],\n",
    "    'Precision': [\n",
    "        precision_rf if 'precision_rf' in locals() else np.nan,\n",
    "        precision_rf_unified if 'precision_rf_unified' in locals() else np.nan,\n",
    "        precision_rf_tuned,\n",
    "        precision_lr if 'precision_lr' in locals() else np.nan,\n",
    "        precision_lr_unified if 'precision_lr_unified' in locals() else np.nan,\n",
    "        precision_lr_tuned\n",
    "    ],\n",
    "    'Recall': [\n",
    "        recall_rf if 'recall_rf' in locals() else np.nan,\n",
    "        recall_rf_unified if 'recall_rf_unified' in locals() else np.nan,\n",
    "        recall_rf_tuned,\n",
    "        recall_lr if 'recall_lr' in locals() else np.nan,\n",
    "        recall_lr_unified if 'recall_lr_unified' in locals() else np.nan,\n",
    "        recall_lr_tuned\n",
    "    ],\n",
    "    'F1-Score': [\n",
    "        f1_rf if 'f1_rf' in locals() else np.nan,\n",
    "        f1_rf_unified if 'f1_rf_unified' in locals() else np.nan,\n",
    "        f1_rf_tuned,\n",
    "        f1_lr if 'f1_lr' in locals() else np.nan,\n",
    "        f1_lr_unified if 'f1_lr_unified' in locals() else np.nan,\n",
    "        f1_lr_tuned\n",
    "    ]\n",
    "})\n",
    "\n",
    "all_models_comparison = all_models_comparison.round(4)\n",
    "\n",
    "print(\"\\nüìä COMPLETE MODEL EVOLUTION:\")\n",
    "print(\"=\"*80)\n",
    "display(all_models_comparison)\n",
    "\n",
    "# ============================================================================\n",
    "# STAP 5E: VISUALIZATION - TUNING RESULTS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STAP 5E: VISUALIZATION - TUNING RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Plot 1: Model Evolution - Test Accuracy\n",
    "model_names = ['RF\\nBaseline', 'RF\\nUnified', 'RF\\nTuned', 'LR\\nBaseline', 'LR\\nUnified', 'LR\\nTuned']\n",
    "test_accs = all_models_comparison['Test Accuracy'].values\n",
    "\n",
    "colors = ['lightblue', 'steelblue', 'darkblue', 'lightgreen', 'seagreen', 'darkgreen']\n",
    "bars = axes[0, 0].bar(model_names, test_accs, color=colors, alpha=0.8)\n",
    "axes[0, 0].set_ylabel('Test Accuracy')\n",
    "axes[0, 0].set_title('Model Evolution: Baseline ‚Üí Unified ‚Üí Tuned', fontweight='bold', fontsize=12)\n",
    "axes[0, 0].set_ylim([0.85, 1.0])\n",
    "axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "axes[0, 0].axhline(y=0.9, color='red', linestyle='--', alpha=0.5, label='90% threshold')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Add value labels\n",
    "for bar, acc in zip(bars, test_accs):\n",
    "    if not np.isnan(acc):\n",
    "        height = bar.get_height()\n",
    "        axes[0, 0].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                       f'{acc:.3f}',\n",
    "                       ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Plot 2: F1-Score Evolution\n",
    "f1_scores = all_models_comparison['F1-Score'].values\n",
    "bars = axes[0, 1].bar(model_names, f1_scores, color=colors, alpha=0.8)\n",
    "axes[0, 1].set_ylabel('F1-Score (Weighted)')\n",
    "axes[0, 1].set_title('F1-Score Evolution', fontweight='bold', fontsize=12)\n",
    "axes[0, 1].set_ylim([0.85, 1.0])\n",
    "axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar, f1 in zip(bars, f1_scores):\n",
    "    if not np.isnan(f1):\n",
    "        height = bar.get_height()\n",
    "        axes[0, 1].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                       f'{f1:.3f}',\n",
    "                       ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Plot 3: Random Forest - CV Results Distribution\n",
    "rf_cv_results = pd.DataFrame(rf_random_search.cv_results_)\n",
    "top_20_indices = rf_cv_results.nlargest(20, 'mean_test_score').index\n",
    "\n",
    "axes[1, 0].boxplot([rf_cv_results.loc[i, [f'split{j}_test_score' for j in range(5)]].values \n",
    "                     for i in top_20_indices],\n",
    "                    labels=[f'{i+1}' for i in range(20)])\n",
    "axes[1, 0].axhline(y=rf_best_score, color='red', linestyle='--', linewidth=2, label='Best score')\n",
    "axes[1, 0].set_xlabel('Top 20 Hyperparameter Combinations')\n",
    "axes[1, 0].set_ylabel('Cross-Validation Accuracy')\n",
    "axes[1, 0].set_title('Random Forest - CV Score Distribution (Top 20)', fontweight='bold', fontsize=12)\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 4: Logistic Regression - CV Results Distribution\n",
    "lr_cv_results = pd.DataFrame(lr_random_search.cv_results_)\n",
    "top_15_indices = lr_cv_results.nlargest(15, 'mean_test_score').index\n",
    "\n",
    "axes[1, 1].boxplot([lr_cv_results.loc[i, [f'split{j}_test_score' for j in range(5)]].values \n",
    "                     for i in top_15_indices],\n",
    "                    labels=[f'{i+1}' for i in range(15)])\n",
    "axes[1, 1].axhline(y=lr_best_score, color='red', linestyle='--', linewidth=2, label='Best score')\n",
    "axes[1, 1].set_xlabel('Top 15 Hyperparameter Combinations')\n",
    "axes[1, 1].set_ylabel('Cross-Validation Accuracy')\n",
    "axes[1, 1].set_title('Logistic Regression - CV Score Distribution (Top 15)', fontweight='bold', fontsize=12)\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.suptitle('Step 5: Hyperparameter Tuning Results', fontsize=16, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# STAP 5F: CONFUSION MATRICES - TUNED MODELS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STAP 5F: CONFUSION MATRICES - TUNED MODELS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Random Forest - Tuned\n",
    "cm_rf_tuned = confusion_matrix(y_test, y_test_pred_rf_tuned)\n",
    "sns.heatmap(cm_rf_tuned, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Insomnia', 'None', 'Sleep Apnea'],\n",
    "            yticklabels=['Insomnia', 'None', 'Sleep Apnea'],\n",
    "            ax=axes[0], cbar_kws={'label': 'Count'})\n",
    "axes[0].set_title(f'Random Forest (Tuned)\\nAccuracy: {test_acc_rf_tuned*100:.2f}% | F1: {f1_rf_tuned:.4f}',\n",
    "                  fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Actual Class')\n",
    "axes[0].set_xlabel('Predicted Class')\n",
    "\n",
    "# Logistic Regression - Tuned\n",
    "cm_lr_tuned = confusion_matrix(y_test, y_test_pred_lr_tuned)\n",
    "sns.heatmap(cm_lr_tuned, annot=True, fmt='d', cmap='Greens',\n",
    "            xticklabels=['Insomnia', 'None', 'Sleep Apnea'],\n",
    "            yticklabels=['Insomnia', 'None', 'Sleep Apnea'],\n",
    "            ax=axes[1], cbar_kws={'label': 'Count'})\n",
    "axes[1].set_title(f'Logistic Regression (Tuned)\\nAccuracy: {test_acc_lr_tuned*100:.2f}% | F1: {f1_lr_tuned:.4f}',\n",
    "                  fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Actual Class')\n",
    "axes[1].set_xlabel('Predicted Class')\n",
    "\n",
    "plt.suptitle('Confusion Matrices - Tuned Models (Test Set)', \n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# STAP 5G: DETAILED CLASSIFICATION REPORTS - TUNED MODELS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STAP 5G: DETAILED CLASSIFICATION REPORTS - TUNED MODELS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"RANDOM FOREST (TUNED) - CLASSIFICATION REPORT\")\n",
    "print(\"-\"*80)\n",
    "print(classification_report(y_test, y_test_pred_rf_tuned,\n",
    "                          target_names=['Insomnia', 'None', 'Sleep Apnea'],\n",
    "                          digits=4))\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"LOGISTIC REGRESSION (TUNED) - CLASSIFICATION REPORT\")\n",
    "print(\"-\"*80)\n",
    "print(classification_report(y_test, y_test_pred_lr_tuned,\n",
    "                          target_names=['Insomnia', 'None', 'Sleep Apnea'],\n",
    "                          digits=4))\n",
    "\n",
    "# ============================================================================\n",
    "# STAP 5H: FINAL SUMMARY & BEST MODEL SELECTION\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STAP 5H: FINAL SUMMARY & BEST MODEL SELECTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\"\"\n",
    "HYPERPARAMETER TUNING RESULTS:\n",
    "===============================\n",
    "\n",
    "RANDOM FOREST:\n",
    "‚Ä¢ Tuning time: {tuning_time_rf/60:.2f} minutes\n",
    "‚Ä¢ Best CV score: {rf_best_score*100:.2f}%\n",
    "‚Ä¢ Test accuracy: {test_acc_rf_tuned*100:.2f}%\n",
    "‚Ä¢ F1-Score: {f1_rf_tuned:.4f}\n",
    "\n",
    "Best hyperparameters:\n",
    "\"\"\")\n",
    "for param, value in rf_best_params.items():\n",
    "    print(f\"  ‚Ä¢ {param}: {value}\")\n",
    "\n",
    "print(f\"\"\"\n",
    "LOGISTIC REGRESSION:\n",
    "‚Ä¢ Tuning time: {tuning_time_lr/60:.2f} minutes\n",
    "‚Ä¢ Best CV score: {lr_best_score*100:.2f}%\n",
    "‚Ä¢ Test accuracy: {test_acc_lr_tuned*100:.2f}%\n",
    "‚Ä¢ F1-Score: {f1_lr_tuned:.4f}\n",
    "\n",
    "Best hyperparameters:\n",
    "\"\"\")\n",
    "for param, value in lr_best_params.items():\n",
    "    print(f\"  ‚Ä¢ {param}: {value}\")\n",
    "\n",
    "# Determine overall best model\n",
    "best_overall_acc = max(test_acc_rf_tuned, test_acc_lr_tuned)\n",
    "if test_acc_rf_tuned > test_acc_lr_tuned:\n",
    "    best_overall_model = \"Random Forest (Tuned)\"\n",
    "    best_overall_f1 = f1_rf_tuned\n",
    "else:\n",
    "    best_overall_model = \"Logistic Regression (Tuned)\"\n",
    "    best_overall_f1 = f1_lr_tuned\n",
    "\n",
    "print(f\"\\nüèÜ BEST OVERALL MODEL:\")\n",
    "print(f\"  ‚Ä¢ Model: {best_overall_model}\")\n",
    "print(f\"  ‚Ä¢ Test Accuracy: {best_overall_acc*100:.2f}%\")\n",
    "print(f\"  ‚Ä¢ F1-Score: {best_overall_f1:.4f}\")\n",
    "\n",
    "print(f\"\"\"\n",
    "IMPROVEMENT SUMMARY:\n",
    "====================\n",
    "\n",
    "From Baseline to Tuned:\n",
    "\"\"\")\n",
    "\n",
    "if 'test_accuracy_rf' in locals() and 'test_accuracy_lr' in locals():\n",
    "    rf_improvement = (test_acc_rf_tuned - test_accuracy_rf) * 100\n",
    "    lr_improvement = (test_acc_lr_tuned - test_accuracy_lr) * 100\n",
    "\n",
    "    print(f\"RANDOM FOREST:\")\n",
    "    print(f\"  ‚Ä¢ Accuracy Improvement: {rf_improvement:+.2f}%\")\n",
    "    print(f\"  ‚Ä¢ F1-Score Improvement: {(f1_rf_tuned - f1_rf)*100:.2f}%\")\n",
    "\n",
    "    print(f\"\\nLOGISTIC REGRESSION:\")\n",
    "    print(f\"  ‚Ä¢ Accuracy Improvement: {lr_improvement:+.2f}%\")\n",
    "    print(f\"  ‚Ä¢ F1-Score Improvement: {(f1_lr_tuned - f1_lr)*100:.2f}%\")\n",
    "\n",
    "# ============================================================================\n",
    "# STAP 4E: VISUALIZATION - UNIFIED MODELS PERFORMANCE\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STAP 4E: VISUALIZATION - UNIFIED MODELS PERFORMANCE\")\n",
    "print(\"=\"*80)\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "# Plot 1: Unified Models - Test Accuracy Comparison\n",
    "model_names = ['Random Forest\\n(Unified)', 'Logistic Regression\\n(Unified)']\n",
    "test_accs_unified = [test_acc_rf_unified, test_acc_lr_unified]\n",
    "bars = axes[0, 0].bar(model_names, test_accs_unified, color=['steelblue', 'seagreen'], alpha=0.8)\n",
    "axes[0, 0].set_ylabel('Test Accuracy')\n",
    "axes[0, 0].set_title('Unified Models - Test Accuracy Comparison', fontweight='bold', fontsize=12)\n",
    "axes[0, 0].set_ylim([0.85, 1.0])\n",
    "axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "# Add value labels  \n",
    "for bar, acc in zip(bars, test_accs_unified):\n",
    "    height = bar.get_height()\n",
    "    axes[0, 0].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{acc:.3f}',\n",
    "                   ha='center', va='bottom', fontsize=9)\n",
    "# Plot 2: Random Forest - Per-Class Metrics\n",
    "rf_precision = [rf_report['0']['precision'], rf_report['1']['precision'], rf_report['2']['precision']]\n",
    "rf_recall = [rf_report['0']['recall'], rf_report['1']['recall'], rf_report['2']['recall']]\n",
    "rf_f1 = [rf_report['0']['f1-score'], rf_report['1']['f1-score'], rf_report['2']['f1-score']]\n",
    "x = np.arange(len(classes))\n",
    "width = 0.25    \n",
    "\n",
    "axes[1, 1].bar(x, lr_precision, width, label='Precision', color='green', alpha=0.8)\n",
    "axes[1, 1].bar(x + width, lr_recall, width, label='Recall', color='orange', alpha=0.8)\n",
    "axes[1, 1].bar(x + 2*width, lr_f1, width, label='F1-Score', color='purple', alpha=0.8)              \n",
    "axes[1, 1].set_ylabel('Score')\n",
    "axes[1, 1].set_title('Logistic Regression (Unified) - Per-Class Metrics', fontweight='bold', fontsize=12)\n",
    "axes[1, 1].set_xticks(x + width)\n",
    "axes[1, 1].set_xticklabels(classes) \n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].set_ylim([0.0, 1.0])\n",
    "axes[1, 1].grid(axis='y', alpha=0.3)    \n",
    "plt.suptitle('Step 4: Unified Models Performance Comparison', fontsize=16, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.show()  \n",
    "# ============================================================================\n",
    "# STAP 4H: FINAL SUMMARY & RECOMMENDATIONS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fd8310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STAP 6: FINAL MODEL SELECTION & COMPREHENSIVE EVALUATION (KORT)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STAP 6: FINAL MODEL SELECTION & COMPREHENSIVE EVALUATION (KORT)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "DOEL:\n",
    "Selecteer het beste finale model voor deployment na preprocessing, feature selection en tuning.\n",
    "\n",
    "EVALUATIE:\n",
    "1) Performance: Accuracy, (weighted/macro) F1, per-class metrics\n",
    "2) Robustness: Cohen‚Äôs Kappa, MCC\n",
    "3) Praktisch: interpretability, inference speed, complexity, deployment/maintenance\n",
    "Vergelijking: Random Forest (tuned) vs Logistic Regression (tuned)\n",
    "\"\"\")\n",
    "\n",
    "# ============================================================================\n",
    "# 6A: CHECK OF ALLES BESCHIKBAAR IS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"6A: AVAILABILITY CHECK\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "required = ['rf_tuned', 'lr_tuned', 'X_test_unified', 'y_test']\n",
    "available = {k: (k in locals()) for k in required}\n",
    "\n",
    "for k, v in available.items():\n",
    "    print(f\"  ‚Ä¢ {k}: {'‚úì' if v else '‚ùå'}\")\n",
    "\n",
    "if not all(available.values()):\n",
    "    print(\"\\n‚ö†Ô∏è Run eerst Stap 5 / zorg dat tuned models + testdata beschikbaar zijn.\")\n",
    "else:\n",
    "    print(\"\\n‚úì Alles beschikbaar, evaluatie start...\")\n",
    "\n",
    "# ============================================================================\n",
    "# 6B: CORE METRICS + ROBUSTNESS\n",
    "# ============================================================================\n",
    "if all(available.values()):\n",
    "    from sklearn.metrics import (\n",
    "        accuracy_score, precision_score, recall_score, f1_score,\n",
    "        classification_report, confusion_matrix,\n",
    "        cohen_kappa_score, matthews_corrcoef\n",
    "    )\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import time\n",
    "\n",
    "    # Predictions\n",
    "    y_rf = rf_tuned.predict(X_test_unified)\n",
    "    y_lr = lr_tuned.predict(X_test_unified)\n",
    "\n",
    "    # Metrics table\n",
    "    metrics = {\n",
    "        \"RF (tuned)\": {\n",
    "            \"Accuracy\": accuracy_score(y_test, y_rf),\n",
    "            \"F1 (weighted)\": f1_score(y_test, y_rf, average=\"weighted\"),\n",
    "            \"F1 (macro)\": f1_score(y_test, y_rf, average=\"macro\"),\n",
    "            \"Kappa\": cohen_kappa_score(y_test, y_rf),\n",
    "            \"MCC\": matthews_corrcoef(y_test, y_rf),\n",
    "        },\n",
    "        \"LR (tuned)\": {\n",
    "            \"Accuracy\": accuracy_score(y_test, y_lr),\n",
    "            \"F1 (weighted)\": f1_score(y_test, y_lr, average=\"weighted\"),\n",
    "            \"F1 (macro)\": f1_score(y_test, y_lr, average=\"macro\"),\n",
    "            \"Kappa\": cohen_kappa_score(y_test, y_lr),\n",
    "            \"MCC\": matthews_corrcoef(y_test, y_lr),\n",
    "        },\n",
    "    }\n",
    "    metrics_df = pd.DataFrame(metrics).T.round(4)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"CORE METRICS\")\n",
    "    print(\"=\"*80)\n",
    "    display(metrics_df)\n",
    "\n",
    "    # ============================================================================\n",
    "    # 6C: PER-CLASS SUMMARY (F1)\n",
    "    # ============================================================================\n",
    "    class_names = ['Insomnia', 'None', 'Sleep Apnea']\n",
    "\n",
    "    rf_rep = classification_report(y_test, y_rf, target_names=class_names, output_dict=True)\n",
    "    lr_rep = classification_report(y_test, y_lr, target_names=class_names, output_dict=True)\n",
    "\n",
    "    per_class = pd.DataFrame({\n",
    "        \"RF F1\": [rf_rep[c][\"f1-score\"] for c in class_names],\n",
    "        \"LR F1\": [lr_rep[c][\"f1-score\"] for c in class_names],\n",
    "        \"Support\": [rf_rep[c][\"support\"] for c in class_names],\n",
    "    }, index=class_names).round(4)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PER-CLASS (F1)\")\n",
    "    print(\"=\"*80)\n",
    "    display(per_class)\n",
    "\n",
    "    # ============================================================================\n",
    "    # 6D: CONFUSION MATRIX (compact)\n",
    "    # ============================================================================\n",
    "    cm_rf = confusion_matrix(y_test, y_rf)\n",
    "    cm_lr = confusion_matrix(y_test, y_lr)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"CONFUSION MATRIX (RAW)\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"RF:\\n\", cm_rf)\n",
    "    print(\"\\nLR:\\n\", cm_lr)\n",
    "\n",
    "    # ============================================================================\n",
    "    # 6E: PRAKTISCH (SPEED + COMPLEXITY)\n",
    "    # ============================================================================\n",
    "    start = time.time()\n",
    "    for _ in range(10):\n",
    "        rf_tuned.predict(X_test_unified)\n",
    "    rf_ms = (time.time() - start) / 10 * 1000\n",
    "\n",
    "    start = time.time()\n",
    "    for _ in range(10):\n",
    "        lr_tuned.predict(X_test_unified)\n",
    "    lr_ms = (time.time() - start) / 10 * 1000\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PRAKTISCH\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"‚Ä¢ Inference time (batch): RF={rf_ms:.2f} ms | LR={lr_ms:.2f} ms\")\n",
    "    print(f\"‚Ä¢ Complexity: RF hoog (ensemble {rf_tuned.n_estimators} trees) | LR laag (lineair, {X_test_unified.shape[1]} coeffs)\")\n",
    "    print(\"‚Ä¢ Interpretability: RF medium (feature importance) | LR hoog (coefs/odds)\")\n",
    "\n",
    "    # ============================================================================\n",
    "    # 6F: EINDKEUZE (simpele regel)\n",
    "    # ============================================================================\n",
    "    # Primary rule: kies model met hogere weighted F1, tenzij verschil klein en interpretability belangrijk\n",
    "    rf_f1 = metrics_df.loc[\"RF (tuned)\", \"F1 (weighted)\"]\n",
    "    lr_f1 = metrics_df.loc[\"LR (tuned)\", \"F1 (weighted)\"]\n",
    "    rf_acc = metrics_df.loc[\"RF (tuned)\", \"Accuracy\"]\n",
    "    lr_acc = metrics_df.loc[\"LR (tuned)\", \"Accuracy\"]\n",
    "\n",
    "    diff = abs(rf_f1 - lr_f1)\n",
    "\n",
    "    if rf_f1 > lr_f1:\n",
    "        recommended = \"Random Forest (tuned)\"\n",
    "        runner_up = \"Logistic Regression (tuned)\"\n",
    "    else:\n",
    "        recommended = \"Logistic Regression (tuned)\"\n",
    "        runner_up = \"Random Forest (tuned)\"\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FINAL RECOMMENDATION\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"üèÜ Aanbevolen model: {recommended}\")\n",
    "    print(f\"‚Ä¢ Accuracy: RF={rf_acc:.3f} | LR={lr_acc:.3f}\")\n",
    "    print(f\"‚Ä¢ F1 (weighted): RF={rf_f1:.3f} | LR={lr_f1:.3f}\")\n",
    "    print(f\"‚Ä¢ Verschil in F1: {diff:.4f}  -> {'klein' if diff < 0.02 else 'duidelijk'}\")\n",
    "    print(f\"Backup/runner-up: {runner_up}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
